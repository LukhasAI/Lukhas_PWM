"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘ ğŸ§  LUKHAS AI - SYMBOLIC TRACE LOGGER
â•‘ Track bio-symbolic metrics and patterns.
â•‘ Copyright (c) 2025 LUKHAS AI. All rights reserved.
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘ Module: symbolic_trace_logger.py
â•‘ Path: lukhas/[subdirectory]/symbolic_trace_logger.py
â•‘ Version: 1.0.0 | Created: 2025-07-25 | Modified: 2025-07-25
â•‘ Authors: LUKHAS AI Consciousness Team | Jules
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘ DESCRIPTION
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘ Track bio-symbolic metrics and patterns.
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

# Module imports
import logging
from typing import Optional, Dict, Any

# Configure module logger
logger = logging.getLogger(__name__)

# Module constants
MODULE_VERSION = "1.0.0"
MODULE_NAME = "symbolic trace logger"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FILENAME: symbolic_trace_logger.py
# MODULE: core.advanced.brain.awareness.symbolic_trace_logger
# DESCRIPTION: Enhanced symbolic trace logger for tracking bio-symbolic metrics
#              and patterns in the Lukhas Awareness Protocol.
# DEPENDENCIES: logging, json, datetime, typing, pathlib
# LICENSE: PROPRIETARY - LUKHAS AI SYSTEMS - UNAUTHORIZED ACCESS PROHIBITED
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
Enhanced symbolic trace logger for tracking bio-symbolic metrics and patterns
in the Lukhas Awareness Protocol.
"""

import logging
import json
from datetime import datetime
from typing import Dict, Any, List, Optional
from pathlib import Path

# Initialize logger for Î›TRACE
logger = logging.getLogger("Î›TRACE.core.advanced.brain.awareness.symbolic_trace_logger")
logger.info("Î›TRACE: Initializing symbolic_trace_logger module.")


# Placeholder for the tier decorator
# Human-readable comment: Placeholder for tier requirement decorator.
def lukhas_tier_required(level: int):
    """Conceptual placeholder for a tier requirement decorator."""
    def decorator(func):
        def wrapper(*args, **kwargs):
            # In a real scenario, user_id might be extracted from args, kwargs, or context
            # For a utility class like this, tier checks might be done by the caller.
            logger.debug(f"Î›TRACE: (Placeholder) Tier check: Function '{func.__name__}' requires Tier {level}.")
            return func(*args, **kwargs)
        return wrapper
    return decorator


# Human-readable comment: Logger for bio-symbolic traces from the Lukhas Awareness Protocol.
class SymbolicTraceLogger:
    """
    Bio-inspired trace logger that tracks:
    - Quantum state patterns
    - Energy gradients
    - Pattern recognition hits
    - Security signatures
    - Awareness context and metrics
    """

    # Human-readable comment: Initializes the SymbolicTraceLogger.
    @lukhas_tier_required(level=1) # Example: Initializing a logger might be a Basic tier operation
    def __init__(self, log_path: Optional[str] = None, user_id_context: Optional[str] = None):
        """
        Initializes the SymbolicTraceLogger.
        Args:
            log_path (Optional[str]): Directory path to store log files.
                                      Defaults to "logs/symbolic_traces".
            user_id_context (Optional[str]): Optional user ID for contextual logging if this instance is user-specific.
        """
        self.instance_logger = logger.getChild(f"SymbolicTraceLogger.{user_id_context or 'global'}")
        self.instance_logger.info(f"Î›TRACE: Initializing SymbolicTraceLogger instance. Log path: '{log_path}'.")

        # TODO: Make log_path configurable via a centralized config system or environment variable.
        default_log_dir = Path("logs/symbolic_traces")
        self.log_path: Path = Path(log_path) if log_path else default_log_dir

        try:
            self.log_path.mkdir(parents=True, exist_ok=True)
            self.instance_logger.info(f"Î›TRACE: Log directory ensured at '{self.log_path}'.")
        except OSError as e:
            self.instance_logger.error(f"Î›TRACE: Failed to create log directory '{self.log_path}': {e}", exc_info=True)
            # Fallback to a temporary directory or disable file logging if critical
            # For now, operations might fail if directory can't be created.

        self.bio_metrics_buffer: List[Dict[str, Any]] = []
        self.quantum_like_states_buffer: List[Dict[str, Any]] = []
        self.pattern_buffer: List[Dict[str, Any]] = [] # Stores recent trace_data for pattern analysis

        self.metrics: Dict[str, Any] = {
            "total_traces_logged": 0, "pattern_matches_detected": 0,
            "avg_quantum_coherence": 1.0, "avg_gradient_efficiency": 1.0,
            "last_flush_timestamp": None
        }
        self.instance_logger.debug(f"Î›TRACE: SymbolicTraceLogger initialized with default metrics.")

    # Human-readable comment: Logs an awareness trace with bio-symbolic context.
    @lukhas_tier_required(level=0) # Example: Logging traces might be a Free/Guest tier operation
    def log_awareness_trace(self, trace_data: Dict[str, Any]) -> None:
        """
        Log awareness trace with bio-symbolic context.
        Args:
            trace_data (Dict[str, Any]): The trace data dictionary to log.
        """
        self.instance_logger.debug(f"Î›TRACE: Received request to log awareness trace. Keys: {list(trace_data.keys())}")

        timestamp = trace_data.get("timestamp", datetime.utcnow().isoformat())
        # Ensure timestamp is a string, not datetime object, for JSON serialization consistency
        if isinstance(timestamp, datetime):
            timestamp = timestamp.isoformat()
        trace_data["timestamp"] = timestamp # Standardize timestamp in logged data

        if "bio_metrics" in trace_data and isinstance(trace_data["bio_metrics"], dict):
            self.bio_metrics_buffer.append({"timestamp": timestamp, "metrics": trace_data["bio_metrics"]})

        if "quantum_like_states" in trace_data and isinstance(trace_data["quantum_like_states"], dict):
            self.quantum_like_states_buffer.append({"timestamp": timestamp, "states": trace_data["quantum_like_states"]})

        self._update_internal_metrics(trace_data) # Renamed from _update_metrics, logs internally
        self._write_trace_to_file(trace_data)     # Renamed from _write_trace, logs internally

        # Example buffer flushing condition
        # TODO: Make buffer sizes and flush conditions configurable.
        if len(self.bio_metrics_buffer) >= 50 or len(self.quantum_like_states_buffer) >= 50:
            self.instance_logger.info("Î›TRACE: Buffer limit reached, flushing metric buffers.")
            self._flush_metric_buffers() # Renamed from _flush_buffers, logs internally
        self.instance_logger.debug(f"Î›TRACE: Awareness trace logged successfully. Total traces: {self.metrics['total_traces_logged']}.")

    # Human-readable comment: Analyzes collected bio-symbolic patterns and metrics.
    @lukhas_tier_required(level=2) # Example: Pattern analysis might be Professional tier
    def get_pattern_analysis(self) -> Dict[str, Any]:
        """
        Analyze collected bio-symbolic patterns and current metric trends.
        Returns:
            Dict[str, Any]: A dictionary containing analysis results.
        """
        self.instance_logger.info("Î›TRACE: Requesting pattern analysis.")
        analysis_results = {
            "total_traces_logged": self.metrics["total_traces_logged"],
            "pattern_matches_detected": self.metrics["pattern_matches_detected"],
            "current_avg_quantum_coherence": self.metrics["avg_quantum_coherence"],
            "current_avg_gradient_efficiency": self.metrics["avg_gradient_efficiency"],
            "bio_metrics_trends": self._analyze_buffered_bio_metrics(), # Renamed, logs internally
            "quantum_like_state_trends": self._analyze_buffered_quantum_like_states(), # Renamed, logs internally
            "analysis_timestamp": datetime.utcnow().isoformat()
        }
        self.instance_logger.info(f"Î›TRACE: Pattern analysis generated. Matches: {analysis_results['pattern_matches_detected']}")
        return analysis_results

    # Human-readable comment: Updates internal performance and tracking metrics.
    def _update_internal_metrics(self, trace_data: Dict[str, Any]) -> None:
        """Update internal performance and tracking metrics based on new trace data."""
        self.instance_logger.debug("Î›TRACE: Internal: Updating internal metrics.")
        self.metrics["total_traces_logged"] += 1

        # Weighted average update for coherence and efficiency
        smoothing_factor = 0.1
        if "bio_metrics" in trace_data and isinstance(trace_data["bio_metrics"], dict):
            current_gradient = trace_data["bio_metrics"].get("proton_gradient", self.metrics["avg_gradient_efficiency"])
            self.metrics["avg_gradient_efficiency"] = (1 - smoothing_factor) * self.metrics["avg_gradient_efficiency"] + \
                                                      smoothing_factor * float(current_gradient)

        if "quantum_like_states" in trace_data and isinstance(trace_data["quantum_like_states"], dict):
            current_coherence = trace_data["quantum_like_states"].get("coherence", self.metrics["avg_quantum_coherence"])
            self.metrics["avg_quantum_coherence"] = (1 - smoothing_factor) * self.metrics["avg_quantum_coherence"] + \
                                                     smoothing_factor * float(current_coherence)

        if self._detect_significant_pattern(trace_data): # Renamed, logs internally
            self.metrics["pattern_matches_detected"] += 1
        self.instance_logger.debug(f"Î›TRACE: Internal metrics updated: {self.metrics}")

    # Human-readable comment: Detects interesting or significant patterns in trace data.
    def _detect_significant_pattern(self, trace_data: Dict[str, Any]) -> bool:
        """Detect interesting or significant patterns in trace data."""
        # self.instance_logger.debug("Î›TRACE: Internal: Detecting significant patterns.") # Can be verbose
        self.pattern_buffer.append(trace_data)
        if len(self.pattern_buffer) > 10: # Keep buffer size manageable
            self.pattern_buffer.pop(0)

        if len(self.pattern_buffer) == 10: # Analyze when buffer is full
            is_pattern = self._analyze_trace_sequence_for_pattern() # Renamed, logs internally
            if is_pattern: self.instance_logger.info("Î›TRACE: Significant pattern detected in trace sequence.")
            return is_pattern
        return False

    # Human-readable comment: Analyzes a sequence of traces for predefined patterns.
    def _analyze_trace_sequence_for_pattern(self) -> bool:
        """Analyze sequence of traces for predefined patterns (e.g., trends in confidence scores)."""
        # self.instance_logger.debug("Î›TRACE: Internal: Analyzing trace sequence for patterns.")
        if len(self.pattern_buffer) < 2: return False # Need at least 2 points for a trend

        confidence_scores = [trace.get("confidence_score", 0.0) for trace in self.pattern_buffer if isinstance(trace, dict)]
        if len(confidence_scores) < 2 : return False

        # Simple trend detection (example)
        is_increasing = all(confidence_scores[i] <= confidence_scores[i+1] for i in range(len(confidence_scores)-1))
        is_decreasing = all(confidence_scores[i] >= confidence_scores[i+1] for i in range(len(confidence_scores)-1))

        if is_increasing and confidence_scores[-1] > 0.7: # Example: strong upward trend
            self.instance_logger.debug("Î›TRACE: Increasing confidence pattern detected.")
            return True
        if is_decreasing and confidence_scores[-1] < 0.3: # Example: strong downward trend
            self.instance_logger.debug("Î›TRACE: Decreasing confidence pattern detected.")
            return True
        return False

    # Human-readable comment: Analyzes trends in buffered bio-metrics.
    def _analyze_buffered_bio_metrics(self) -> Dict[str, float]:
        """Analyze trends in buffered bio-metrics."""
        self.instance_logger.debug("Î›TRACE: Internal: Analyzing buffered bio-metrics.")
        if not self.bio_metrics_buffer: return {}

        # Consider last N metrics for trend analysis, e.g., last 10 or all in buffer if fewer
        recent_metric_entries = self.bio_metrics_buffer[-10:]
        num_entries = len(recent_metric_entries)
        if num_entries == 0: return {}

        # Initialize sums and counts for averaging
        aggregated_metrics: Dict[str, List[float]] = {}
        for entry in recent_metric_entries:
            if isinstance(entry.get("metrics"), dict):
                for key, value in entry["metrics"].items():
                    try:
                        float_val = float(value)
                        if key not in aggregated_metrics: aggregated_metrics[key] = []
                        aggregated_metrics[key].append(float_val)
                    except (ValueError, TypeError):
                        self.instance_logger.warning(f"Î›TRACE: Non-numeric value '{value}' for bio-metric '{key}' skipped.")

        trends = {key: sum(values)/len(values) for key, values in aggregated_metrics.items() if values}
        self.instance_logger.debug(f"Î›TRACE: Bio-metric trends calculated: {trends}")
        return trends

    # Human-readable comment: Analyzes trends in buffered quantum-like states.
    def _analyze_buffered_quantum_like_states(self) -> Dict[str, float]:
        """Analyze trends in buffered quantum-like states."""
        self.instance_logger.debug("Î›TRACE: Internal: Analyzing buffered quantum-like states.")
        if not self.quantum_like_states_buffer: return {}

        recent_state_entries = self.quantum_like_states_buffer[-10:]
        num_entries = len(recent_state_entries)
        if num_entries == 0: return {}

        coherence_values: List[float] = []
        entanglement_counts: List[int] = []

        for entry in recent_state_entries:
            if isinstance(entry.get("states"), dict):
                states_dict = entry["states"]
                if "coherence" in states_dict:
                    try: coherence_values.append(float(states_dict["coherence"]))
                    except (ValueError, TypeError): pass
                if "entanglement" in states_dict and isinstance(states_dict["entanglement"], dict):
                    entanglement_counts.append(len(states_dict["entanglement"]))

        analysis: Dict[str, float] = {}
        if coherence_values: analysis["avg_coherence_trend"] = sum(coherence_values) / len(coherence_values)
        if entanglement_counts: analysis["avg_entanglement_count"] = sum(entanglement_counts) / len(entanglement_counts)

        self.instance_logger.debug(f"Î›TRACE: Quantum state trends calculated: {analysis}")
        return analysis

    # Human-readable comment: Writes a single trace data dictionary to a daily log file.
    def _write_trace_to_file(self, trace_data: Dict[str, Any]) -> None:
        """Write trace data to a daily log file."""
        self.instance_logger.debug("Î›TRACE: Internal: Writing trace to file.")
        try:
            # Ensure timestamp is a string for strftime
            timestamp_str = trace_data.get("timestamp", datetime.utcnow().isoformat())
            if isinstance(timestamp_str, datetime): # Should have been converted before, but double check
                timestamp_dt = timestamp_str
            else:
                timestamp_dt = datetime.fromisoformat(timestamp_str.replace("Z", "+00:00"))

            date_str = timestamp_dt.strftime("%Y%m%d")
            log_file = self.log_path / f"symbolic_trace_{date_str}.jsonl"

            # For security and privacy, consider filtering or summarizing sensitive fields before logging
            # E.g., trace_data_to_log = self._filter_sensitive_data(trace_data)

            with open(log_file, "a", encoding='utf-8') as f: # Added encoding
                f.write(json.dumps(trace_data) + "\n")
            self.instance_logger.debug(f"Î›TRACE: Trace data appended to '{log_file}'.")

        except Exception as e:
            self.instance_logger.error(f"Î›TRACE: Error writing trace to file '{self.log_path}': {e}", exc_info=True)

    # Human-readable comment: Flushes buffered metrics to disk (e.g., as aggregated JSON files).
    def _flush_metric_buffers(self) -> None:
        """Flush metric buffers to disk, e.g., as aggregated JSON files."""
        self.instance_logger.info("Î›TRACE: Flushing metric buffers to disk.")
        try:
            # Example: Write bio metrics buffer
            if self.bio_metrics_buffer:
                bio_metrics_file = self.log_path / f"aggregated_bio_metrics_{datetime.utcnow().strftime('%Y%m%d%H%M%S')}.json"
                with open(bio_metrics_file, "w", encoding='utf-8') as f:
                    json.dump(self.bio_metrics_buffer, f, indent=2)
                self.instance_logger.info(f"Î›TRACE: Bio-metrics buffer flushed to '{bio_metrics_file}'.")
                self.bio_metrics_buffer = [] # Clear buffer after flushing

            # Example: Write quantum-like states buffer
            if self.quantum_like_states_buffer:
                quantum_like_states_file = self.log_path / f"aggregated_quantum_like_states_{datetime.utcnow().strftime('%Y%m%d%H%M%S')}.json"
                with open(quantum_like_states_file, "w", encoding='utf-8') as f:
                    json.dump(self.quantum_like_states_buffer, f, indent=2)
                self.instance_logger.info(f"Î›TRACE: Quantum states buffer flushed to '{quantum_like_states_file}'.")
                self.quantum_like_states_buffer = [] # Clear buffer

            self.metrics["last_flush_timestamp"] = datetime.utcnow().isoformat()

        except Exception as e:
            self.instance_logger.error(f"Î›TRACE: Error flushing metric buffers: {e}", exc_info=True)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FILENAME: symbolic_trace_logger.py
# VERSION: 1.1.0
# TIER SYSTEM: Tier 2-4 (Logging infrastructure; advanced pattern analysis might imply higher tiers)
# Î›TRACE INTEGRATION: ENABLED
# CAPABILITIES: Logs detailed bio-symbolic awareness traces, buffers metrics,
#               performs trend analysis on buffered data, and flushes aggregated data to disk.
# FUNCTIONS: None directly exposed (all logic within SymbolicTraceLogger class).
# CLASSES: SymbolicTraceLogger.
# DECORATORS: @lukhas_tier_required (conceptual placeholder).
# DEPENDENCIES: logging, json, datetime, typing, pathlib.
# INTERFACES: Public methods of SymbolicTraceLogger (log_awareness_trace, get_pattern_analysis).
# ERROR HANDLING: Catches exceptions during file I/O and logs them.
# LOGGING: Î›TRACE_ENABLED using hierarchical loggers for its own operational logging.
# AUTHENTICATION: Not directly handled; tier checks are conceptual.
# HOW TO USE:
#   from core.advanced.brain.awareness.symbolic_trace_logger import SymbolicTraceLogger
#   trace_logger = SymbolicTraceLogger(log_path="path/to/my/traces")
#   trace_logger.log_awareness_trace({"event_type": "test", "data": "some_data"})
#   analysis = trace_logger.get_pattern_analysis()
# INTEGRATION NOTES: The default log path is relative ("logs/symbolic_traces"). This should
#                    be configured to an absolute path or a path relative to a defined
#                    application data directory in production environments.
#                    Buffer flushing logic and conditions may need tuning based on log volume.
# MAINTENANCE: Regularly review buffer sizes and flush conditions.
#              Enhance pattern detection and analysis methods as requirements evolve.
#              Ensure log rotation or management for trace files if volume is high.
# CONTACT: LUKHAS DEVELOPMENT TEAM
# LICENSE: PROPRIETARY - LUKHAS AI SYSTEMS - UNAUTHORIZED ACCESS PROHIBITED
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘ ğŸ“‹ FOOTER - LUKHAS AI
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘ VALIDATION:
â•‘   - Tests: lukhas/tests/test_symbolic_trace_logger.py
â•‘   - Coverage: N/A%
â•‘   - Linting: pylint N/A/10
â•‘
â•‘ MONITORING:
â•‘   - Metrics: N/A
â•‘   - Logs: N/A
â•‘   - Alerts: N/A
â•‘
â•‘ COMPLIANCE:
â•‘   - Standards: N/A
â•‘   - Ethics: Refer to LUKHAS Ethics Guidelines
â•‘   - Safety: Refer to LUKHAS Safety Protocols
â•‘
â•‘ REFERENCES:
â•‘   - Docs: docs/consciousness/symbolic trace logger.md
â•‘   - Issues: github.com/lukhas-ai/lukhas/issues?label=symbolic trace logger
â•‘   - Wiki: N/A
â•‘
â•‘ COPYRIGHT & LICENSE:
â•‘   Copyright (c) 2025 LUKHAS AI. All rights reserved.
â•‘   Licensed under the LUKHAS AI Proprietary License.
â•‘   Unauthorized use, reproduction, or distribution is prohibited.
â•‘
â•‘ DISCLAIMER:
â•‘   This module is part of the LUKHAS AGI system. Use only as intended
â•‘   within the system architecture. Modifications may affect system
â•‘   stability and require approval from the LUKHAS Architecture Board.
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""