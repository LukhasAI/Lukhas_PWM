#!/usr/bin/env python3
"""
End-to-End System Integration Tests
Comprehensive testing of cross-system workflows and integration points
"""

import asyncio
import pytest
import time
import json
from pathlib import Path
import sys
from typing import Dict, Any, List
from unittest.mock import Mock, AsyncMock, patch
from dataclasses import dataclass
from datetime import datetime

# Add project root to path
sys.path.insert(0, str(Path(__file__).resolve().parents[1]))


@dataclass
class TestMetrics:
    """Metrics collected during test execution"""
    start_time: float
    end_time: float = 0
    operations: int = 0
    errors: int = 0
    latencies: List[float] = None

    def __post_init__(self):
        if self.latencies is None:
            self.latencies = []

    @property
    def duration(self) -> float:
        return self.end_time - self.start_time

    @property
    def avg_latency(self) -> float:
        return sum(self.latencies) / len(self.latencies) if self.latencies else 0

    @property
    def success_rate(self) -> float:
        total = self.operations
        return ((total - self.errors) / total * 100) if total > 0 else 0


class TestSystemIntegrationE2E:
    """End-to-end system integration test suite"""

    @pytest.fixture
    def metrics(self):
        """Test metrics collector"""
        return TestMetrics(start_time=time.time())

    @pytest.fixture
    async def mock_hub_registry(self):
        """Mock hub registry with all system hubs"""
        registry = Mock()

        # Define available hubs
        hubs = {
            'core': AsyncMock(),
            'consciousness': AsyncMock(),
            'memory': AsyncMock(),
            'quantum': AsyncMock(),
            'safety': AsyncMock(),
            'bio': AsyncMock(),
            'orchestration': AsyncMock(),
            'nias': AsyncMock(),
            'dream': AsyncMock(),
            'symbolic': AsyncMock(),
            'learning': AsyncMock()
        }

        # Configure hub behaviors
        for name, hub in hubs.items():
            hub.name = name
            hub.is_initialized = True
            hub.services = {}
            hub.health_check = AsyncMock(return_value={'status': 'healthy'})
            hub.process_event = AsyncMock(return_value={'processed': True})
            hub.get_service = Mock(side_effect=lambda s: hub.services.get(s))
            hub.register_service = Mock(side_effect=lambda n, s: hub.services.update({n: s}))

        registry.hubs = hubs
        registry.get_hub = Mock(side_effect=lambda name: hubs.get(name))
        registry.health_check_all = AsyncMock(return_value={
            name: {'status': 'healthy'} for name in hubs
        })

        return registry

    @pytest.fixture
    async def mock_bridge_registry(self):
        """Mock bridge registry with system bridges"""
        registry = Mock()

        bridges = {
            'core_consciousness': AsyncMock(),
            'consciousness_quantum': AsyncMock(),
            'core_safety': AsyncMock(),
            'memory_consciousness': AsyncMock(),
            'nias_dream': AsyncMock(),
            'quantum_memory': AsyncMock()
        }

        # Configure bridge behaviors
        for name, bridge in bridges.items():
            bridge.name = name
            bridge.is_connected = True
            bridge.connect = AsyncMock(return_value=True)
            bridge.sync_state = AsyncMock(return_value=True)

            # Add bidirectional methods
            systems = name.split('_')
            setattr(bridge, f"{systems[0]}_to_{systems[1]}",
                   AsyncMock(return_value={'forwarded': True}))
            setattr(bridge, f"{systems[1]}_to_{systems[0]}",
                   AsyncMock(return_value={'forwarded': True}))

        registry.bridges = bridges
        registry.get_bridge = Mock(side_effect=lambda name: bridges.get(name))
        registry.health_check_all = AsyncMock(return_value={
            name: {'connected': True} for name in bridges
        })

        return registry

    @pytest.fixture
    async def mock_service_discovery(self, mock_hub_registry):
        """Mock service discovery system"""
        discovery = Mock()

        # Define available services
        services = {
            'ai_interface': {'hub': 'core', 'type': 'interface'},
            'quantum_consciousness_hub': {'hub': 'quantum', 'type': 'processing'},
            'memory_manager': {'hub': 'memory', 'type': 'storage'},
            'ai_safety_orchestrator': {'hub': 'safety', 'type': 'validation'},
            'nias_core': {'hub': 'nias', 'type': 'filtering'},
            'learning_service': {'hub': 'learning', 'type': 'adaptation'}
        }

        discovery.services = services
        discovery.find_service = Mock(side_effect=lambda name: services.get(name))
        discovery.list_all_services = Mock(return_value={
            hub: [s for s, info in services.items() if info['hub'] == hub]
            for hub in mock_hub_registry.hubs
        })
        discovery.health_check_service = AsyncMock(return_value={'status': 'available'})

        return discovery

    @pytest.mark.asyncio
    async def test_full_system_initialization(self, mock_hub_registry, mock_bridge_registry,
                                            mock_service_discovery, metrics):
        """Test complete system initialization sequence"""
        # Initialize all hubs
        for hub_name, hub in mock_hub_registry.hubs.items():
            await hub.initialize() if hasattr(hub, 'initialize') else None
            metrics.operations += 1

        # Connect all bridges
        for bridge_name, bridge in mock_bridge_registry.bridges.items():
            connected = await bridge.connect()
            assert connected == True
            metrics.operations += 1

        # Verify service discovery
        all_services = mock_service_discovery.list_all_services()
        total_services = sum(len(services) for services in all_services.values())
        assert total_services >= 6  # At least our key services

        # Health check all systems
        hub_health = await mock_hub_registry.health_check_all()
        bridge_health = await mock_bridge_registry.health_check_all()

        # All should be healthy
        assert all(h['status'] == 'healthy' for h in hub_health.values())
        assert all(b['connected'] == True for b in bridge_health.values())

        metrics.end_time = time.time()
        assert metrics.success_rate == 100

    @pytest.mark.asyncio
    async def test_cross_system_message_flow(self, mock_hub_registry, mock_bridge_registry, metrics):
        """Test message flow across multiple systems"""
        # Scenario: Core -> Safety -> Consciousness -> Memory

        # Step 1: Core receives request
        core_hub = mock_hub_registry.get_hub('core')
        request = {
            'type': 'process_request',
            'data': 'test_data',
            'requires_safety_check': True
        }

        start = time.time()
        core_result = await core_hub.process_event('request', request)
        assert core_result['processed'] == True
        metrics.operations += 1

        # Step 2: Safety validation via bridge
        safety_bridge = mock_bridge_registry.get_bridge('core_safety')
        safety_result = await safety_bridge.core_to_safety('validate', request)
        assert safety_result['forwarded'] == True
        metrics.operations += 1

        # Step 3: Consciousness processing
        consciousness_bridge = mock_bridge_registry.get_bridge('core_consciousness')
        consciousness_result = await consciousness_bridge.core_to_consciousness('process', {
            **request,
            'safety_approved': True
        })
        assert consciousness_result['forwarded'] == True
        metrics.operations += 1

        # Step 4: Memory storage
        memory_bridge = mock_bridge_registry.get_bridge('memory_consciousness')
        memory_result = await memory_bridge.consciousness_to_memory('store', {
            'processed_data': 'result',
            'timestamp': datetime.now().isoformat()
        })
        assert memory_result['forwarded'] == True
        metrics.operations += 1

        end = time.time()
        metrics.latencies.append((end - start) * 1000)

        # Verify complete flow
        assert metrics.errors == 0
        assert metrics.avg_latency < 100  # Under 100ms for full flow

    @pytest.mark.asyncio
    async def test_nias_dream_integration_workflow(self, mock_hub_registry,
                                                  mock_bridge_registry,
                                                  mock_service_discovery, metrics):
        """Test NIAS-Dream integration workflow"""
        # Get NIAS service
        nias_service = mock_service_discovery.find_service('nias_core')
        assert nias_service is not None

        # Configure NIAS mock
        nias_hub = mock_hub_registry.get_hub('nias')
        nias_hub.push_symbolic_message = AsyncMock(return_value={
            'status': 'deferred_to_dream',
            'dream_id': 'dream_123'
        })

        # Test message that should defer to dreams
        test_message = {
            'content': 'Symbolic message for dream processing',
            'category': 'symbolic',
            'required_tier': 5
        }

        test_context = {
            'user_id': 'test_user',
            'tier': 3,  # Lower than required
            'consent_categories': ['symbolic']
        }

        start = time.time()

        # Process through NIAS
        result = await nias_hub.push_symbolic_message(test_message, test_context)
        assert result['status'] == 'deferred_to_dream'
        assert result['dream_id'] == 'dream_123'
        metrics.operations += 1

        # Verify bridge communication
        nias_dream_bridge = mock_bridge_registry.get_bridge('nias_dream')
        dream_result = await nias_dream_bridge.nias_to_dream('defer_message', {
            'message': test_message,
            'dream_id': result['dream_id']
        })
        assert dream_result['forwarded'] == True
        metrics.operations += 1

        end = time.time()
        metrics.latencies.append((end - start) * 1000)

        assert metrics.errors == 0

    @pytest.mark.asyncio
    async def test_quantum_consciousness_enhancement(self, mock_hub_registry,
                                                   mock_bridge_registry, metrics):
        """Test quantum-enhanced consciousness processing"""
        # Get hubs
        consciousness_hub = mock_hub_registry.get_hub('consciousness')
        quantum_hub = mock_hub_registry.get_hub('quantum')

        # Configure quantum enhancement
        quantum_hub.enhance_consciousness = AsyncMock(return_value={
            'attention_analysis': {
                'focus_level': 0.85,
                'quantum_coherence': 0.72
            }
        })

        # Test consciousness event
        test_event = {
            'agent_id': 'test_agent',
            'event_type': 'awareness_query',
            'event_data': {
                'query': 'Test quantum consciousness',
                'attention_level': 0.8
            }
        }

        start = time.time()

        # Process through consciousness
        consciousness_result = await consciousness_hub.process_event(
            test_event['event_type'],
            test_event['event_data']
        )
        assert consciousness_result['processed'] == True
        metrics.operations += 1

        # Enhance with quantum
        quantum_result = await quantum_hub.enhance_consciousness(test_event['event_data'])
        assert 'attention_analysis' in quantum_result
        assert quantum_result['attention_analysis']['focus_level'] > 0.8
        metrics.operations += 1

        # Verify bridge communication
        quantum_bridge = mock_bridge_registry.get_bridge('consciousness_quantum')
        bridge_result = await quantum_bridge.consciousness_to_quantum('enhance', test_event)
        assert bridge_result['forwarded'] == True
        metrics.operations += 1

        end = time.time()
        metrics.latencies.append((end - start) * 1000)

        assert metrics.errors == 0
        assert metrics.avg_latency < 50  # Quantum processing should be fast

    @pytest.mark.asyncio
    async def test_memory_learning_feedback_loop(self, mock_hub_registry, metrics):
        """Test memory-learning feedback integration"""
        memory_hub = mock_hub_registry.get_hub('memory')
        learning_hub = mock_hub_registry.get_hub('learning')

        # Configure feedback loop
        memory_hub.register_learning_feedback = Mock()
        learning_hub.process_memory_feedback = AsyncMock(return_value={
            'insights': ['pattern_detected', 'optimization_possible']
        })

        # Test feedback registration
        feedback_event = {
            'memory_id': 'mem_123',
            'event_type': 'access_pattern',
            'data': {
                'access_count': 10,
                'time_pattern': 'periodic',
                'importance_score': 0.8
            }
        }

        start = time.time()

        # Register feedback
        memory_hub.register_learning_feedback('memory_access', feedback_event)
        metrics.operations += 1

        # Process in learning system
        learning_result = await learning_hub.process_memory_feedback(feedback_event)
        assert 'insights' in learning_result
        assert len(learning_result['insights']) > 0
        metrics.operations += 1

        end = time.time()
        metrics.latencies.append((end - start) * 1000)

        assert metrics.errors == 0

    @pytest.mark.asyncio
    async def test_system_wide_health_monitoring(self, mock_hub_registry,
                                               mock_bridge_registry,
                                               mock_service_discovery, metrics):
        """Test comprehensive system health monitoring"""
        start = time.time()

        # Check all hubs
        hub_health = await mock_hub_registry.health_check_all()
        assert len(hub_health) == 11  # All hubs
        metrics.operations += len(hub_health)

        # Check all bridges
        bridge_health = await mock_bridge_registry.health_check_all()
        assert len(bridge_health) == 6  # All bridges
        metrics.operations += len(bridge_health)

        # Check key services
        key_services = ['ai_interface', 'quantum_consciousness_hub', 'memory_manager']
        service_health = {}

        for service in key_services:
            health = await mock_service_discovery.health_check_service(service)
            service_health[service] = health
            assert health['status'] == 'available'
            metrics.operations += 1

        # Calculate overall health
        healthy_hubs = sum(1 for h in hub_health.values() if h['status'] == 'healthy')
        healthy_bridges = sum(1 for b in bridge_health.values() if b['connected'])
        healthy_services = sum(1 for s in service_health.values() if s['status'] == 'available')

        overall_health = {
            'hubs': f"{healthy_hubs}/{len(hub_health)}",
            'bridges': f"{healthy_bridges}/{len(bridge_health)}",
            'services': f"{healthy_services}/{len(service_health)}",
            'overall': 'healthy' if (healthy_hubs == len(hub_health) and
                                   healthy_bridges == len(bridge_health)) else 'degraded'
        }

        assert overall_health['overall'] == 'healthy'

        end = time.time()
        metrics.latencies.append((end - start) * 1000)
        metrics.end_time = end

        # Performance check
        assert metrics.avg_latency < 200  # Health checks should be fast
        assert metrics.success_rate == 100

    @pytest.mark.asyncio
    async def test_error_recovery_scenario(self, mock_hub_registry, mock_bridge_registry, metrics):
        """Test system behavior under error conditions"""
        # Simulate hub failure
        failing_hub = mock_hub_registry.get_hub('quantum')
        failing_hub.process_event = AsyncMock(side_effect=Exception("Hub failure"))

        # Try to process event
        try:
            await failing_hub.process_event('test', {})
            metrics.operations += 1
        except Exception:
            metrics.errors += 1

        # System should handle gracefully
        assert metrics.errors == 1

        # Other hubs should still work
        core_hub = mock_hub_registry.get_hub('core')
        result = await core_hub.process_event('test', {})
        assert result['processed'] == True
        metrics.operations += 1

        # Bridges should detect and handle
        quantum_bridge = mock_bridge_registry.get_bridge('consciousness_quantum')
        quantum_bridge.consciousness_to_quantum = AsyncMock(return_value={
            'forwarded': False,
            'error': 'quantum_hub_unavailable'
        })

        bridge_result = await quantum_bridge.consciousness_to_quantum('test', {})
        assert bridge_result['forwarded'] == False
        assert 'error' in bridge_result
        metrics.operations += 1

        # System should still be operational
        overall_health = await mock_hub_registry.health_check_all()
        operational_hubs = sum(1 for h in overall_health.values()
                             if h['status'] == 'healthy')
        assert operational_hubs >= 10  # Only quantum failed

        metrics.end_time = time.time()
        # Success rate should be reasonable despite error
        assert metrics.success_rate >= 66  # 2/3 operations succeeded

    @pytest.mark.asyncio
    async def test_performance_under_load(self, mock_hub_registry, metrics):
        """Test system performance under load"""
        core_hub = mock_hub_registry.get_hub('core')

        # Configure realistic processing delay
        async def delayed_process(*args, **kwargs):
            await asyncio.sleep(0.005)  # 5ms processing time
            return {'processed': True}

        core_hub.process_event = delayed_process

        # Run concurrent operations
        tasks = []
        num_operations = 50

        start = time.time()

        for i in range(num_operations):
            task = asyncio.create_task(
                core_hub.process_event('load_test', {'index': i})
            )
            tasks.append(task)

        # Wait for all to complete
        results = await asyncio.gather(*tasks, return_exceptions=True)

        end = time.time()
        total_time = (end - start) * 1000  # Convert to ms

        # Count successes
        successes = sum(1 for r in results if isinstance(r, dict) and r.get('processed'))
        metrics.operations = num_operations
        metrics.errors = num_operations - successes

        # Performance assertions
        assert successes >= num_operations * 0.95  # 95% success rate
        assert total_time < num_operations * 10  # Should benefit from concurrency

        # Calculate throughput
        throughput = num_operations / (total_time / 1000)  # ops/second
        assert throughput > 10  # At least 10 ops/second

        metrics.end_time = end
        print(f"Load test: {throughput:.2f} ops/sec, {metrics.success_rate:.1f}% success")


class TestIntegrationReportGenerator:
    """Generate comprehensive test report"""

    @staticmethod
    def generate_report(test_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Generate test summary report"""
        total_tests = len(test_results)
        passed = sum(1 for r in test_results if r['passed'])
        failed = total_tests - passed

        return {
            'summary': {
                'total_tests': total_tests,
                'passed': passed,
                'failed': failed,
                'success_rate': (passed / total_tests * 100) if total_tests > 0 else 0,
                'timestamp': datetime.now().isoformat()
            },
            'test_results': test_results,
            'recommendations': TestIntegrationReportGenerator.get_recommendations(test_results)
        }

    @staticmethod
    def get_recommendations(test_results: List[Dict[str, Any]]) -> List[str]:
        """Generate recommendations based on test results"""
        recommendations = []

        # Check for performance issues
        slow_tests = [r for r in test_results if r.get('duration', 0) > 1.0]
        if slow_tests:
            recommendations.append(f"Optimize {len(slow_tests)} slow tests")

        # Check for failures
        failures = [r for r in test_results if not r['passed']]
        if failures:
            recommendations.append(f"Fix {len(failures)} failing tests")

        # Check for missing coverage
        if len(test_results) < 20:
            recommendations.append("Add more integration tests for better coverage")

        return recommendations


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])