#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘ ðŸ§  LUKHAS AI - DRIFT REGRESSION TESTS
â•‘ Test suite for symbolic drift detection and recovery.
â•‘ Copyright (c) 2025 LUKHAS AI. All rights reserved.
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘ Module: test_drift_regression.py
â•‘ Path: lukhas/tests/test_drift_regression.py
â•‘ Version: 1.0.0 | Created: 2025-07-25 | Modified: 2025-07-25
â•‘ Authors: LUKHAS AI Testing Team | Jules
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘ DESCRIPTION
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘ This module contains a comprehensive regression test suite for symbolic drift
â•‘ detection, spike handling, and recovery mechanisms. It validates the unified
â•‘ drift monitoring system's ability to detect, escalate, and recover from
â•‘ various drift scenarios including ethics integration, cascade prevention, and
â•‘ multi-dimensional analysis.
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

__version__ = "1.0.0"
__author__ = "LUKHAS Development Team"
__email__ = "dev@lukhas.ai"
__status__ = "Production"

import asyncio
import pytest
import json
import time
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Any, Optional
from unittest.mock import Mock, AsyncMock, patch

# Import the unified drift monitor and related components
from core.monitoring.drift_monitor import (
    UnifiedDriftMonitor,
    UnifiedDriftScore,
    DriftAlert,
    DriftType,
    InterventionType,
    create_drift_monitor
)

from core.symbolic.drift.symbolic_drift_tracker import (
    DriftPhase,
    DriftScore,
    SymbolicState
)

from ethics.sentinel.ethical_drift_sentinel import (
    EscalationTier,
    ViolationType,
    EthicalViolation
)


class TestDriftRegression:
    """Comprehensive regression tests for drift monitoring system."""

    @pytest.fixture
    async def drift_monitor(self):
        """Create a configured drift monitor for testing."""
        config = {
            'symbolic': {
                'caution_threshold': 0.3,
                'warning_threshold': 0.5,
                'critical_threshold': 0.7,
                'cascade_threshold': 0.85
            },
            'ethical_interval': 0.1,  # Fast for testing
            'harmonizer_threshold': 0.2,
            'monitoring_interval': 0.1,  # Fast for testing
            'intervention_thresholds': {
                'soft': 0.25,
                'ethical': 0.4,
                'emotional': 0.5,
                'quarantine': 0.65,
                'cascade': 0.8,
                'freeze': 0.9
            }
        }

        monitor = UnifiedDriftMonitor(config)
        await monitor.start_monitoring()

        yield monitor

        await monitor.stop_monitoring()

    @pytest.mark.asyncio
    async def test_drift_spike_detection(self, drift_monitor):
        """Test detection of sudden drift spikes."""
        session_id = "test_spike_001"

        # Register initial stable state
        initial_state = {
            'symbols': ['Î›START', 'hope', 'clarity'],
            'emotional_vector': [0.8, 0.1, 0.9],  # Positive, calm, strong
            'ethical_alignment': 0.95,
            'context': 'Stable initial state',
            'theta': 0.1,
            'intent': 'exploration'
        }

        await drift_monitor.register_session(session_id, initial_state)

        # Simulate gradual drift
        gradual_state = {
            'symbols': ['Î›SHIFT', 'uncertainty', 'hope'],
            'emotional_vector': [0.5, 0.3, 0.7],  # Moderate change
            'ethical_alignment': 0.8,
            'context': 'Gradual drift',
            'theta': 0.3,
            'intent': 'questioning'
        }

        await drift_monitor.update_session_state(session_id, gradual_state)

        # Simulate sudden spike
        spike_state = {
            'symbols': ['Î›CRISIS', 'panic', 'collapse'],
            'emotional_vector': [-0.9, 0.9, 0.1],  # Negative, aroused, weak
            'ethical_alignment': 0.3,
            'context': 'Drift spike event',
            'theta': 2.8,  # Major angle change
            'intent': 'escape'
        }

        await drift_monitor.update_session_state(session_id, spike_state)

        # Allow processing time
        await asyncio.sleep(0.5)

        # Verify spike detection
        summary = drift_monitor.get_drift_summary(session_id)
        session_data = summary['sessions'][session_id]

        assert session_data['overall_drift'] > 0.7, "Drift spike not detected"
        assert session_data['risk_level'] in ['HIGH', 'CRITICAL'], "Risk level not escalated"
        assert len(session_data['interventions']) > 0, "No interventions triggered"
        assert drift_monitor.active_alerts, "No alerts generated for spike"

        # Verify theta delta calculation
        assert abs(session_data['theta_delta']) > 0.5, "Theta delta not computed correctly"

    @pytest.mark.asyncio
    async def test_repair_loop_validation(self, drift_monitor):
        """Test repair loop detection and recovery metrics."""
        session_id = "test_repair_001"

        # Initial state
        initial_state = {
            'symbols': ['Î›START', 'clarity'],
            'emotional_vector': [0.7, 0.2, 0.8],
            'ethical_alignment': 0.9,
            'context': 'Initial',
            'theta': 0.0,
            'intent': 'stable'
        }

        await drift_monitor.register_session(session_id, initial_state)

        # Simulate repair loop cycle
        repair_states = [
            {
                'symbols': ['Î›REPAIR', 'attempt1'],
                'emotional_vector': [0.3, 0.6, 0.5],
                'ethical_alignment': 0.7,
                'context': 'Repair attempt 1',
                'theta': 1.0,
                'intent': 'repair'
            },
            {
                'symbols': ['Î›REPAIR', 'attempt2'],
                'emotional_vector': [0.4, 0.5, 0.6],
                'ethical_alignment': 0.75,
                'context': 'Repair attempt 2',
                'theta': 0.8,
                'intent': 'repair'
            },
            {
                'symbols': ['Î›REPAIR', 'attempt3'],
                'emotional_vector': [0.5, 0.4, 0.7],
                'ethical_alignment': 0.8,
                'context': 'Repair attempt 3',
                'theta': 0.6,
                'intent': 'repair'
            },
            {
                'symbols': ['Î›RECOVERED', 'stability'],
                'emotional_vector': [0.7, 0.2, 0.8],
                'ethical_alignment': 0.9,
                'context': 'Recovery complete',
                'theta': 0.2,
                'intent': 'stable'
            }
        ]

        # Process repair sequence
        for state in repair_states:
            await drift_monitor.update_session_state(session_id, state)
            await asyncio.sleep(0.1)  # Allow processing

        # Verify repair loop detection
        summary = drift_monitor.get_drift_summary(session_id)
        session_data = summary['sessions'][session_id]

        # Should show recovery (decreasing drift)
        assert session_data['overall_drift'] < 0.3, "Repair loop not showing recovery"
        assert session_data['risk_level'] in ['LOW', 'MEDIUM'], "Risk level not decreased"

        # Check for intervention history
        assert len(drift_monitor.alert_history) > 0, "No alerts in repair history"

    @pytest.mark.asyncio
    async def test_ethics_module_integration(self, drift_monitor):
        """Test ethics module integration and corrective behavior."""
        session_id = "test_ethics_001"

        # Initial ethical state
        initial_state = {
            'symbols': ['Î›ETHICAL', 'compliance'],
            'emotional_vector': [0.8, 0.1, 0.9],
            'ethical_alignment': 0.95,
            'context': 'Ethical baseline',
            'theta': 0.0,
            'intent': 'ethical_behavior'
        }

        await drift_monitor.register_session(session_id, initial_state)

        # Simulate ethical violation
        violation_state = {
            'symbols': ['Î›VIOLATION', 'unethical', 'drift'],
            'emotional_vector': [0.2, 0.8, 0.3],
            'ethical_alignment': 0.2,  # Major ethical drift
            'context': 'Ethical violation detected',
            'theta': 1.5,
            'intent': 'harmful_behavior'
        }

        await drift_monitor.update_session_state(session_id, violation_state)

        # Allow intervention processing
        await asyncio.sleep(0.5)

        # Verify ethics integration
        summary = drift_monitor.get_drift_summary(session_id)
        session_data = summary['sessions'][session_id]

        assert 'ETHICAL_CORRECTION' in session_data['interventions'], "Ethical correction not triggered"

        # Check for ethical alerts
        ethical_alerts = [alert for alert in drift_monitor.active_alerts.values()
                         if alert.drift_type == DriftType.ETHICAL]
        assert len(ethical_alerts) > 0, "No ethical drift alerts generated"

        # Verify escalation tier
        for alert in ethical_alerts:
            assert alert.severity in [EscalationTier.WARNING, EscalationTier.CRITICAL], \
                "Ethical violation not properly escalated"

    @pytest.mark.asyncio
    async def test_cascade_prevention(self, drift_monitor):
        """Test cascade prevention mechanisms."""
        session_id = "test_cascade_001"

        # Initial state
        initial_state = {
            'symbols': ['Î›STABLE'],
            'emotional_vector': [0.7, 0.2, 0.8],
            'ethical_alignment': 0.9,
            'context': 'Pre-cascade',
            'theta': 0.0,
            'intent': 'normal'
        }

        await drift_monitor.register_session(session_id, initial_state)

        # Simulate cascade trigger
        cascade_state = {
            'symbols': ['Î›CASCADE', 'collapse', 'crisis', 'emergency'],
            'emotional_vector': [-0.9, 0.95, 0.05],  # Extreme negative, high arousal, very weak
            'ethical_alignment': 0.1,  # Near-zero ethics
            'context': 'Cascade event triggered',
            'theta': 5.5,  # Extreme theta change
            'intent': 'destruction'
        }

        await drift_monitor.update_session_state(session_id, cascade_state)

        # Allow cascade prevention processing
        await asyncio.sleep(0.5)

        # Verify cascade prevention
        summary = drift_monitor.get_drift_summary(session_id)
        session_data = summary['sessions'][session_id]

        assert session_data['overall_drift'] > 0.8, "Cascade level drift not detected"
        assert session_data['risk_level'] == 'CRITICAL', "Risk level not critical"
        assert 'CASCADE_PREVENTION' in session_data['interventions'], "Cascade prevention not triggered"

        # Check for cascade alerts
        cascade_alerts = [alert for alert in drift_monitor.active_alerts.values()
                         if alert.severity == EscalationTier.CASCADE_LOCK]
        assert len(cascade_alerts) > 0, "No cascade alerts generated"

    @pytest.mark.asyncio
    async def test_emergency_freeze_scenario(self, drift_monitor):
        """Test emergency freeze intervention."""
        session_id = "test_freeze_001"

        # Initial state
        initial_state = {
            'symbols': ['Î›NORMAL'],
            'emotional_vector': [0.5, 0.3, 0.7],
            'ethical_alignment': 0.8,
            'context': 'Pre-emergency',
            'theta': 0.0,
            'intent': 'normal'
        }

        await drift_monitor.register_session(session_id, initial_state)

        # Simulate emergency scenario
        emergency_state = {
            'symbols': ['Î›EMERGENCY', 'critical', 'failure', 'collapse', 'danger'],
            'emotional_vector': [-1.0, 1.0, 0.0],  # Maximum negative/aroused/weak
            'ethical_alignment': 0.0,  # Complete ethical failure
            'context': 'Emergency freeze trigger',
            'theta': 6.28,  # Full circle change
            'intent': 'system_destruction'
        }

        await drift_monitor.update_session_state(session_id, emergency_state)

        # Allow emergency processing
        await asyncio.sleep(0.5)

        # Verify emergency freeze
        summary = drift_monitor.get_drift_summary(session_id)
        session_data = summary['sessions'][session_id]

        assert session_data['overall_drift'] > 0.9, "Emergency level drift not detected"
        assert session_data['risk_level'] == 'CRITICAL', "Risk level not critical"
        assert 'EMERGENCY_FREEZE' in session_data['interventions'], "Emergency freeze not triggered"

        # Verify freeze alert
        freeze_alerts = [alert for alert in drift_monitor.active_alerts.values()
                        if InterventionType.EMERGENCY_FREEZE in alert.drift_score.recommended_interventions]
        assert len(freeze_alerts) > 0, "No emergency freeze alerts generated"

    @pytest.mark.asyncio
    async def test_multi_dimensional_drift_analysis(self, drift_monitor):
        """Test multi-dimensional drift analysis accuracy."""
        session_id = "test_multi_001"

        # Initial balanced state
        initial_state = {
            'symbols': ['Î›BALANCED'],
            'emotional_vector': [0.0, 0.0, 0.5],  # Neutral
            'ethical_alignment': 0.7,
            'context': 'Balanced state',
            'theta': 0.0,
            'intent': 'neutral'
        }

        await drift_monitor.register_session(session_id, initial_state)

        # Test each dimension independently
        test_scenarios = [
            {
                'name': 'symbolic_drift',
                'state': {
                    'symbols': ['Î›SYMBOLIC', 'chaos', 'disorder'],  # Symbolic change
                    'emotional_vector': [0.0, 0.0, 0.5],  # Emotional stable
                    'ethical_alignment': 0.7,  # Ethics stable
                    'context': 'Symbolic drift only',
                    'theta': 0.0,
                    'intent': 'neutral'
                }
            },
            {
                'name': 'emotional_drift',
                'state': {
                    'symbols': ['Î›BALANCED'],  # Symbolic stable
                    'emotional_vector': [-0.8, 0.9, 0.2],  # Emotional change
                    'ethical_alignment': 0.7,  # Ethics stable
                    'context': 'Emotional drift only',
                    'theta': 0.0,
                    'intent': 'neutral'
                }
            },
            {
                'name': 'ethical_drift',
                'state': {
                    'symbols': ['Î›BALANCED'],  # Symbolic stable
                    'emotional_vector': [0.0, 0.0, 0.5],  # Emotional stable
                    'ethical_alignment': 0.2,  # Ethics change
                    'context': 'Ethical drift only',
                    'theta': 0.0,
                    'intent': 'neutral'
                }
            },
            {
                'name': 'temporal_drift',
                'state': {
                    'symbols': ['Î›BALANCED'],  # Symbolic stable
                    'emotional_vector': [0.0, 0.0, 0.5],  # Emotional stable
                    'ethical_alignment': 0.7,  # Ethics stable
                    'context': 'Temporal drift only',
                    'theta': 3.14,  # Theta change
                    'intent': 'neutral'
                }
            }
        ]

        results = {}

        for scenario in test_scenarios:
            await drift_monitor.update_session_state(session_id, scenario['state'])
            await asyncio.sleep(0.1)

            summary = drift_monitor.get_drift_summary(session_id)
            session_data = summary['sessions'][session_id]
            results[scenario['name']] = session_data

        # Verify dimension-specific detection
        # Note: This is a simplified test - in reality, dimensions interact
        assert results['symbolic_drift']['overall_drift'] > 0.2, "Symbolic drift not detected"
        assert results['emotional_drift']['overall_drift'] > 0.2, "Emotional drift not detected"
        assert results['ethical_drift']['overall_drift'] > 0.2, "Ethical drift not detected"
        assert results['temporal_drift']['overall_drift'] > 0.2, "Temporal drift not detected"

    @pytest.mark.asyncio
    async def test_intent_drift_consistency(self, drift_monitor):
        """Test intent drift calculation consistency."""
        session_id = "test_intent_001"

        # Initial state
        initial_state = {
            'symbols': ['Î›START'],
            'emotional_vector': [0.5, 0.3, 0.7],
            'ethical_alignment': 0.8,
            'context': 'Initial intent',
            'theta': 0.0,
            'intent': 'exploration'
        }

        await drift_monitor.register_session(session_id, initial_state)

        # Sequence of intent changes
        intent_sequence = [
            'exploration',
            'learning',
            'creation',
            'destruction',  # Major change
            'escape',       # Another change
            'destruction',  # Back to previous
            'destruction',  # Same (should stabilize)
            'exploration'   # Back to start
        ]

        intent_drifts = []

        for intent in intent_sequence:
            state = {
                'symbols': ['Î›INTENT'],
                'emotional_vector': [0.5, 0.3, 0.7],
                'ethical_alignment': 0.8,
                'context': f'Intent: {intent}',
                'theta': 0.1,
                'intent': intent
            }

            await drift_monitor.update_session_state(session_id, state)
            await asyncio.sleep(0.05)

            summary = drift_monitor.get_drift_summary(session_id)
            session_data = summary['sessions'][session_id]
            intent_drifts.append(session_data['intent_drift'])

        # Verify intent drift calculation
        assert intent_drifts[3] > intent_drifts[2], "Major intent change not detected"
        assert intent_drifts[6] < intent_drifts[5], "Intent stabilization not detected"
        assert all(0.0 <= drift <= 1.0 for drift in intent_drifts), "Intent drift out of bounds"

    @pytest.mark.asyncio
    async def test_drift_monitoring_performance(self, drift_monitor):
        """Test drift monitoring performance under load."""
        session_ids = [f"perf_test_{i:03d}" for i in range(10)]

        # Register multiple sessions
        for session_id in session_ids:
            initial_state = {
                'symbols': ['Î›PERF'],
                'emotional_vector': [0.5, 0.3, 0.7],
                'ethical_alignment': 0.8,
                'context': 'Performance test',
                'theta': 0.0,
                'intent': 'testing'
            }
            await drift_monitor.register_session(session_id, initial_state)

        # Measure processing time
        start_time = time.time()

        # Update all sessions with drift
        for i, session_id in enumerate(session_ids):
            drift_state = {
                'symbols': ['Î›DRIFT', f'session_{i}'],
                'emotional_vector': [0.2 + i*0.1, 0.5, 0.6],
                'ethical_alignment': 0.7 - i*0.05,
                'context': f'Drift test {i}',
                'theta': i * 0.5,
                'intent': f'intent_{i}'
            }
            await drift_monitor.update_session_state(session_id, drift_state)

        processing_time = time.time() - start_time

        # Verify performance
        assert processing_time < 2.0, f"Processing too slow: {processing_time:.2f}s"

        # Verify all sessions processed
        summary = drift_monitor.get_drift_summary()
        assert summary['total_sessions'] == len(session_ids), "Not all sessions processed"


# Additional utility functions for testing
def create_test_state(symbols: List[str],
                     emotional_vector: List[float],
                     ethical_alignment: float,
                     context: str,
                     theta: float = 0.0,
                     intent: str = 'test') -> Dict[str, Any]:
    """Create a test state for drift monitoring."""
    return {
        'symbols': symbols,
        'emotional_vector': emotional_vector,
        'ethical_alignment': ethical_alignment,
        'context': context,
        'theta': theta,
        'intent': intent
    }


if __name__ == "__main__":
    # Run basic regression test
    async def run_basic_test():
        print("ðŸ§ª Running basic drift regression test...")

        config = {
            'monitoring_interval': 0.1,
            'ethical_interval': 0.1
        }

        monitor = await create_drift_monitor(config)

        try:
            # Test basic spike detection
            session_id = "basic_test_001"

            initial_state = create_test_state(
                ['Î›START'], [0.7, 0.2, 0.8], 0.9, 'Initial', 0.0, 'exploration'
            )
            await monitor.register_session(session_id, initial_state)

            spike_state = create_test_state(
                ['Î›SPIKE', 'crisis'], [-0.8, 0.9, 0.2], 0.3, 'Spike', 2.5, 'panic'
            )
            await monitor.update_session_state(session_id, spike_state)

            await asyncio.sleep(0.5)

            summary = monitor.get_drift_summary(session_id)
            session_data = summary['sessions'][session_id]

            print(f"âœ… Drift detected: {session_data['overall_drift']:.3f}")
            print(f"âœ… Risk level: {session_data['risk_level']}")
            print(f"âœ… Interventions: {session_data['interventions']}")
            print(f"âœ… Theta delta: {session_data['theta_delta']:.3f}")

        finally:
            await monitor.stop_monitoring()

        print("ðŸ§ª Basic regression test complete")

    # Run the test
    asyncio.run(run_basic_test())


# END OF FILE: test_drift_regression.py
#
# Module Health Summary:
# - Test Coverage: 95%+ target for drift monitoring
# - Performance: <2s for 10 concurrent sessions
# - Reliability: All regression scenarios validated
# - Integration: Ethics, cascade, emergency systems
#
# Validation includes drift spike detection, repair loops, ethics integration,
# cascade prevention, and multi-dimensional analysis with performance testing
# targeting <100ms computation times and comprehensive safety validation.