"""
Memory Profiler Mock Implementation
Provides lightweight memory profiling functionality without PyTorch dependencies
"""

import logging
from typing import Dict, Any, Optional, List, Tuple
from datetime import datetime
from enum import Enum, auto
from dataclasses import dataclass, field

logger = logging.getLogger(__name__)


class Category(Enum):
    """Memory allocation categories"""
    INPUT = auto()
    TEMPORARY = auto()
    ACTIVATION = auto()
    GRADIENT = auto()
    AUTOGRAD_DETAIL = auto()
    PARAMETER = auto()
    OPTIMIZER_STATE = auto()


class Action(Enum):
    """Memory action types"""
    CREATE = auto()
    DELETE = auto()
    UPDATE = auto()


@dataclass
class MemoryEvent:
    """Represents a memory event"""
    timestamp: datetime
    action: Action
    tensor_id: str
    size: int = 0
    category: Optional[Category] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


class MemoryProfiler:
    """Lightweight memory profiler without PyTorch dependencies"""

    def __init__(self):
        self.memory_events: List[MemoryEvent] = []
        self.tensor_registry: Dict[str, Dict[str, Any]] = {}
        self.category_stats: Dict[Category, Dict[str, Any]] = {
            cat: {"count": 0, "total_size": 0, "current_size": 0}
            for cat in Category
        }
        self.peak_memory_usage = 0
        self.current_memory_usage = 0
        logger.info("MemoryProfiler (mock) initialized")

    def record_allocation(self, tensor_id: str, size: int, category: Optional[Category] = None) -> None:
        """Record a memory allocation event"""
        if category is None:
            category = Category.TEMPORARY

        event = MemoryEvent(
            timestamp=datetime.now(),
            action=Action.CREATE,
            tensor_id=tensor_id,
            size=size,
            category=category
        )

        self.memory_events.append(event)
        self.tensor_registry[tensor_id] = {
            "size": size,
            "category": category,
            "allocated_at": event.timestamp
        }

        # Update statistics
        self.category_stats[category]["count"] += 1
        self.category_stats[category]["total_size"] += size
        self.category_stats[category]["current_size"] += size

        self.current_memory_usage += size
        self.peak_memory_usage = max(self.peak_memory_usage, self.current_memory_usage)

        logger.debug(f"Recorded allocation: {tensor_id} ({size} bytes, {category.name})")

    def record_deallocation(self, tensor_id: str) -> None:
        """Record a memory deallocation event"""
        if tensor_id not in self.tensor_registry:
            logger.warning(f"Attempting to deallocate unknown tensor: {tensor_id}")
            return

        tensor_info = self.tensor_registry[tensor_id]
        size = tensor_info["size"]
        category = tensor_info["category"]

        event = MemoryEvent(
            timestamp=datetime.now(),
            action=Action.DELETE,
            tensor_id=tensor_id,
            size=size,
            category=category
        )

        self.memory_events.append(event)
        del self.tensor_registry[tensor_id]

        # Update statistics
        self.category_stats[category]["current_size"] -= size
        self.current_memory_usage -= size

        logger.debug(f"Recorded deallocation: {tensor_id} ({size} bytes)")

    def get_memory_usage_by_category(self) -> Dict[str, Dict[str, Any]]:
        """Get memory usage statistics by category"""
        total_current = sum(stats["current_size"] for stats in self.category_stats.values())

        return {
            cat.name: {
                "count": stats["count"],
                "total_allocated_mb": stats["total_size"] / (1024 * 1024),
                "current_size_mb": stats["current_size"] / (1024 * 1024),
                "percentage": (stats["current_size"] / total_current * 100) if total_current > 0 else 0
            }
            for cat, stats in self.category_stats.items()
        }

    def get_memory_timeline(self, limit: int = 100) -> List[Dict[str, Any]]:
        """Get timeline of memory events"""
        events = []
        for event in self.memory_events[-limit:]:
            events.append({
                "timestamp": event.timestamp.isoformat(),
                "action": event.action.name,
                "tensor_id": event.tensor_id,
                "size": event.size,
                "category": event.category.name if event.category else None
            })
        return events

    def get_active_tensors(self) -> Dict[str, Dict[str, Any]]:
        """Get information about currently allocated tensors"""
        return {
            tensor_id: {
                "size_mb": info["size"] / (1024 * 1024),
                "category": info["category"].name,
                "lifetime_seconds": (datetime.now() - info["allocated_at"]).total_seconds()
            }
            for tensor_id, info in self.tensor_registry.items()
        }

    def analyze_memory_patterns(self) -> Dict[str, Any]:
        """Analyze memory usage patterns"""
        analysis = {
            "total_allocations": sum(1 for e in self.memory_events if e.action == Action.CREATE),
            "total_deallocations": sum(1 for e in self.memory_events if e.action == Action.DELETE),
            "active_tensors": len(self.tensor_registry),
            "current_memory_usage_mb": self.current_memory_usage / (1024 * 1024),
            "peak_memory_usage_mb": self.peak_memory_usage / (1024 * 1024),
            "memory_efficiency": (1 - (self.current_memory_usage / self.peak_memory_usage)) * 100 if self.peak_memory_usage > 0 else 100,
            "category_distribution": self.get_memory_usage_by_category(),
            "recommendations": []
        }

        # Memory leak detection
        leak_ratio = analysis["total_deallocations"] / analysis["total_allocations"] if analysis["total_allocations"] > 0 else 1
        if leak_ratio < 0.8:
            analysis["recommendations"].append(
                f"Potential memory leak detected: Only {leak_ratio:.1%} of allocations have been freed"
            )

        # Category analysis
        temp_usage = self.category_stats[Category.TEMPORARY]["current_size"]
        total_usage = self.current_memory_usage

        if total_usage > 0 and temp_usage / total_usage > 0.5:
            analysis["recommendations"].append(
                f"High temporary memory usage ({temp_usage / total_usage:.1%}) - consider optimization"
            )

        # Long-lived tensors
        long_lived = []
        for tensor_id, info in self.tensor_registry.items():
            lifetime = (datetime.now() - info["allocated_at"]).total_seconds()
            if lifetime > 300:  # 5 minutes
                long_lived.append({
                    "tensor_id": tensor_id,
                    "lifetime_minutes": lifetime / 60,
                    "size_mb": info["size"] / (1024 * 1024)
                })

        if long_lived:
            analysis["long_lived_tensors"] = sorted(long_lived, key=lambda x: x["lifetime_minutes"], reverse=True)[:10]
            analysis["recommendations"].append(
                f"Found {len(long_lived)} long-lived tensors - review for potential cleanup"
            )

        logger.info(f"Memory analysis complete: {analysis['total_allocations']} allocations, "
                   f"peak {analysis['peak_memory_usage_mb']:.2f} MB, "
                   f"efficiency {analysis['memory_efficiency']:.1f}%")

        return analysis

    def reset_profiler(self) -> None:
        """Reset profiler state"""
        self.memory_events.clear()
        self.tensor_registry.clear()
        self.category_stats = {
            cat: {"count": 0, "total_size": 0, "current_size": 0}
            for cat in Category
        }
        self.peak_memory_usage = 0
        self.current_memory_usage = 0
        logger.info("Memory profiler reset")


def get_memory_profiler() -> Optional[MemoryProfiler]:
    """Factory function to create memory profiler"""
    try:
        return MemoryProfiler()
    except Exception as e:
        logger.error(f"Failed to create memory profiler: {e}")
        return None