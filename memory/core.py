"""
Consolidated module for better performance
"""

from aiohttp import web
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from datetime import datetime, timezone
from enum import Enum
from hybrid_memory_fold import HybridMemoryFold, VectorStorageLayer, MemoryAttentionLayer, ContinuousLearningEngine
from lazy_loading_embeddings import LazyEmbeddingLoader, LazyMemoryItem, create_lazy_embedding_system
from memory.structural_conscience import create_structural_conscience
from memory_fold_system import MemoryFoldSystem, MemoryItem, TagInfo
from memory_fold_system import MemoryItem
from optimized_hybrid_memory_fold import OptimizedHybridMemoryFold
from optimized_memory_item import OptimizedMemoryItem, create_optimized_memory, convert_from_legacy, convert_to_legacy
from optimized_memory_item import QuantizationCodec
from pathlib import Path
from typing import Dict, List, Optional, Any, Set, Tuple
from typing import Dict, List, Optional, Tuple, Any, Union
import aiofiles
import aiohttp
import asyncio
import hashlib
import json
import math
import numpy
import random
import socket
import string
import structlog
import time
import torch
import torch.nn
import torch.nn.functional


def create_hybrid_memory_fold(embedding_dim: int=1024, enable_attention: bool=True, enable_continuous_learning: bool=True, enable_conscience: bool=True, **kwargs) -> HybridMemoryFold:
    """
    Create an AGI-ready hybrid memory fold system.

    Args:
        embedding_dim: Dimension of vector embeddings
        enable_attention: Enable attention mechanisms
        enable_continuous_learning: Enable adaptive learning
        enable_conscience: Enable structural conscience
        **kwargs: Additional arguments for base system

    Returns:
        Configured HybridMemoryFold instance
    """
    if enable_conscience:
        from memory.structural_conscience import create_structural_conscience
        conscience = create_structural_conscience()
        kwargs['structural_conscience'] = conscience
    return HybridMemoryFold(embedding_dim=embedding_dim, enable_attention=enable_attention, enable_continuous_learning=enable_continuous_learning, **kwargs)

def create_optimized_hybrid_memory_fold_with_lazy_loading(embedding_dim: int=1024, enable_attention: bool=True, enable_continuous_learning: bool=True, enable_quantization: bool=True, enable_compression: bool=True, enable_conscience: bool=True, lazy_loading_cache_size: int=10000, lazy_loading_cache_memory_mb: int=512, lazy_loading_storage_path: Optional[str]=None, **kwargs) -> OptimizedHybridMemoryFold:
    """
    Create ultra-optimized hybrid memory fold with lazy loading for massive scalability.

    Perfect for:
    - Very large memory collections (millions of memories)
    - Resource-constrained environments
    - Systems with limited RAM
    - Distributed memory architectures

    Args:
        embedding_dim: Dimension of vector embeddings (512 or 1024 supported)
        enable_attention: Enable attention mechanisms
        enable_continuous_learning: Enable adaptive learning
        enable_quantization: Enable embedding quantization (75% reduction)
        enable_compression: Enable content compression (50-80% reduction)
        enable_conscience: Enable structural conscience
        lazy_loading_cache_size: Maximum embeddings to cache in memory
        lazy_loading_cache_memory_mb: Maximum memory for embedding cache
        lazy_loading_storage_path: Path for persistent embedding storage
        **kwargs: Additional arguments for base system

    Returns:
        Configured OptimizedHybridMemoryFold with lazy loading enabled

    Note:
        Lazy loading provides virtually unlimited memory capacity by storing
        embeddings on disk and loading them on-demand with intelligent caching.
    """
    from .optimized_memory_item import QuantizationCodec
    if embedding_dim not in QuantizationCodec.SUPPORTED_DIMENSIONS:
        logger.warning(f'Embedding dimension {embedding_dim} not optimal. Supported dimensions: {QuantizationCodec.SUPPORTED_DIMENSIONS}. Using {embedding_dim} anyway.')
    if enable_conscience:
        try:
            from memory.structural_conscience import create_structural_conscience
            conscience = create_structural_conscience()
            kwargs['structural_conscience'] = conscience
        except ImportError:
            logger.warning('Structural conscience not available, continuing without')
    return OptimizedHybridMemoryFold(embedding_dim=embedding_dim, enable_attention=enable_attention, enable_continuous_learning=enable_continuous_learning, enable_quantization=enable_quantization, enable_compression=enable_compression, enable_lazy_loading=True, lazy_loading_cache_size=lazy_loading_cache_size, lazy_loading_cache_memory_mb=lazy_loading_cache_memory_mb, lazy_loading_storage_path=lazy_loading_storage_path, **kwargs)

def create_optimized_hybrid_memory_fold(embedding_dim: int=1024, enable_attention: bool=True, enable_continuous_learning: bool=True, enable_quantization: bool=True, enable_compression: bool=True, enable_conscience: bool=True, **kwargs) -> OptimizedHybridMemoryFold:
    """
    Create an ultra-optimized AGI-ready hybrid memory fold system.

    Args:
        embedding_dim: Dimension of vector embeddings (512 or 1024 supported)
        enable_attention: Enable attention mechanisms
        enable_continuous_learning: Enable adaptive learning
        enable_quantization: Enable embedding quantization (75% reduction)
        enable_compression: Enable content compression (50-80% reduction)
        enable_conscience: Enable structural conscience
        **kwargs: Additional arguments for base system

    Returns:
        Configured OptimizedHybridMemoryFold instance

    Note:
        - 512-dim embeddings: 50% more memory efficient, good for most use cases
        - 1024-dim embeddings: Full semantic richness, best for complex reasoning
    """
    from .optimized_memory_item import QuantizationCodec
    if embedding_dim not in QuantizationCodec.SUPPORTED_DIMENSIONS:
        logger.warning(f'Embedding dimension {embedding_dim} not optimal. Supported dimensions: {QuantizationCodec.SUPPORTED_DIMENSIONS}. Using {embedding_dim} anyway.')
    if enable_conscience:
        try:
            from memory.structural_conscience import create_structural_conscience
            conscience = create_structural_conscience()
            kwargs['structural_conscience'] = conscience
        except ImportError:
            logger.warning('Structural conscience not available, continuing without')
    return OptimizedHybridMemoryFold(embedding_dim=embedding_dim, enable_attention=enable_attention, enable_continuous_learning=enable_continuous_learning, enable_quantization=enable_quantization, enable_compression=enable_compression, **kwargs)

def create_optimized_hybrid_memory_fold_512(enable_attention: bool=True, enable_continuous_learning: bool=True, enable_quantization: bool=True, enable_compression: bool=True, enable_conscience: bool=True, **kwargs) -> OptimizedHybridMemoryFold:
    """
    Create ultra-optimized hybrid memory fold with 512-dimensional embeddings.

    This provides additional 50% memory savings compared to 1024-dim while
    maintaining good semantic quality for most AGI applications.

    Perfect for:
    - Resource-constrained environments
    - Large-scale memory systems
    - Real-time applications
    - Mobile/edge deployment

    Returns:
        Configured OptimizedHybridMemoryFold with 512-dim embeddings
    """
    return create_optimized_hybrid_memory_fold(embedding_dim=512, enable_attention=enable_attention, enable_continuous_learning=enable_continuous_learning, enable_quantization=enable_quantization, enable_compression=enable_compression, enable_conscience=enable_conscience, **kwargs)

@dataclass
class HybridMemoryItem(MemoryItem):
    """Extended memory item with vector embeddings"""
    text_embedding: Optional[np.ndarray] = None
    image_embedding: Optional[np.ndarray] = None
    audio_embedding: Optional[np.ndarray] = None
    unified_embedding: Optional[np.ndarray] = None
    importance_score: float = 1.0
    attention_weights: Dict[str, float] = field(default_factory=dict)
    usage_count: int = 0
    last_used: Optional[datetime] = None
    td_error: Optional[float] = None
    causes: List[str] = field(default_factory=list)
    effects: List[str] = field(default_factory=list)
    causal_strength: Dict[str, float] = field(default_factory=dict)

class VectorStorageLayer:
    """High-performance vector storage with multiple index types"""

    def __init__(self, dimension: int=1024):
        self.dimension = dimension
        self.vectors = {}
        self.vector_matrix = None
        self.id_to_index = {}
        self.index_to_id = {}
        self.next_index = 0

    def add_vector(self, memory_id: str, vector: np.ndarray):
        """Add vector to storage"""
        if memory_id in self.vectors:
            idx = self.id_to_index[memory_id]
            if self.vector_matrix is not None:
                self.vector_matrix[idx] = vector
        else:
            self.vectors[memory_id] = vector
            self.id_to_index[memory_id] = self.next_index
            self.index_to_id[self.next_index] = memory_id
            self.next_index += 1
            self._rebuild_matrix()

    def search_similar(self, query_vector: np.ndarray, top_k: int=10, threshold: float=0.0) -> List[Tuple[str, float]]:
        """Find similar vectors using cosine similarity"""
        if self.vector_matrix is None or len(self.vectors) == 0:
            return []
        query_norm = query_vector / (np.linalg.norm(query_vector) + 1e-08)
        similarities = np.dot(self.vector_matrix, query_norm)
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        results = []
        for idx in top_indices:
            sim = similarities[idx]
            if sim >= threshold:
                memory_id = self.index_to_id[idx]
                results.append((memory_id, float(sim)))
        return results

    def _rebuild_matrix(self):
        """Rebuild vector matrix for efficient search"""
        if not self.vectors:
            self.vector_matrix = None
            return
        vectors = []
        for i in range(self.next_index):
            if i in self.index_to_id:
                memory_id = self.index_to_id[i]
                vector = self.vectors[memory_id]
                norm = np.linalg.norm(vector) + 1e-08
                vectors.append(vector / norm)
        self.vector_matrix = np.array(vectors)

class MemoryAttentionLayer:
    """Attention mechanisms for memory relevance scoring"""

    def __init__(self, hidden_dim: int=1024, num_heads: int=8):
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.head_dim = hidden_dim // num_heads
        if TORCH_AVAILABLE:
            self.attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, dropout=0.1)

    def compute_attention_scores(self, query_embedding: np.ndarray, memory_embeddings: List[np.ndarray], context: Dict[str, Any]) -> List[float]:
        """Compute attention scores for memories given query"""
        if not memory_embeddings:
            return []
        query_norm = query_embedding / (np.linalg.norm(query_embedding) + 1e-08)
        scores = []
        for mem_emb in memory_embeddings:
            mem_norm = mem_emb / (np.linalg.norm(mem_emb) + 1e-08)
            semantic_score = np.dot(query_norm, mem_norm)
            context_boost = 1.0
            if 'temporal_weight' in context:
                context_boost *= context['temporal_weight']
            if 'emotional_weight' in context:
                context_boost *= context['emotional_weight']
            final_score = semantic_score * context_boost
            scores.append(float(final_score))
        if scores:
            max_score = max(scores)
            exp_scores = [math.exp(s - max_score) for s in scores]
            sum_exp = sum(exp_scores)
            scores = [e / sum_exp for e in exp_scores]
        return scores

class ContinuousLearningEngine:
    """Adaptive learning for memory importance and tag weights"""

    def __init__(self, learning_rate: float=0.001):
        self.learning_rate = learning_rate
        self.tag_weights = defaultdict(lambda : 1.0)
        self.tag_usage_stats = defaultdict(lambda : {'success': 0, 'total': 0})
        self.base_decay = 0.99
        self.usage_boost = 1.1
        self.min_weight = 0.1
        self.max_weight = 10.0

    def update_tag_importance(self, tag: str, feedback: float, context: Dict[str, Any]):
        """Update tag weight based on feedback"""
        current_weight = self.tag_weights[tag]
        self.tag_usage_stats[tag]['total'] += 1
        if feedback > 0:
            self.tag_usage_stats[tag]['success'] += 1
        if feedback > 0:
            delta = self.usage_boost ** feedback
            new_weight = current_weight * delta
        else:
            delta = self.base_decay + 0.1 * feedback
            new_weight = current_weight * delta
        self.tag_weights[tag] = max(self.min_weight, min(new_weight, self.max_weight))
        logger.debug('Updated tag weight', tag=tag, old_weight=current_weight, new_weight=self.tag_weights[tag], feedback=feedback)

    def get_tag_importance(self, tag: str) -> float:
        """Get current importance weight for tag"""
        return self.tag_weights[tag]

    def decay_all_weights(self, decay_factor: float=0.99):
        """Apply temporal decay to all tag weights"""
        for tag in list(self.tag_weights.keys()):
            self.tag_weights[tag] *= decay_factor
            if self.tag_weights[tag] < self.min_weight:
                del self.tag_weights[tag]

class HybridMemoryFold(MemoryFoldSystem):
    """
    Enhanced memory fold system with neural-symbolic integration.

    Combines the efficiency of symbolic tags with the semantic
    richness of vector embeddings for AGI-ready memory.
    """

    def __init__(self, embedding_dim: int=1024, enable_attention: bool=True, enable_continuous_learning: bool=True, **kwargs):
        super().__init__(**kwargs)
        self.vector_store = VectorStorageLayer(dimension=embedding_dim)
        self.embedding_dim = embedding_dim
        self.enable_attention = enable_attention
        if enable_attention:
            self.attention_layer = MemoryAttentionLayer(hidden_dim=embedding_dim)
        self.enable_continuous_learning = enable_continuous_learning
        if enable_continuous_learning:
            self.learning_engine = ContinuousLearningEngine()
        self.embedding_cache = {}
        self.causal_graph = defaultdict(lambda : {'causes': [], 'effects': []})
        logger.info('Hybrid Memory Fold initialized', embedding_dim=embedding_dim, attention_enabled=enable_attention, learning_enabled=enable_continuous_learning)

    async def fold_in_with_embedding(self, data: Any, tags: List[str], embedding: Optional[np.ndarray]=None, text_content: Optional[str]=None, image_content: Optional[np.ndarray]=None, audio_content: Optional[np.ndarray]=None, **kwargs) -> str:
        """
        Enhanced fold-in with vector embeddings.

        Args:
            data: Memory content
            tags: Symbolic tags
            embedding: Pre-computed unified embedding
            text_content: Text for embedding generation
            image_content: Image data for embedding
            audio_content: Audio data for embedding
            **kwargs: Additional arguments for base fold_in

        Returns:
            Memory ID
        """
        if embedding is None:
            embedding = await self._generate_embedding(data, text_content, image_content, audio_content)
        memory_id = await super().fold_in(data, tags, **kwargs)
        if embedding is not None:
            self.vector_store.add_vector(memory_id, embedding)
            self.embedding_cache[memory_id] = embedding
        if self.enable_continuous_learning:
            for tag in tags:
                self.learning_engine.update_tag_importance(tag, 0.1, {})
        logger.info('Hybrid memory folded in', memory_id=memory_id, has_embedding=embedding is not None, num_tags=len(tags))
        return memory_id

    async def fold_out_semantic(self, query: Union[str, np.ndarray], top_k: int=10, use_attention: bool=True, combine_with_tags: bool=True, context: Optional[Dict[str, Any]]=None) -> List[Tuple[MemoryItem, float]]:
        """
        Semantic search using vector embeddings.

        Args:
            query: Query text or embedding vector
            top_k: Number of results
            use_attention: Apply attention scoring
            combine_with_tags: Also consider tag matches
            context: Additional context for attention

        Returns:
            List of (memory, score) tuples
        """
        if isinstance(query, str):
            query_embedding = await self._generate_text_embedding(query)
        else:
            query_embedding = query
        vector_results = self.vector_store.search_similar(query_embedding, top_k=top_k * 2 if combine_with_tags else top_k)
        results = []
        for (memory_id, similarity) in vector_results:
            if memory_id not in self.items:
                continue
            memory = self.items[memory_id]
            score = similarity
            if use_attention and self.enable_attention:
                memory_embedding = self.embedding_cache.get(memory_id)
                if memory_embedding is not None:
                    attention_scores = self.attention_layer.compute_attention_scores(query_embedding, [memory_embedding], context or {})
                    if attention_scores:
                        score *= attention_scores[0]
            if self.enable_continuous_learning:
                tag_boost = 1.0
                item_tags = self.item_tags.get(memory_id, set())
                for tag_id in item_tags:
                    tag_name = self.tag_registry.get(tag_id, {}).tag_name
                    if tag_name:
                        importance = self.learning_engine.get_tag_importance(tag_name)
                        tag_boost = max(tag_boost, importance)
                score *= tag_boost
            results.append((memory, score))
        if combine_with_tags and isinstance(query, str):
            query_tags = query.lower().split()
            for tag in query_tags:
                if tag in self.tag_name_index:
                    tag_results = await self.fold_out_by_tag(tag, max_items=top_k)
                    for (tag_memory, _) in tag_results:
                        if not any((m[0].item_id == tag_memory.item_id for m in results)):
                            results.append((tag_memory, 0.5))
        results.sort(key=lambda x: x[1], reverse=True)
        return results[:top_k]

    async def add_causal_link(self, cause_id: str, effect_id: str, strength: float=1.0, evidence: Optional[List[str]]=None):
        """Add causal relationship between memories"""
        if cause_id not in self.items or effect_id not in self.items:
            raise ValueError('Both memories must exist')
        cause_time = self.items[cause_id].timestamp
        effect_time = self.items[effect_id].timestamp
        if cause_time >= effect_time:
            raise ValueError('Cause must precede effect temporally')
        self.causal_graph[cause_id]['effects'].append({'id': effect_id, 'strength': strength, 'evidence': evidence or []})
        self.causal_graph[effect_id]['causes'].append({'id': cause_id, 'strength': strength, 'evidence': evidence or []})
        logger.info('Causal link added', cause=cause_id, effect=effect_id, strength=strength)

    async def trace_causal_chain(self, memory_id: str, direction: str='backward', max_depth: int=5) -> List[List[Tuple[str, float]]]:
        """
        Trace causal chains from a memory.

        Args:
            memory_id: Starting memory
            direction: "backward" (causes) or "forward" (effects)
            max_depth: Maximum chain depth

        Returns:
            List of causal paths, each path is list of (memory_id, strength) tuples
        """
        paths = []

        def trace_recursive(current_id, path, cumulative_strength, depth):
            if depth >= max_depth:
                return
            connections = self.causal_graph[current_id]
            links = connections['causes'] if direction == 'backward' else connections['effects']
            if not links:
                if len(path) > 1:
                    paths.append(path.copy())
            else:
                for link in links:
                    next_id = link['id']
                    strength = link['strength']
                    if any((p[0] == next_id for p in path)):
                        continue
                    new_path = path + [(next_id, cumulative_strength * strength)]
                    trace_recursive(next_id, new_path, cumulative_strength * strength, depth + 1)
        trace_recursive(memory_id, [(memory_id, 1.0)], 1.0, 0)
        paths.sort(key=lambda p: p[-1][1], reverse=True)
        return paths

    async def update_memory_importance(self, memory_id: str, feedback: float, context: Optional[Dict[str, Any]]=None):
        """Update memory importance based on usage feedback"""
        if memory_id not in self.items:
            return
        memory = self.items[memory_id]
        memory.access_count += 1
        memory.last_accessed = datetime.now(timezone.utc)
        if self.enable_continuous_learning:
            item_tags = self.item_tags.get(memory_id, set())
            for tag_id in item_tags:
                tag_info = self.tag_registry.get(tag_id)
                if tag_info:
                    self.learning_engine.update_tag_importance(tag_info.tag_name, feedback, context or {})
        logger.debug('Updated memory importance', memory_id=memory_id, feedback=feedback, access_count=memory.access_count)

    async def _generate_embedding(self, data: Any, text: Optional[str]=None, image: Optional[np.ndarray]=None, audio: Optional[np.ndarray]=None) -> np.ndarray:
        """Generate unified embedding from multi-modal inputs"""
        embeddings = []
        if text or isinstance(data, str):
            text_content = text or str(data)
            text_emb = await self._generate_text_embedding(text_content)
            embeddings.append(text_emb)
        if image is not None:
            image_emb = await self._generate_image_embedding(image)
            embeddings.append(image_emb)
        if audio is not None:
            audio_emb = await self._generate_audio_embedding(audio)
            embeddings.append(audio_emb)
        if not embeddings:
            return np.random.randn(self.embedding_dim).astype(np.float32)
        elif len(embeddings) == 1:
            return embeddings[0]
        else:
            return np.mean(embeddings, axis=0)

    async def _generate_text_embedding(self, text: str) -> np.ndarray:
        """Generate text embedding (stub - would use sentence-transformers)"""
        text_hash = hashlib.sha256(text.encode()).digest()
        embedding = np.frombuffer(text_hash, dtype=np.uint8).astype(np.float32)
        if len(embedding) < self.embedding_dim:
            repeats = self.embedding_dim // len(embedding) + 1
            embedding = np.tile(embedding, repeats)[:self.embedding_dim]
        else:
            embedding = embedding[:self.embedding_dim]
        embedding = embedding / 255.0 - 0.5
        norm = np.linalg.norm(embedding)
        if norm > 0:
            embedding = embedding / norm
        return embedding

    async def _generate_image_embedding(self, image: np.ndarray) -> np.ndarray:
        """Generate image embedding (stub - would use CLIP/ViT)"""
        return np.random.randn(self.embedding_dim).astype(np.float32)

    async def _generate_audio_embedding(self, audio: np.ndarray) -> np.ndarray:
        """Generate audio embedding (stub - would use wav2vec2)"""
        return np.random.randn(self.embedding_dim).astype(np.float32)

    def get_enhanced_statistics(self) -> Dict[str, Any]:
        """Get statistics including vector and learning metrics"""
        base_stats = super().get_statistics()
        base_stats['vector_stats'] = {'total_vectors': len(self.vector_store.vectors), 'embedding_dim': self.embedding_dim, 'cache_size': len(self.embedding_cache)}
        if self.enable_continuous_learning:
            base_stats['learning_stats'] = {'total_tag_weights': len(self.learning_engine.tag_weights), 'avg_tag_weight': np.mean(list(self.learning_engine.tag_weights.values())) if self.learning_engine.tag_weights else 0, 'most_important_tags': sorted(self.learning_engine.tag_weights.items(), key=lambda x: x[1], reverse=True)[:10]}
        base_stats['causal_stats'] = {'memories_with_causes': sum((1 for m in self.causal_graph.values() if m['causes'])), 'memories_with_effects': sum((1 for m in self.causal_graph.values() if m['effects'])), 'total_causal_links': sum((len(m['causes']) + len(m['effects']) for m in self.causal_graph.values())) // 2}
        return base_stats

class OptimizedVectorStorageLayer(VectorStorageLayer):
    """Optimized vector storage with quantized embeddings"""

    def __init__(self, dimension: int=1024, enable_quantization: bool=True):
        super().__init__(dimension)
        self.enable_quantization = enable_quantization
        self.memory_usage_bytes = 0

    def add_vector(self, memory_id: str, vector: np.ndarray):
        """Add vector with optional quantization tracking"""
        super().add_vector(memory_id, vector)
        if self.enable_quantization:
            vector_size = len(vector) + 4
        else:
            vector_size = len(vector) * 4
        self.memory_usage_bytes += vector_size
        logger.debug('Vector added to optimized storage', memory_id=memory_id, vector_size_bytes=vector_size, total_usage_mb=self.memory_usage_bytes / (1024 * 1024), quantized=self.enable_quantization)

    def get_memory_usage_stats(self) -> Dict[str, Any]:
        """Get detailed memory usage statistics"""
        num_vectors = len(self.vectors)
        avg_size_per_vector = self.memory_usage_bytes / num_vectors if num_vectors > 0 else 0
        return {'total_vectors': num_vectors, 'total_memory_bytes': self.memory_usage_bytes, 'total_memory_mb': self.memory_usage_bytes / (1024 * 1024), 'avg_bytes_per_vector': avg_size_per_vector, 'quantization_enabled': self.enable_quantization, 'compression_ratio': self.dimension * 4 / avg_size_per_vector if avg_size_per_vector > 0 else 1.0}

class OptimizedHybridMemoryFold(HybridMemoryFold):
    """
    Ultra-optimized hybrid memory fold with 16x memory reduction.

    Maintains full API compatibility with HybridMemoryFold while
    achieving massive memory efficiency improvements.
    """

    def __init__(self, embedding_dim: int=1024, enable_attention: bool=True, enable_continuous_learning: bool=True, enable_quantization: bool=True, enable_compression: bool=True, enable_lazy_loading: bool=False, lazy_loading_cache_size: int=10000, lazy_loading_cache_memory_mb: int=512, lazy_loading_storage_path: Optional[str]=None, **kwargs):
        super().__init__(embedding_dim=embedding_dim, enable_attention=enable_attention, enable_continuous_learning=enable_continuous_learning, **kwargs)
        self.vector_store = OptimizedVectorStorageLayer(dimension=embedding_dim, enable_quantization=enable_quantization)
        self.enable_quantization = enable_quantization
        self.enable_compression = enable_compression
        self.enable_lazy_loading = enable_lazy_loading
        self.lazy_loader: Optional[LazyEmbeddingLoader] = None
        if enable_lazy_loading and create_lazy_embedding_system is not None:
            storage_path = lazy_loading_storage_path or f'./lazy_embeddings_{id(self)}'
            self.lazy_loader = create_lazy_embedding_system(storage_path=storage_path, cache_size=lazy_loading_cache_size, cache_memory_mb=lazy_loading_cache_memory_mb)
            logger.info('Lazy loading enabled', storage_path=storage_path, cache_size=lazy_loading_cache_size, cache_memory_mb=lazy_loading_cache_memory_mb)
        elif enable_lazy_loading:
            logger.warning('Lazy loading requested but not available (missing dependencies)')
            self.enable_lazy_loading = False
        self.total_memory_saved = 0
        self.optimization_stats = {'memories_optimized': 0, 'total_size_before': 0, 'total_size_after': 0, 'compression_ratios': []}
        logger.info('Optimized Hybrid Memory Fold initialized', embedding_dim=embedding_dim, quantization_enabled=enable_quantization, compression_enabled=enable_compression, expected_memory_reduction='16x')

    async def fold_in_with_embedding(self, data: Any, tags: List[str], embedding: Optional[np.ndarray]=None, text_content: Optional[str]=None, image_content: Optional[np.ndarray]=None, audio_content: Optional[np.ndarray]=None, **kwargs) -> str:
        """
        Optimized fold-in using OptimizedMemoryItem.

        Maintains full API compatibility while using optimized storage.
        """
        if embedding is None:
            embedding = await self._generate_embedding(data, text_content, image_content, audio_content)
        memory_id = self._generate_memory_id()
        metadata = {'timestamp': datetime.now(timezone.utc), 'importance': kwargs.get('importance', 0.5), 'access_count': 0, 'type': kwargs.get('memory_type', 'knowledge'), 'emotion': kwargs.get('emotion', 'neutral')}
        if 'collapse_hash' in kwargs:
            metadata['collapse_hash'] = kwargs['collapse_hash']
        if 'drift_score' in kwargs:
            metadata['drift_score'] = kwargs['drift_score']
        legacy_size = self._estimate_legacy_size(data, tags, embedding, metadata)
        optimized_memory = create_optimized_memory(content=str(data), tags=tags, embedding=embedding, metadata=metadata, compress_content=self.enable_compression, quantize_embedding=self.enable_quantization)
        self.items[memory_id] = optimized_memory
        legacy_memory = MemoryItem(item_id=memory_id, data=data, timestamp=metadata['timestamp'], access_count=0, last_accessed=None)
        for tag in tags:
            tag_id = await self._register_tag(tag)
            self.item_tags[memory_id].add(tag_id)
            self.tag_items[tag_id].add(memory_id)
        if embedding is not None:
            self.vector_store.add_vector(memory_id, embedding)
            self.embedding_cache[memory_id] = embedding
        if self.enable_continuous_learning:
            for tag in tags:
                self.learning_engine.update_tag_importance(tag, 0.1, {})
        optimized_size = optimized_memory.memory_usage
        compression_ratio = legacy_size / optimized_size if optimized_size > 0 else 1.0
        self.optimization_stats['memories_optimized'] += 1
        self.optimization_stats['total_size_before'] += legacy_size
        self.optimization_stats['total_size_after'] += optimized_size
        self.optimization_stats['compression_ratios'].append(compression_ratio)
        self.total_memory_saved += legacy_size - optimized_size
        logger.info('Optimized memory folded in', memory_id=memory_id, legacy_size_kb=legacy_size / 1024, optimized_size_kb=optimized_size / 1024, compression_ratio=f'{compression_ratio:.1f}x', total_saved_mb=self.total_memory_saved / (1024 * 1024), has_embedding=embedding is not None)
        return memory_id

    async def fold_out_by_id(self, memory_id: str) -> Optional[MemoryItem]:
        """
        Retrieve memory by ID with automatic format conversion.

        Returns standard MemoryItem for API compatibility.
        """
        if memory_id not in self.items:
            return None
        optimized_memory = self.items[memory_id]
        if isinstance(optimized_memory, OptimizedMemoryItem):
            content = optimized_memory.get_content()
            tags = optimized_memory.get_tags()
            metadata = optimized_memory.get_metadata() or {}
            legacy_memory = MemoryItem(item_id=memory_id, data=content, timestamp=metadata.get('timestamp', datetime.now(timezone.utc)), access_count=metadata.get('access_count', 0), last_accessed=metadata.get('last_accessed'))
            metadata['access_count'] = metadata.get('access_count', 0) + 1
            metadata['last_accessed'] = datetime.now(timezone.utc)
            logger.debug('Memory retrieved and converted', memory_id=memory_id, content_length=len(content), num_tags=len(tags))
            return legacy_memory
        return optimized_memory

    async def fold_out_semantic(self, query: Union[str, np.ndarray], top_k: int=10, use_attention: bool=True, combine_with_tags: bool=True, context: Optional[Dict[str, Any]]=None) -> List[Tuple[MemoryItem, float]]:
        """
        Semantic search with optimized memory retrieval.

        Maintains full API compatibility while using optimized storage.
        """
        if isinstance(query, str):
            query_embedding = await self._generate_text_embedding(query)
        else:
            query_embedding = query
        vector_results = self.vector_store.search_similar(query_embedding, top_k=top_k * 2 if combine_with_tags else top_k)
        results = []
        for (memory_id, similarity) in vector_results:
            if memory_id not in self.items:
                continue
            memory = await self.fold_out_by_id(memory_id)
            if memory is None:
                continue
            score = similarity
            if use_attention and self.enable_attention:
                memory_embedding = self.embedding_cache.get(memory_id)
                if memory_embedding is not None:
                    attention_scores = self.attention_layer.compute_attention_scores(query_embedding, [memory_embedding], context or {})
                    if attention_scores:
                        score *= attention_scores[0]
            if self.enable_continuous_learning:
                tag_boost = 1.0
                item_tags = self.item_tags.get(memory_id, set())
                for tag_id in item_tags:
                    tag_name = self.tag_registry.get(tag_id, {}).tag_name
                    if tag_name:
                        importance = self.learning_engine.get_tag_importance(tag_name)
                        tag_boost = max(tag_boost, importance)
                score *= tag_boost
            results.append((memory, score))
        if combine_with_tags and isinstance(query, str):
            query_tags = query.lower().split()
            for tag in query_tags:
                if tag in self.tag_name_index:
                    tag_results = await self.fold_out_by_tag(tag, max_items=top_k)
                    for (tag_memory, _) in tag_results:
                        if not any((m[0].item_id == tag_memory.item_id for m in results)):
                            results.append((tag_memory, 0.5))
        results.sort(key=lambda x: x[1], reverse=True)
        return results[:top_k]

    def _estimate_legacy_size(self, data: Any, tags: List[str], embedding: Optional[np.ndarray], metadata: Dict[str, Any]) -> int:
        """Estimate size of legacy memory representation"""
        content_size = len(str(data).encode('utf-8'))
        tags_size = sum((len(tag.encode('utf-8')) for tag in tags))
        embedding_size = embedding.nbytes if embedding is not None else 0
        metadata_json = json.dumps(metadata, default=str)
        metadata_size = len(metadata_json.encode('utf-8'))
        python_overhead = 500
        system_overhead = 1000
        return content_size + tags_size + embedding_size + metadata_size + python_overhead + system_overhead

    def get_optimization_statistics(self) -> Dict[str, Any]:
        """Get detailed optimization statistics"""
        stats = self.optimization_stats.copy()
        if stats['memories_optimized'] > 0:
            stats['avg_compression_ratio'] = np.mean(stats['compression_ratios'])
            stats['avg_size_before_kb'] = stats['total_size_before'] / stats['memories_optimized'] / 1024
            stats['avg_size_after_kb'] = stats['total_size_after'] / stats['memories_optimized'] / 1024
            stats['total_memory_saved_mb'] = self.total_memory_saved / (1024 * 1024)
        stats['vector_storage'] = self.vector_store.get_memory_usage_stats()
        if stats['total_size_before'] > 0:
            stats['overall_compression_ratio'] = stats['total_size_before'] / stats['total_size_after']
            stats['memory_efficiency_improvement'] = f"{stats['overall_compression_ratio']:.1f}x"
            stats['storage_capacity_multiplier'] = stats['overall_compression_ratio']
        return stats

    def get_enhanced_statistics(self) -> Dict[str, Any]:
        """Get comprehensive statistics including optimization metrics"""
        base_stats = super().get_enhanced_statistics()
        base_stats['optimization_stats'] = self.get_optimization_statistics()
        if self.optimization_stats['memories_optimized'] > 0:
            avg_optimized_size = self.optimization_stats['total_size_after'] / self.optimization_stats['memories_optimized']
            gb_in_bytes = 1024 * 1024 * 1024
            memories_per_gb = int(gb_in_bytes / avg_optimized_size)
            base_stats['capacity_projections'] = {'avg_memory_size_bytes': avg_optimized_size, 'avg_memory_size_kb': avg_optimized_size / 1024, 'memories_per_gb': memories_per_gb, 'memories_per_10gb': memories_per_gb * 10, 'storage_efficiency': f'{memories_per_gb:,} memories/GB (vs ~2,560 unoptimized)'}
        return base_stats

    async def run_optimization_benchmark(self, num_test_memories: int=100, include_embeddings: bool=True) -> Dict[str, Any]:
        """
        Run optimization benchmark to validate memory savings.

        Args:
            num_test_memories: Number of test memories to create
            include_embeddings: Whether to include embeddings in test

        Returns:
            Benchmark results
        """
        import time
        import random
        import string
        logger.info('Starting optimization benchmark', test_memories=num_test_memories, include_embeddings=include_embeddings)
        test_memories = []
        for i in range(num_test_memories):
            content_length = random.randint(50, 500)
            content = 'Memory content: ' + ''.join(random.choices(string.ascii_letters + string.digits + ' ', k=content_length))
            num_tags = random.randint(2, 8)
            tags = [f'tag_{random.randint(1, 100)}' for _ in range(num_tags)]
            embedding = np.random.randn(self.embedding_dim).astype(np.float32) if include_embeddings else None
            test_memories.append((content, tags, embedding))
        start_time = time.time()
        memory_ids = []
        for (content, tags, embedding) in test_memories:
            memory_id = await self.fold_in_with_embedding(data=content, tags=tags, embedding=embedding)
            memory_ids.append(memory_id)
        insertion_time = time.time() - start_time
        start_time = time.time()
        retrieval_count = 0
        for memory_id in memory_ids:
            memory = await self.fold_out_by_id(memory_id)
            if memory:
                retrieval_count += 1
        retrieval_time = time.time() - start_time
        start_time = time.time()
        search_results = []
        for i in range(min(20, num_test_memories)):
            query = f'test query {i}'
            results = await self.fold_out_semantic(query, top_k=5)
            search_results.extend(results)
        search_time = time.time() - start_time
        opt_stats = self.get_optimization_statistics()
        benchmark_results = {'test_configuration': {'num_memories': num_test_memories, 'include_embeddings': include_embeddings, 'embedding_dim': self.embedding_dim}, 'performance_metrics': {'insertion_time_ms': insertion_time * 1000, 'insertion_rate_per_sec': num_test_memories / insertion_time, 'retrieval_time_ms': retrieval_time * 1000, 'retrieval_rate_per_sec': retrieval_count / retrieval_time, 'search_time_ms': search_time * 1000, 'search_rate_per_sec': 20 / search_time if search_time > 0 else 0}, 'memory_optimization': opt_stats, 'validation': {'all_memories_stored': len(memory_ids) == num_test_memories, 'all_memories_retrieved': retrieval_count == num_test_memories, 'search_returned_results': len(search_results) > 0}}
        logger.info('Optimization benchmark completed', insertion_rate=f"{benchmark_results['performance_metrics']['insertion_rate_per_sec']:.1f}/sec", retrieval_rate=f"{benchmark_results['performance_metrics']['retrieval_rate_per_sec']:.1f}/sec", compression_ratio=f"{opt_stats.get('avg_compression_ratio', 1):.1f}x", memory_saved_mb=f"{opt_stats.get('total_memory_saved_mb', 0):.1f}MB")
        return benchmark_results

class NodeState(Enum):
    """States for distributed memory nodes"""
    FOLLOWER = 'follower'
    CANDIDATE = 'candidate'
    LEADER = 'leader'
    OFFLINE = 'offline'
    RECOVERING = 'recovering'

class MessageType(Enum):
    """Message types for consensus protocol"""
    HEARTBEAT = 'heartbeat'
    VOTE_REQUEST = 'vote_request'
    VOTE_RESPONSE = 'vote_response'
    APPEND_ENTRIES = 'append_entries'
    APPEND_RESPONSE = 'append_response'
    MEMORY_SYNC = 'memory_sync'
    MEMORY_QUERY = 'memory_query'
    MEMORY_RESPONSE = 'memory_response'
    NODE_JOIN = 'node_join'
    NODE_LEAVE = 'node_leave'

@dataclass
class DistributedMemoryEntry:
    """Entry in the distributed memory log"""
    memory_id: str
    content_hash: str
    memory_data: bytes
    embedding_hash: str
    node_id: str
    timestamp: datetime
    term: int
    index: int
    consensus_achieved: bool = False
    validation_votes: Set[str] = field(default_factory=set)

    def to_dict(self) -> Dict[str, Any]:
        return {'memory_id': self.memory_id, 'content_hash': self.content_hash, 'memory_data': self.memory_data.hex(), 'embedding_hash': self.embedding_hash, 'node_id': self.node_id, 'timestamp': self.timestamp.isoformat(), 'term': self.term, 'index': self.index, 'consensus_achieved': self.consensus_achieved, 'validation_votes': list(self.validation_votes)}

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'DistributedMemoryEntry':
        return cls(memory_id=data['memory_id'], content_hash=data['content_hash'], memory_data=bytes.fromhex(data['memory_data']), embedding_hash=data['embedding_hash'], node_id=data['node_id'], timestamp=datetime.fromisoformat(data['timestamp']), term=data['term'], index=data['index'], consensus_achieved=data['consensus_achieved'], validation_votes=set(data['validation_votes']))

@dataclass
class NodeInfo:
    """Information about a node in the distributed network"""
    node_id: str
    address: str
    port: int
    state: NodeState
    last_heartbeat: datetime
    term: int = 0
    vote_count: int = 0
    consciousness_level: float = 0.0
    memory_capacity: int = 0

    @property
    def endpoint(self) -> str:
        return f'http://{self.address}:{self.port}'

    def is_alive(self, timeout_seconds: int=30) -> bool:
        """Check if node is considered alive based on last heartbeat"""
        return (datetime.now() - self.last_heartbeat).total_seconds() < timeout_seconds

class ConsensusProtocol:
    """
    RAFT-based consensus protocol for distributed AGI memory.

    Implements Byzantine fault tolerance with consciousness-aware
    validation for AGI memory networks.
    """

    def __init__(self, node_id: str, port: int, min_nodes_for_consensus: int=3, consciousness_threshold: float=0.7):
        self.node_id = node_id
        self.port = port
        self.min_nodes_for_consensus = min_nodes_for_consensus
        self.consciousness_threshold = consciousness_threshold
        self.current_term = 0
        self.voted_for: Optional[str] = None
        self.state = NodeState.FOLLOWER
        self.leader_id: Optional[str] = None
        self.memory_log: List[DistributedMemoryEntry] = []
        self.commit_index = 0
        self.last_applied = 0
        self.nodes: Dict[str, NodeInfo] = {}
        self.nodes[node_id] = NodeInfo(node_id=node_id, address='localhost', port=port, state=NodeState.FOLLOWER, last_heartbeat=datetime.now())
        self.election_timeout = random.uniform(5.0, 10.0)
        self.heartbeat_interval = 2.0
        self.last_heartbeat_received = datetime.now()
        self.consciousness_level = 0.8
        logger.info('Consensus protocol initialized', node_id=node_id, port=port, min_nodes=min_nodes_for_consensus, consciousness_threshold=consciousness_threshold)

    async def start_node(self):
        """Start the distributed node"""
        asyncio.create_task(self._heartbeat_timer())
        asyncio.create_task(self._election_timer())
        await self._start_http_server()
        logger.info(f'Distributed memory node started', node_id=self.node_id, port=self.port)

    async def _start_http_server(self):
        """Start HTTP server for inter-node communication"""
        from aiohttp import web
        app = web.Application()
        app.router.add_post('/consensus/heartbeat', self._handle_heartbeat)
        app.router.add_post('/consensus/vote_request', self._handle_vote_request)
        app.router.add_post('/consensus/vote_response', self._handle_vote_response)
        app.router.add_post('/consensus/append_entries', self._handle_append_entries)
        app.router.add_post('/memory/sync', self._handle_memory_sync)
        app.router.add_post('/memory/query', self._handle_memory_query)
        app.router.add_post('/node/join', self._handle_node_join)
        runner = web.AppRunner(app)
        await runner.setup()
        site = web.TCPSite(runner, 'localhost', self.port)
        await site.start()

    async def _heartbeat_timer(self):
        """Send heartbeats if leader, check for heartbeats if follower"""
        while True:
            if self.state == NodeState.LEADER:
                await self._send_heartbeats()
            elif self.state == NodeState.FOLLOWER:
                time_since_heartbeat = (datetime.now() - self.last_heartbeat_received).total_seconds()
                if time_since_heartbeat > self.election_timeout:
                    await self._start_election()
            await asyncio.sleep(self.heartbeat_interval)

    async def _election_timer(self):
        """Handle election timeouts"""
        while True:
            await asyncio.sleep(self.election_timeout)
            if self.state == NodeState.CANDIDATE:
                await self._start_election()

    async def _start_election(self):
        """Start leader election process"""
        logger.info('Starting leader election', node_id=self.node_id, term=self.current_term + 1)
        self.state = NodeState.CANDIDATE
        self.current_term += 1
        self.voted_for = self.node_id
        self.nodes[self.node_id].vote_count = 1
        self.election_timeout = random.uniform(5.0, 10.0)
        vote_tasks = []
        for (node_id, node_info) in self.nodes.items():
            if node_id != self.node_id and node_info.is_alive():
                task = asyncio.create_task(self._send_vote_request(node_info))
                vote_tasks.append(task)
        if vote_tasks:
            try:
                await asyncio.wait_for(asyncio.gather(*vote_tasks, return_exceptions=True), timeout=3.0)
            except asyncio.TimeoutError:
                logger.warning('Vote request timeout', node_id=self.node_id)
        alive_nodes = sum((1 for node in self.nodes.values() if node.is_alive()))
        required_votes = alive_nodes // 2 + 1
        if self.nodes[self.node_id].vote_count >= required_votes:
            await self._become_leader()
        else:
            self.state = NodeState.FOLLOWER
            self.voted_for = None

    async def _become_leader(self):
        """Transition to leader state"""
        logger.info('Became leader', node_id=self.node_id, term=self.current_term)
        self.state = NodeState.LEADER
        self.leader_id = self.node_id
        for node in self.nodes.values():
            node.vote_count = 0
        await self._send_heartbeats()

    async def _send_heartbeats(self):
        """Send heartbeat messages to all followers"""
        heartbeat_tasks = []
        for (node_id, node_info) in self.nodes.items():
            if node_id != self.node_id and node_info.is_alive():
                task = asyncio.create_task(self._send_heartbeat(node_info))
                heartbeat_tasks.append(task)
        if heartbeat_tasks:
            await asyncio.gather(*heartbeat_tasks, return_exceptions=True)

    async def _send_heartbeat(self, node_info: NodeInfo):
        """Send heartbeat to specific node"""
        try:
            async with aiohttp.ClientSession() as session:
                payload = {'type': MessageType.HEARTBEAT.value, 'term': self.current_term, 'leader_id': self.node_id, 'commit_index': self.commit_index, 'consciousness_level': self.consciousness_level}
                async with session.post(f'{node_info.endpoint}/consensus/heartbeat', json=payload, timeout=aiohttp.ClientTimeout(total=2.0)) as response:
                    if response.status == 200:
                        node_info.last_heartbeat = datetime.now()
        except Exception as e:
            logger.warning(f'Failed to send heartbeat to {node_info.node_id}', error=str(e))

    async def _send_vote_request(self, node_info: NodeInfo):
        """Send vote request to specific node"""
        try:
            async with aiohttp.ClientSession() as session:
                payload = {'type': MessageType.VOTE_REQUEST.value, 'term': self.current_term, 'candidate_id': self.node_id, 'last_log_index': len(self.memory_log) - 1, 'last_log_term': self.memory_log[-1].term if self.memory_log else 0, 'consciousness_level': self.consciousness_level}
                async with session.post(f'{node_info.endpoint}/consensus/vote_request', json=payload, timeout=aiohttp.ClientTimeout(total=2.0)) as response:
                    if response.status == 200:
                        response_data = await response.json()
                        if response_data.get('vote_granted', False):
                            self.nodes[self.node_id].vote_count += 1
        except Exception as e:
            logger.warning(f'Failed to send vote request to {node_info.node_id}', error=str(e))

    async def _handle_heartbeat(self, request):
        """Handle incoming heartbeat message"""
        data = await request.json()
        term = data['term']
        leader_id = data['leader_id']
        if term > self.current_term:
            self.current_term = term
            self.voted_for = None
            self.state = NodeState.FOLLOWER
        if term == self.current_term:
            self.state = NodeState.FOLLOWER
            self.leader_id = leader_id
            self.last_heartbeat_received = datetime.now()
            if leader_id in self.nodes:
                self.nodes[leader_id].consciousness_level = data.get('consciousness_level', 0.0)
        return aiohttp.web.json_response({'success': True, 'term': self.current_term})

    async def _handle_vote_request(self, request):
        """Handle incoming vote request"""
        data = await request.json()
        term = data['term']
        candidate_id = data['candidate_id']
        candidate_consciousness = data.get('consciousness_level', 0.0)
        vote_granted = False
        if term > self.current_term:
            self.current_term = term
            self.voted_for = None
            self.state = NodeState.FOLLOWER
        if term == self.current_term and (self.voted_for is None or self.voted_for == candidate_id) and (candidate_consciousness >= self.consciousness_threshold):
            self.voted_for = candidate_id
            vote_granted = True
            logger.debug('Vote granted', candidate=candidate_id, term=term)
        return aiohttp.web.json_response({'vote_granted': vote_granted, 'term': self.current_term})

    async def _handle_vote_response(self, request):
        """Handle vote response (not typically called directly)"""
        return aiohttp.web.json_response({'success': True})

    async def _handle_append_entries(self, request):
        """Handle append entries request for log replication"""
        data = await request.json()
        return aiohttp.web.json_response({'success': True, 'term': self.current_term})

    async def _handle_memory_sync(self, request):
        """Handle memory synchronization request"""
        data = await request.json()
        try:
            memory_entry = DistributedMemoryEntry.from_dict(data['memory_entry'])
            if await self._validate_memory_entry(memory_entry):
                self.memory_log.append(memory_entry)
                logger.debug('Memory synchronized', memory_id=memory_entry.memory_id, from_node=memory_entry.node_id)
                return aiohttp.web.json_response({'success': True, 'accepted': True})
            else:
                return aiohttp.web.json_response({'success': True, 'accepted': False})
        except Exception as e:
            logger.error('Memory sync failed', error=str(e))
            return aiohttp.web.json_response({'success': False, 'error': str(e)})

    async def _handle_memory_query(self, request):
        """Handle memory query request"""
        data = await request.json()
        query_id = data.get('query_id')
        matching_memories = []
        for entry in self.memory_log:
            if entry.consensus_achieved:
                matching_memories.append(entry.to_dict())
        return aiohttp.web.json_response({'success': True, 'query_id': query_id, 'memories': matching_memories})

    async def _handle_node_join(self, request):
        """Handle new node joining the network"""
        data = await request.json()
        node_id = data['node_id']
        address = data['address']
        port = data['port']
        consciousness_level = data.get('consciousness_level', 0.0)
        self.nodes[node_id] = NodeInfo(node_id=node_id, address=address, port=port, state=NodeState.FOLLOWER, last_heartbeat=datetime.now(), consciousness_level=consciousness_level)
        logger.info('Node joined network', node_id=node_id, address=address, port=port)
        return aiohttp.web.json_response({'success': True})

    async def _validate_memory_entry(self, entry: DistributedMemoryEntry) -> bool:
        """
        Validate memory entry using Byzantine fault tolerance.

        Implements consciousness-aware validation for AGI memories.
        """
        if not entry.memory_id or not entry.content_hash:
            return False
        calculated_hash = hashlib.sha256(entry.memory_data).hexdigest()
        if calculated_hash != entry.content_hash:
            logger.warning('Memory content hash mismatch', memory_id=entry.memory_id)
            return False
        if entry.node_id in self.nodes:
            node_consciousness = self.nodes[entry.node_id].consciousness_level
            if node_consciousness < self.consciousness_threshold:
                logger.warning('Memory from low-consciousness node rejected', node_id=entry.node_id, consciousness=node_consciousness)
                return False
        return True

class DistributedMemoryFold:
    """
    Distributed memory fold system with consensus protocol.

    Provides distributed AGI memory with Byzantine fault tolerance
    and consciousness-aware validation.
    """

    def __init__(self, node_id: str, port: int, bootstrap_nodes: List[Tuple[str, int]]=None, consciousness_level: float=0.8):
        self.node_id = node_id
        self.port = port
        self.bootstrap_nodes = bootstrap_nodes or []
        self.consciousness_level = consciousness_level
        self.consensus = ConsensusProtocol(node_id=node_id, port=port, consciousness_threshold=0.7)
        self.local_memories: Dict[str, Any] = {}
        self.distributed_memories: Dict[str, DistributedMemoryEntry] = {}
        try:
            from .optimized_hybrid_memory_fold import OptimizedHybridMemoryFold
            self.local_memory_system = OptimizedHybridMemoryFold(embedding_dim=1024, enable_quantization=True, enable_compression=True)
        except ImportError:
            self.local_memory_system = None
            logger.warning('Optimized memory system not available')
        logger.info('Distributed memory fold initialized', node_id=node_id, port=port, bootstrap_nodes=len(bootstrap_nodes), consciousness_level=consciousness_level)

    async def start(self):
        """Start the distributed memory system"""
        await self.consensus.start_node()
        if self.bootstrap_nodes:
            await self._join_network()
        logger.info('Distributed memory fold started', node_id=self.node_id)

    async def _join_network(self):
        """Join existing distributed network"""
        for (address, port) in self.bootstrap_nodes:
            try:
                async with aiohttp.ClientSession() as session:
                    payload = {'node_id': self.node_id, 'address': 'localhost', 'port': self.port, 'consciousness_level': self.consciousness_level}
                    async with session.post(f'http://{address}:{port}/node/join', json=payload, timeout=aiohttp.ClientTimeout(total=5.0)) as response:
                        if response.status == 200:
                            logger.info(f'Successfully joined network via {address}:{port}')
                            bootstrap_node_id = f'{address}:{port}'
                            self.consensus.nodes[bootstrap_node_id] = NodeInfo(node_id=bootstrap_node_id, address=address, port=port, state=NodeState.FOLLOWER, last_heartbeat=datetime.now())
                            break
            except Exception as e:
                logger.warning(f'Failed to join via {address}:{port}', error=str(e))

    async def store_memory(self, content: str, tags: List[str]=None, embedding: np.ndarray=None, metadata: Dict[str, Any]=None, require_consensus: bool=True) -> str:
        """
        Store memory in distributed system with consensus.

        Args:
            content: Memory content
            tags: Memory tags
            embedding: Vector embedding
            metadata: Additional metadata
            require_consensus: Whether to require network consensus

        Returns:
            Memory ID
        """
        if self.local_memory_system:
            memory_id = await self.local_memory_system.fold_in_with_embedding(data=content, tags=tags or [], embedding=embedding, **metadata or {})
        else:
            memory_id = hashlib.sha256(f'{content}{datetime.now().isoformat()}'.encode()).hexdigest()[:16]
        memory_data = json.dumps({'content': content, 'tags': tags or [], 'metadata': metadata or {}, 'embedding': embedding.tolist() if embedding is not None else None}).encode('utf-8')
        distributed_entry = DistributedMemoryEntry(memory_id=memory_id, content_hash=hashlib.sha256(memory_data).hexdigest(), memory_data=memory_data, embedding_hash=hashlib.sha256(embedding.tobytes()).hexdigest() if embedding is not None else '', node_id=self.node_id, timestamp=datetime.now(), term=self.consensus.current_term, index=len(self.consensus.memory_log))
        self.consensus.memory_log.append(distributed_entry)
        self.distributed_memories[memory_id] = distributed_entry
        if require_consensus and self.consensus.state == NodeState.LEADER:
            await self._propagate_memory(distributed_entry)
        logger.debug('Memory stored in distributed system', memory_id=memory_id, require_consensus=require_consensus, is_leader=self.consensus.state == NodeState.LEADER)
        return memory_id

    async def _propagate_memory(self, entry: DistributedMemoryEntry):
        """Propagate memory entry to other nodes"""
        propagation_tasks = []
        for (node_id, node_info) in self.consensus.nodes.items():
            if node_id != self.node_id and node_info.is_alive():
                task = asyncio.create_task(self._send_memory_sync(node_info, entry))
                propagation_tasks.append(task)
        if propagation_tasks:
            results = await asyncio.gather(*propagation_tasks, return_exceptions=True)
            successful_propagations = sum((1 for result in results if not isinstance(result, Exception) and result))
            total_nodes = len(self.consensus.nodes)
            required_acceptance = total_nodes // 2 + 1
            if successful_propagations >= required_acceptance:
                entry.consensus_achieved = True
                logger.info('Memory consensus achieved', memory_id=entry.memory_id, acceptances=successful_propagations, total_nodes=total_nodes)

    async def _send_memory_sync(self, node_info: NodeInfo, entry: DistributedMemoryEntry) -> bool:
        """Send memory sync to specific node"""
        try:
            async with aiohttp.ClientSession() as session:
                payload = {'memory_entry': entry.to_dict()}
                async with session.post(f'{node_info.endpoint}/memory/sync', json=payload, timeout=aiohttp.ClientTimeout(total=5.0)) as response:
                    if response.status == 200:
                        response_data = await response.json()
                        return response_data.get('accepted', False)
            return False
        except Exception as e:
            logger.warning(f'Failed to sync memory to {node_info.node_id}', error=str(e))
            return False

    async def query_memory(self, query: str, top_k: int=10, include_distributed: bool=True) -> List[Dict[str, Any]]:
        """
        Query memories from distributed system.

        Args:
            query: Search query
            top_k: Maximum results to return
            include_distributed: Whether to query other nodes

        Returns:
            List of matching memories
        """
        results = []
        if self.local_memory_system:
            local_results = await self.local_memory_system.fold_out_semantic(query=query, top_k=top_k, use_attention=True)
            for (memory, score) in local_results:
                results.append({'memory': memory, 'score': score, 'source': 'local', 'node_id': self.node_id})
        if include_distributed:
            distributed_results = await self._query_distributed_memories(query, top_k)
            results.extend(distributed_results)
        results.sort(key=lambda x: x.get('score', 0), reverse=True)
        return results[:top_k]

    async def _query_distributed_memories(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """Query memories from other nodes in the network"""
        query_tasks = []
        query_id = hashlib.sha256(f'{query}{datetime.now().isoformat()}'.encode()).hexdigest()[:8]
        for (node_id, node_info) in self.consensus.nodes.items():
            if node_id != self.node_id and node_info.is_alive():
                task = asyncio.create_task(self._send_memory_query(node_info, query, query_id))
                query_tasks.append(task)
        if not query_tasks:
            return []
        results = await asyncio.gather(*query_tasks, return_exceptions=True)
        distributed_memories = []
        for result in results:
            if not isinstance(result, Exception) and result:
                for memory_data in result:
                    distributed_memories.append({'memory': memory_data, 'score': 0.5, 'source': 'distributed', 'node_id': memory_data.get('node_id', 'unknown')})
        return distributed_memories

    async def _send_memory_query(self, node_info: NodeInfo, query: str, query_id: str) -> List[Dict[str, Any]]:
        """Send memory query to specific node"""
        try:
            async with aiohttp.ClientSession() as session:
                payload = {'query': query, 'query_id': query_id}
                async with session.post(f'{node_info.endpoint}/memory/query', json=payload, timeout=aiohttp.ClientTimeout(total=3.0)) as response:
                    if response.status == 200:
                        response_data = await response.json()
                        return response_data.get('memories', [])
            return []
        except Exception as e:
            logger.warning(f'Failed to query memory from {node_info.node_id}', error=str(e))
            return []

    def get_network_status(self) -> Dict[str, Any]:
        """Get status of the distributed network"""
        alive_nodes = [node for node in self.consensus.nodes.values() if node.is_alive()]
        return {'node_id': self.node_id, 'state': self.consensus.state.value, 'term': self.consensus.current_term, 'leader_id': self.consensus.leader_id, 'total_nodes': len(self.consensus.nodes), 'alive_nodes': len(alive_nodes), 'local_memories': len(self.local_memories), 'distributed_memories': len(self.distributed_memories), 'consensus_memories': sum((1 for entry in self.distributed_memories.values() if entry.consensus_achieved)), 'consciousness_level': self.consciousness_level, 'network_health': len(alive_nodes) / len(self.consensus.nodes) if self.consensus.nodes else 0.0}

