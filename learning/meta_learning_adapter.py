# ═══════════════════════════════════════════════════════════════════════════
# FILENAME: meta_learning_adapter.py
# MODULE: learning.meta_learning_adapter
# DESCRIPTION: Provides an adapter to bridge the Meta-Learning Enhancement System
#              with the Unified AI Enhancement Framework. It handles dynamic learning
#              rate adjustment, federated learning coordination, symbolic feedback loops,
#              and performance monitoring.
# DEPENDENCIES: asyncio, numpy, typing, dataclasses, enum, datetime, structlog
# LICENSE: PROPRIETARY - LUKHAS AI SYSTEMS - UNAUTHORIZED ACCESS PROHIBITED
# ═══════════════════════════════════════════════════════════════════════════
# ΛORIGIN_AGENT: Jules-04
# ΛTASK_ID: 171-176
# ΛCOMMIT_WINDOW: pre-audit
# ΛAPPROVED_BY: Human Overseer (GRDM)
# ΛUDIT: Standardized header/footer, added comments, normalized logger, applied ΛTAGs.

#!/usr/bin/env python3
"""
🧠 Meta-Learning Enhancement Integration Adapter
Bridges the Meta-Learning Enhancement System with the Unified AI Enhancement Framework

This adapter provides standardized interfaces for:
- Dynamic learning rate adjustment
- Federated learning coordination
- Symbolic feedback loops
- Performance monitoring and dashboard integration

Author: lukhas AI Enhancement Team
Date: 2025-01-27
"""

import asyncio # Not actively used by async def, but kept for future consistency
# import logging # Original logging
import structlog # ΛTRACE: Using structlog for structured logging
import numpy as np
from typing import Dict, List, Any, Optional, Tuple, Union # Union, Tuple not used
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime, timedelta # timedelta not used

# ΛTRACE: Initialize logger for learning phase
logger = structlog.get_logger().bind(tag="learning_phase")

# # Enum for phases of meta-learning enhancement
class LearningPhase(Enum):
    """Phases of meta-learning enhancement"""
    # ΛNOTE: Defines stages in the meta-learning lifecycle.
    EXPLORATION = "exploration"
    ADAPTATION = "adaptation"
    CONVERGENCE = "convergence"
    STABILIZATION = "stabilization"
    OPTIMIZATION = "optimization"

# # Enum for states of federated learning coordination
class FederatedState(Enum):
    """States of federated learning coordination"""
    # ΛNOTE: Tracks the status of the federated learning process.
    DISCONNECTED = "disconnected"
    CONNECTING = "connecting"
    SYNCHRONIZED = "synchronized"
    LEARNING = "learning"
    CONVERGING = "converging"
    COMPLETE = "complete"

# # Dataclass for comprehensive meta-learning performance metrics
# ΛEXPOSE: Data structure for tracking various meta-learning metrics.
@dataclass
class MetaLearningMetrics:
    """Comprehensive metrics for meta-learning performance"""
    # ΛNOTE: Holds a detailed snapshot of system performance.
    # ΛSEED: Default metric values act as initial seeds.
    current_learning_rate: float = 1e-3
    learning_rate_adaptation: float = 0.0
    learning_rate_stability: float = 1.0
    optimal_rate_distance: float = 0.0
    federated_nodes_active: int = 0
    federated_convergence: float = 0.0
    consensus_quality: float = 0.0
    communication_efficiency: float = 0.0
    symbolic_feedback_quality: float = 0.0
    intent_node_integration: float = 0.0 # ΛNOTE: Relates to symbolic AI integration.
    memoria_coupling: float = 0.0       # ΛNOTE: Relates to memory system integration.
    dream_system_coherence: float = 0.0 # ΛNOTE: Relates to dream/generative system integration.
    overall_performance: float = 0.0
    adaptation_speed: float = 0.0
    stability_score: float = 0.0
    enhancement_factor: float = 1.0
    timestamp: datetime = field(default_factory=datetime.now)
    current_phase: LearningPhase = LearningPhase.EXPLORATION
    federated_state: FederatedState = FederatedState.DISCONNECTED

# # Dataclass for learning rate bounds and constraints
# ΛEXPOSE: Configuration for learning rate adaptation.
@dataclass
class LearningRateBounds:
    """Bounds and constraints for learning rate adaptation"""
    # ΛNOTE: Defines parameters for dynamic learning rate adjustments.
    # ΛSEED: These bounds and factors are initial seeds for the LR adaptation mechanism.
    min_rate: float = 1e-6
    max_rate: float = 1e-2
    optimal_rate: float = 1e-3
    adaptation_factor: float = 1.1 # Factor for increasing LR
    decay_factor: float = 0.95    # Factor for decreasing LR
    momentum: float = 0.9         # Momentum for LR changes (conceptual, not directly used in current logic)

# # Meta-Learning Enhancement Adapter class
# ΛEXPOSE: Main adapter class for integrating meta-learning enhancements.
class MetaLearningEnhancementAdapter:
    """
    Integration adapter for the Meta-Learning Enhancement System

    Provides standardized interfaces for adaptive learning, federated coordination,
    and symbolic feedback integration within the unified AI framework
    """

    # # Initialization
    def __init__(self, meta_learning_system: Any, orchestrator: Optional[Any] = None): # Type Any for external systems
        """Initialize the Meta-Learning Enhancement adapter"""
        # ΛNOTE: Initializes with a meta-learning system and an optional orchestrator.
        # ΛSEED: The provided `meta_learning_system` is a core seed component.
        self.meta_learning_system = meta_learning_system # This is the system being adapted/enhanced
        self.orchestrator = orchestrator # For coordinating with other AI components
        self.current_metrics = MetaLearningMetrics()
        self.learning_rate_bounds = LearningRateBounds()

        self.performance_history: List[Dict[str, Any]] = []
        self.adaptation_history: List[Dict[str, Any]] = [] # History of learning rate adaptations
        self.federated_history: List[Dict[str, Any]] = [] # History of federated learning cycles

        self.active_learning_nodes: List[Dict[str, Any]] = [] # Represents connected nodes in federated learning
        self.symbolic_feedback_buffer: List[Dict[str, Any]] = [] # Stores recent symbolic feedback
        self.intent_node_cache: Dict[str, Any] = {} # Cache for intent node states
        self.memoria_state: Dict[str, Any] = {} # Current state of memoria integration

        self.total_adaptations = 0
        self.successful_adaptations = 0
        self.federated_cycles = 0
        self.symbolic_feedback_cycles = 0

        # ΛTRACE: MetaLearningEnhancementAdapter initialized
        logger.info("meta_learning_enhancement_adapter_initialized", meta_system_type=type(meta_learning_system).__name__)

    # # Initialize the meta-learning enhancement system components
    # ΛEXPOSE: Public method to trigger initialization of sub-systems.
    async def initialize(self) -> bool:
        """Initialize the meta-learning enhancement system"""
        # ΛTRACE: Initializing meta-learning enhancement system
        logger.info("meta_learning_enhancement_system_initialize_start")
        try:
            if hasattr(self.meta_learning_system, 'initialize') and callable(self.meta_learning_system.initialize):
                await self.meta_learning_system.initialize()

            await self._initialize_learning_rate_system()
            await self._initialize_federated_system()
            await self._initialize_symbolic_feedback()
            await self._initialize_performance_monitoring()

            self.current_metrics.current_phase = LearningPhase.ADAPTATION
            self.current_metrics.federated_state = FederatedState.CONNECTING
            # ΛTRACE: Meta-learning enhancement system initialized successfully
            logger.info("meta_learning_enhancement_system_initialize_success")
            return True
        except Exception as e:
            # ΛTRACE: Meta-learning enhancement system initialization failed
            logger.error("meta_learning_enhancement_system_initialize_failed", error=str(e), exc_info=True)
            return False

    # # Get current meta-learning metrics
    # ΛEXPOSE: Public method to retrieve current system metrics.
    async def get_current_metrics(self) -> Dict[str, Any]:
        """Get current meta-learning metrics"""
        # ΛTRACE: Getting current metrics
        logger.info("get_current_metrics_start")
        try:
            await self._update_current_metrics() # Ensure metrics are up-to-date
            metrics_dict = self.current_metrics.__dict__.copy() # Get all fields from dataclass
            metrics_dict["current_phase"] = self.current_metrics.current_phase.value # Convert enums to string
            metrics_dict["federated_state"] = self.current_metrics.federated_state.value
            metrics_dict["timestamp"] = self.current_metrics.timestamp.isoformat()
            # ΛTRACE: Current metrics retrieved
            logger.debug("get_current_metrics_success", metrics_count=len(metrics_dict))
            return metrics_dict
        except Exception as e:
            # ΛTRACE: Failed to get current metrics
            logger.error("get_current_metrics_failed", error=str(e), exc_info=True)
            return self._get_default_metrics().__dict__ # Return default metrics on error

    # # Enhance learning based on current metrics and system feedback
    # ΛEXPOSE: Core method to drive a meta-learning enhancement cycle.
    async def enhance_learning(self, current_performance_metrics: Dict[str, Any], # Renamed from current_metrics to avoid conflict
                             system_feedback: Dict[str, Any]) -> Dict[str, Any]:
        """Enhance learning based on current metrics and system feedback"""
        # ΛDREAM_LOOP: This entire method represents a meta-learning cycle, adapting various aspects of the learning process.
        # ΛTRACE: Starting meta-learning enhancement cycle
        logger.info("meta_learning_enhancement_cycle_start")
        try:
            enhancement_results: Dict[str, Any] = {}

            rate_results = await self._enhance_learning_rate(current_performance_metrics, system_feedback)
            enhancement_results["learning_rate_enhancement"] = rate_results

            federated_results = await self._enhance_federated_learning(current_performance_metrics, system_feedback)
            enhancement_results["federated_learning_enhancement"] = federated_results

            symbolic_results = await self._enhance_symbolic_feedback(current_performance_metrics, system_feedback)
            enhancement_results["symbolic_feedback_enhancement"] = symbolic_results

            dashboard_results = await self._update_performance_dashboard(enhancement_results)
            enhancement_results["performance_dashboard_update"] = dashboard_results

            overall_enhancement = self._calculate_overall_enhancement(enhancement_results)
            enhancement_results["overall_enhancement_metrics"] = overall_enhancement

            self.total_adaptations += 1
            if overall_enhancement.get("success", False): self.successful_adaptations += 1
            await self._update_learning_phase(enhancement_results)

            # ΛTRACE: Meta-learning enhancement cycle completed
            logger.info("meta_learning_enhancement_cycle_completed", overall_adaptation_score=overall_enhancement.get("adaptation_score", 0.0))
            return enhancement_results
        except Exception as e:
            # ΛTRACE: Meta-learning enhancement cycle failed
            logger.error("meta_learning_enhancement_cycle_failed", error=str(e), exc_info=True)
            return {"success": False, "error": str(e), "adaptation_score": 0.0} # Changed key from "adaptation"

    # # Process biological feedback from Crista Optimizer
    # ΛEXPOSE: Integrates biological signals into the learning process.
    async def process_biological_feedback(self, topology_features: Dict[str, Any]) -> Dict[str, Any]:
        """Process biological feedback from Crista Optimizer"""
        # ΛDREAM_LOOP: Adapting learning based on bio-feedback is a unique learning loop.
        # ΛNOTE: This method links to a conceptual bio-inspired optimization component.
        # ΛTRACE: Processing biological feedback
        logger.info("processing_biological_feedback_start", feature_keys=list(topology_features.keys()))
        try:
            cristae_density = topology_features.get("cristae_density", 0.5) # Assume normalized 0-1
            membrane_potential = topology_features.get("membrane_potential", -70.0) # mV
            atp_efficiency = topology_features.get("atp_efficiency", 0.5) # Assume normalized 0-1

            bio_learning_signals = {
                "energy_availability": atp_efficiency,
                "system_stability": self._normalize_membrane_potential(membrane_potential),
                "resource_utilization": cristae_density,
                "metabolic_efficiency": (atp_efficiency + cristae_density) / 2.0
            }
            # ΛTRACE: Bio-learning signals derived
            logger.debug("bio_learning_signals_derived", signals=bio_learning_signals)

            adaptation_suggestions = await self._adapt_to_biological_state(bio_learning_signals)
            await self._integrate_biological_symbolic_feedback(bio_learning_signals)

            # ΛTRACE: Biological feedback processed successfully
            logger.info("biological_feedback_processed_successfully", num_suggestions=len(adaptation_suggestions))
            return {
                "bio_signals_processed_count": len(bio_learning_signals), # Renamed key
                "adaptation_suggestions": adaptation_suggestions,
                "biological_integration_status": True, # Renamed key
                "energy_informed_learning_active": bio_learning_signals["energy_availability"] > 0.5 # Renamed key
            }
        except Exception as e:
            # ΛTRACE: Failed to process biological feedback
            logger.error("process_biological_feedback_failed", error=str(e), exc_info=True)
            return {"success": False, "error": str(e)}

    # # Placeholder: Enhance learning rate based on current performance
    async def _enhance_learning_rate(self, perf_metrics: Dict[str, Any], feedback: Dict[str, Any]) -> Dict[str, Any]: # Renamed metrics
        """Enhance learning rate based on current performance"""
        # ΛNOTE: Dynamically adjusts learning rate.
        # ΛDREAM_LOOP: Adjusting LR based on performance is a meta-learning adaptation.
        # ΛTRACE: Enhancing learning rate
        logger.debug("enhance_learning_rate_start", current_lr=self.current_metrics.current_learning_rate)
        current_rate = self.current_metrics.current_learning_rate
        # Use overall_performance from the input `perf_metrics` if available, else fallback
        performance = perf_metrics.get("overall_performance", self.current_metrics.overall_performance)
        # Assuming stability might come from bio_orchestrator feedback or overall system stability
        stability = feedback.get("system_stability_metric", self.current_metrics.stability_score)


        if performance > 0.8 and stability > 0.7: new_rate = min(current_rate * self.learning_rate_bounds.adaptation_factor, self.learning_rate_bounds.max_rate); adaptation_type = "aggressive"
        elif performance < 0.3 or stability < 0.3: new_rate = max(current_rate * self.learning_rate_bounds.decay_factor, self.learning_rate_bounds.min_rate); adaptation_type = "conservative"
        else: new_rate = current_rate + 0.1 * (self.learning_rate_bounds.optimal_rate - current_rate); adaptation_type = "moderate"

        rate_change = new_rate - current_rate
        self.current_metrics.current_learning_rate = new_rate
        self.current_metrics.learning_rate_adaptation = abs(rate_change) / max(current_rate, 1e-9) # Avoid division by zero
        self.current_metrics.learning_rate_stability = self._calculate_rate_stability()
        self.adaptation_history.append({"timestamp": datetime.now(), "type": "learning_rate", "old_value": current_rate, "new_value": new_rate, "reason": adaptation_type})
        # ΛTRACE: Learning rate enhanced
        logger.info("learning_rate_enhanced", old_rate=current_rate, new_rate=new_rate, type=adaptation_type)
        return {"success": True, "old_rate": current_rate, "new_rate": new_rate, "rate_change_ratio": self.current_metrics.learning_rate_adaptation, "adaptation_type": adaptation_type} # Renamed key "adaptation"

    # # Placeholder: Enhance federated learning coordination
    async def _enhance_federated_learning(self, perf_metrics: Dict[str, Any], feedback: Dict[str, Any]) -> Dict[str, Any]: # Renamed metrics
        """Enhance federated learning coordination"""
        # ΛNOTE: Simulates managing federated learning nodes and calculating convergence.
        # ΛDREAM_LOOP: Adapting federated learning parameters or node selection is a meta-level learning activity.
        # ΛTRACE: Enhancing federated learning
        logger.debug("enhance_federated_learning_start", active_nodes=len(self.active_learning_nodes))
        target_nodes = feedback.get("federated_target_nodes", 5) # Example: target nodes from feedback
        current_nodes_count = len(self.active_learning_nodes) # Renamed current_nodes

        if current_nodes_count < target_nodes:
            new_nodes_list = await self._connect_federated_nodes(target_nodes - current_nodes_count) # Renamed new_nodes
            self.active_learning_nodes.extend(new_nodes_list)

        convergence, consensus, communication_eff = 0.0, 0.0, 0.0
        if self.active_learning_nodes: # Check if list is not empty
            convergence = await self._calculate_federated_convergence()
            consensus = await self._calculate_consensus_quality()
            communication_eff = await self._calculate_communication_efficiency()

        self.current_metrics.federated_nodes_active = len(self.active_learning_nodes)
        self.current_metrics.federated_convergence = convergence
        self.current_metrics.consensus_quality = consensus
        self.current_metrics.communication_efficiency = communication_eff

        if convergence > 0.9: self.current_metrics.federated_state = FederatedState.COMPLETE
        elif convergence > 0.7: self.current_metrics.federated_state = FederatedState.CONVERGING
        elif self.active_learning_nodes: self.current_metrics.federated_state = FederatedState.LEARNING
        else: self.current_metrics.federated_state = FederatedState.DISCONNECTED
        self.federated_cycles += 1
        self.federated_history.append({"timestamp":datetime.now(), "active_nodes":len(self.active_learning_nodes), "convergence":convergence, "state":self.current_metrics.federated_state.value})
        # ΛTRACE: Federated learning enhanced
        logger.info("federated_learning_enhanced", active_nodes=len(self.active_learning_nodes), convergence=convergence, state=self.current_metrics.federated_state.value)
        return {"success": True, "active_nodes": len(self.active_learning_nodes), "convergence": convergence, "consensus_quality": consensus, "communication_efficiency": communication_eff, "federated_state": self.current_metrics.federated_state.value}

    # # Placeholder: Enhance symbolic feedback loops
    async def _enhance_symbolic_feedback(self, perf_metrics: Dict[str, Any], feedback: Dict[str, Any]) -> Dict[str, Any]: # Renamed metrics
        """Enhance symbolic feedback loops"""
        # ΛNOTE: Simulates processing and integrating symbolic feedback from various LUKHAS components.
        # ΛDREAM_LOOP: Refining how symbolic feedback is used is a form of meta-learning.
        # ΛTRACE: Enhancing symbolic feedback
        logger.debug("enhance_symbolic_feedback_start")
        # ΛCAUTION: These are mock integrations. Real integration is complex.
        intent_integration = await self._process_intent_nodes(feedback.get("intent_data", {}))
        memoria_coupling = await self._process_memoria_integration(feedback.get("memoria_data", {}))
        dream_coherence = await self._process_dream_coherence(feedback.get("dream_data", {}))
        symbolic_quality = (intent_integration + memoria_coupling + dream_coherence) / 3.0

        self.current_metrics.symbolic_feedback_quality = symbolic_quality
        self.current_metrics.intent_node_integration = intent_integration
        self.current_metrics.memoria_coupling = memoria_coupling
        self.current_metrics.dream_system_coherence = dream_coherence
        self.symbolic_feedback_buffer.append({"timestamp": datetime.now(), "quality": symbolic_quality, "intent": intent_integration, "memoria": memoria_coupling, "dream": dream_coherence})
        if len(self.symbolic_feedback_buffer) > 100: self.symbolic_feedback_buffer.pop(0)
        self.symbolic_feedback_cycles += 1
        # ΛTRACE: Symbolic feedback enhanced
        logger.info("symbolic_feedback_enhanced", quality=symbolic_quality)
        return {"success": True, "symbolic_quality": symbolic_quality, "intent_integration": intent_integration, "memoria_coupling": memoria_coupling, "dream_coherence": dream_coherence, "buffer_size": len(self.symbolic_feedback_buffer)} # Renamed key

    # # Placeholder: Update performance monitoring dashboard
    async def _update_performance_dashboard(self, enhancement_results: Dict[str, Any]) -> Dict[str, Any]:
        """Update performance monitoring dashboard"""
        # ΛNOTE: Calculates and updates overall performance metrics.
        # ΛTRACE: Updating performance dashboard
        logger.debug("update_performance_dashboard_start")
        learning_rate_perf = enhancement_results.get("learning_rate_enhancement", {}).get("rate_change_ratio", 0.0) # Use new key
        federated_perf = enhancement_results.get("federated_learning_enhancement", {}).get("convergence", 0.0) # Use new key
        symbolic_perf = enhancement_results.get("symbolic_feedback_enhancement", {}).get("symbolic_quality", 0.0) # Use new key
        overall_performance = (0.4 * learning_rate_perf + 0.3 * federated_perf + 0.3 * symbolic_perf)

        adaptation_speed = self._calculate_adaptation_speed()
        stability_score = self._calculate_overall_stability()
        enhancement_factor = self._calculate_enhancement_factor(overall_performance)

        self.current_metrics.overall_performance = overall_performance
        self.current_metrics.adaptation_speed = adaptation_speed
        self.current_metrics.stability_score = stability_score
        self.current_metrics.enhancement_factor = enhancement_factor
        self.performance_history.append({"timestamp": datetime.now(), "overall": overall_performance, "speed": adaptation_speed, "stability": stability_score, "factor": enhancement_factor})
        if len(self.performance_history) > 200: self.performance_history.pop(0)
        # ΛTRACE: Performance dashboard updated
        logger.info("performance_dashboard_updated", overall_performance=overall_performance)
        return {"success": True, "overall_performance": overall_performance, "adaptation_speed": adaptation_speed, "stability_score": stability_score, "enhancement_factor": enhancement_factor, "history_size": len(self.performance_history)} # Renamed key

    # # Get comprehensive performance metrics
    # ΛEXPOSE: Public method to retrieve a summary of system performance.
    async def get_performance_metrics(self) -> Dict[str, Any]:
        """Get comprehensive performance metrics"""
        # ΛTRACE: Getting performance metrics
        logger.info("get_performance_metrics_start")
        success_rate = self.successful_adaptations / max(self.total_adaptations, 1)
        metrics = {
            "total_adaptations": self.total_adaptations, "successful_adaptations": self.successful_adaptations,
            "success_rate": success_rate, "federated_cycles": self.federated_cycles,
            "symbolic_cycles": self.symbolic_feedback_cycles,
            **self.current_metrics.__dict__ # Include all current_metrics fields
        }
        metrics["current_phase"] = self.current_metrics.current_phase.value # ensure enum is string
        metrics["federated_state"] = self.current_metrics.federated_state.value
        metrics["timestamp"] = self.current_metrics.timestamp.isoformat()
        # ΛTRACE: Performance metrics retrieved
        logger.debug("get_performance_metrics_success", total_adaptations=self.total_adaptations)
        return metrics

    # # Apply a specific optimization action
    # ΛEXPOSE: Allows targeted optimization actions to be triggered.
    async def apply_optimization(self, action: str, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """Apply a specific optimization action"""
        # ΛTRACE: Applying optimization
        logger.info("apply_optimization_start", action=action, parameters=parameters)
        try:
            if action == "increase_learning_rate": return await self._increase_learning_rate(parameters.get("rate_multiplier", 1.2))
            elif action == "stabilize_federated_learning": return await self._stabilize_federated_learning(parameters.get("stability_target", 0.8))
            elif action == "enhance_symbolic_feedback_quality": return await self._enhance_symbolic_feedback_quality(parameters.get("enhancement_factor", 1.1)) # Matched key
            else:
                logger.warn("unknown_optimization_action", action_name=action)
                return {"success": False, "error": f"Unknown action: {action}"}
        except Exception as e:
            # ΛTRACE: Failed to apply optimization
            logger.error("apply_optimization_failed", action=action, error=str(e), exc_info=True)
            return {"success": False, "error": str(e)}

    # # Helper methods and calculations (placeholders)
    async def _initialize_learning_rate_system(self) -> None: # Stubs, kept async for consistency
        self.current_metrics.current_learning_rate = self.learning_rate_bounds.optimal_rate
        self.current_metrics.learning_rate_stability = 1.0
        logger.debug("learning_rate_system_initialized_stub")
    async def _initialize_federated_system(self) -> None:
        self.active_learning_nodes = []
        self.current_metrics.federated_state = FederatedState.DISCONNECTED
        logger.debug("federated_system_initialized_stub")
    async def _initialize_symbolic_feedback(self) -> None:
        self.symbolic_feedback_buffer = []
        self.intent_node_cache = {}; self.memoria_state = {}
        logger.debug("symbolic_feedback_system_initialized_stub")
    async def _initialize_performance_monitoring(self) -> None:
        self.performance_history = []
        self.current_metrics.overall_performance = 0.5
        logger.debug("performance_monitoring_initialized_stub")
    async def _update_current_metrics(self) -> None:
        self.current_metrics.timestamp = datetime.now()
        current_rate = self.current_metrics.current_learning_rate
        optimal_rate = self.learning_rate_bounds.optimal_rate
        self.current_metrics.optimal_rate_distance = abs(current_rate - optimal_rate) / max(optimal_rate, 1e-9) # Avoid zero div
        logger.debug("current_metrics_updated_stub")
    def _normalize_membrane_potential(self, potential: float) -> float:
        normalized = 1.0 - abs(potential + 70.0) / 20.0 # Assuming -70mV is optimal baseline
        return max(0.0, min(1.0, normalized))
    async def _adapt_to_biological_state(self, bio_signals: Dict[str, Any]) -> List[Dict[str, Any]]:
        logger.debug("adapt_to_biological_state_stub", bio_signals_keys=list(bio_signals.keys()))
        suggestions = []
        if bio_signals.get("energy_availability", 0) > 0.7 and bio_signals.get("system_stability", 0) > 0.7:
            suggestions.append({"type": "learning_rate", "action": "increase", "factor": 1.05, "reason": "high_bio_energy_stability"})
        elif bio_signals.get("energy_availability", 1) < 0.3 or bio_signals.get("system_stability", 1) < 0.3:
            suggestions.append({"type": "learning_rate", "action": "decrease", "factor": 0.95, "reason": "low_bio_energy_or_stability"})
        return suggestions
    async def _integrate_biological_symbolic_feedback(self, bio_signals: Dict[str, Any]) -> None:
        self.symbolic_feedback_buffer.append({"timestamp": datetime.now(), "type": "biological_integration", **bio_signals})
        logger.debug("integrate_biological_symbolic_feedback_stub")
    def _calculate_rate_stability(self) -> float:
        if len(self.adaptation_history) < 5: return 0.75 # Not enough data for stable calculation, assume moderately stable
        recent_rates = [entry.get("new_value", self.current_metrics.current_learning_rate) for entry in self.adaptation_history[-10:]]
        if not recent_rates: return 0.5
        std_dev = np.std(recent_rates)
        mean_val = np.mean(recent_rates)
        return max(0.0, 1.0 - (std_dev / max(mean_val, 1e-9))) # Coefficient of variation based stability
    async def _connect_federated_nodes(self, num_nodes: int) -> List[Dict[str, Any]]:
        logger.debug("connect_federated_nodes_stub", num_to_connect=num_nodes)
        return [{"id": f"sim_node_{i+len(self.active_learning_nodes)}", "status": "connected", "performance": random.uniform(0.6,0.9)} for i in range(num_nodes)] if __import__('random') else []
    async def _calculate_federated_convergence(self) -> float: return random.uniform(0.5,0.95) if self.active_learning_nodes and __import__('random') else 0.0
    async def _calculate_consensus_quality(self) -> float: return random.uniform(0.6, 0.9) if self.active_learning_nodes and __import__('random') else 0.0
    async def _calculate_communication_efficiency(self) -> float: return random.uniform(0.7,1.0) if self.active_learning_nodes and __import__('random') else 0.0
    async def _process_intent_nodes(self, feedback: Dict[str, Any]) -> float: return random.uniform(0.5,0.9) if __import__('random') else 0.7
    async def _process_memoria_integration(self, feedback: Dict[str, Any]) -> float: return random.uniform(0.6,0.85) if __import__('random') else 0.7
    async def _process_dream_coherence(self, feedback: Dict[str, Any]) -> float: return random.uniform(0.4,0.9) if __import__('random') else 0.65
    def _calculate_adaptation_speed(self) -> float:
        if len(self.performance_history) < 5: return 0.5 # Not enough data
        improvements = np.diff([entry.get("overall_performance",0) for entry in self.performance_history[-5:]])
        return max(0.0, min(1.0, np.mean(improvements) * 5 + 0.5)) if len(improvements)>0 else 0.5 # Scaled average improvement
    def _calculate_overall_stability(self) -> float:
        rate_s = self.current_metrics.learning_rate_stability
        fed_s = self.current_metrics.consensus_quality if self.current_metrics.federated_nodes_active > 0 else 0.5
        sym_s = 1.0 - np.std([e.get("quality",0.5) for e in self.symbolic_feedback_buffer[-10:]]) if len(self.symbolic_feedback_buffer) > 1 else 0.5
        return (0.4 * rate_s + 0.3 * fed_s + 0.3 * sym_s)
    def _calculate_enhancement_factor(self, performance: float) -> float: return 1.0 + performance # Simple additive factor
    def _calculate_overall_enhancement(self, results: Dict[str, Any]) -> Dict[str, Any]:
        lr_adapt = results.get("learning_rate_enhancement", {}).get("rate_change_ratio", 0.0)
        fed_conv = results.get("federated_learning_enhancement", {}).get("convergence", 0.0)
        sym_qual = results.get("symbolic_feedback_enhancement", {}).get("symbolic_quality", 0.0)
        overall = (0.4 * lr_adapt + 0.3 * fed_conv + 0.3 * sym_qual) # This is more a change score than absolute performance
        return {"success": overall > 0.05, "adaptation_score": overall, "lr_contrib": lr_adapt, "fed_contrib": fed_conv, "sym_contrib": sym_qual} # Renamed keys
    async def _update_learning_phase(self, results: Dict[str, Any]) -> None:
        overall_adapt_score = results.get("overall_enhancement_metrics", {}).get("adaptation_score", 0.0) # Use new key
        stability = self.current_metrics.stability_score
        if overall_adapt_score > 0.8 and stability > 0.8: self.current_metrics.current_phase = LearningPhase.OPTIMIZATION # Changed from STABILIZATION
        elif overall_adapt_score > 0.6 and stability > 0.6: self.current_metrics.current_phase = LearningPhase.STABILIZATION # Changed from CONVERGENCE
        elif overall_adapt_score > 0.4: self.current_metrics.current_phase = LearningPhase.CONVERGENCE # Changed from ADAPTATION
        elif overall_adapt_score > 0.1: self.current_metrics.current_phase = LearningPhase.ADAPTATION
        else: self.current_metrics.current_phase = LearningPhase.EXPLORATION
        logger.debug("learning_phase_updated", new_phase=self.current_metrics.current_phase.value)
    async def _increase_learning_rate(self, multiplier: float) -> Dict[str, Any]:
        old_rate = self.current_metrics.current_learning_rate
        new_rate = min(old_rate * multiplier, self.learning_rate_bounds.max_rate)
        self.current_metrics.current_learning_rate = new_rate
        logger.info("learning_rate_increased_manually", old_rate=old_rate, new_rate=new_rate, multiplier=multiplier)
        return {"success": True, "old_rate": old_rate, "new_rate": new_rate, "multiplier_applied": multiplier}
    async def _stabilize_federated_learning(self, stability_target: float) -> Dict[str, Any]:
        # This is a mock. Real stabilization is complex.
        improvement = (stability_target - self.current_metrics.consensus_quality) * random.uniform(0.3,0.7) if __import__('random') else 0.1
        self.current_metrics.consensus_quality = min(stability_target, self.current_metrics.consensus_quality + improvement)
        logger.info("federated_learning_stabilization_attempted", target=stability_target, new_consensus=self.current_metrics.consensus_quality)
        return {"success": True, "target_stability": stability_target, "achieved_consensus": self.current_metrics.consensus_quality}
    async def _enhance_symbolic_feedback_quality(self, factor: float) -> Dict[str, Any]: # Renamed enhancement_factor
        old_quality = self.current_metrics.symbolic_feedback_quality
        new_quality = min(old_quality * factor, 1.0)
        self.current_metrics.symbolic_feedback_quality = new_quality
        logger.info("symbolic_feedback_quality_enhanced_manually", old_quality=old_quality, new_quality=new_quality, factor=factor)
        return {"success": True, "old_quality": old_quality, "new_quality": new_quality, "enhancement_factor_applied": factor} # Renamed key
    def _get_default_metrics(self) -> MetaLearningMetrics: # Return type MetaLearningMetrics
        logger.warn("returning_default_metrics_due_to_error")
        return MetaLearningMetrics() # Return an instance

# ═══════════════════════════════════════════════════════════════════════════
# FILENAME: meta_learning_adapter.py
# VERSION: 1.1 (Jules-04 update)
# TIER SYSTEM: Orchestration / Integration Layer
# ΛTRACE INTEGRATION: ENABLED (structlog)
# CAPABILITIES: Adapts meta-learning system parameters, coordinates federated learning (stub),
#               integrates symbolic feedback (stub), monitors and reports performance.
# FUNCTIONS: MetaLearningEnhancementAdapter (class), LearningPhase (Enum), FederatedState (Enum),
#            MetaLearningMetrics (dataclass), LearningRateBounds (dataclass).
# CLASSES: MetaLearningEnhancementAdapter, LearningPhase, FederatedState, MetaLearningMetrics, LearningRateBounds
# DECORATORS: @dataclass
# DEPENDENCIES: asyncio, structlog, numpy, typing, dataclasses, enum, datetime
# INTERFACES: `initialize()`, `get_current_metrics()`, `enhance_learning()`,
#               `process_biological_feedback()`, `get_performance_metrics()`, `apply_optimization()`
# ERROR HANDLING: Try/except blocks in public methods, logs errors, returns error status in dicts.
#                 Default metrics returned on `get_current_metrics` failure.
# LOGGING: ΛTRACE_ENABLED via structlog, bound with tag="learning_phase". Extensive debug/info logs.
# AUTHENTICATION: N/A (Assumed handled by calling system or orchestrator)
# HOW TO USE:
#   Instantiate `MetaLearningEnhancementAdapter(meta_learning_system_instance, optional_orchestrator)`.
#   Call `initialize()` first.
#   Use `enhance_learning()` to drive adaptation cycles with metrics and feedback.
#   Use `process_biological_feedback()` for bio-inspired adaptations.
#   Retrieve metrics with `get_current_metrics()` or `get_performance_metrics()`.
# INTEGRATION NOTES: Many helper methods are stubs (e.g., federated node connection, symbolic processing)
#                    and require full implementation with actual LUKHAS components.
#                    Relies on an external `meta_learning_system` passed during instantiation.
# MAINTENANCE: Implement all placeholder/stub methods. Refine mock calculations.
#              Ensure robust error handling for interactions with external systems.
# CONTACT: LUKHAS DEVELOPMENT TEAM
# LICENSE: PROPRIETARY - LUKHAS AI SYSTEMS - UNAUTHORIZED ACCESS PROHIBITED
# ═══════════════════════════════════════════════════════════════════════════
