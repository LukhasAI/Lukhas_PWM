"""
Documentation Analytics System
=============================

AI-powered analytics system for documentation quality, usage patterns,
and content optimization recommendations.

Features:
- Documentation quality scoring and analysis
- Usage pattern tracking and insights
- Content gap detection and recommendations
- User behavior analytics and optimization
- Bio-oscillator integration for adaptive content
"""

import asyncio
import logging
from dataclasses import dataclass, field
from enum import Enum
from typing import Dict, List, Any, Optional, Tuple, Set
from datetime import datetime, timedelta
from pathlib import Path
import json
import re
from collections import defaultdict, Counter
import statistics

logger = logging.getLogger(__name__)

class AnalyticsType(Enum):
    """Types of analytics"""
    QUALITY_ANALYSIS = "quality_analysis"
    USAGE_PATTERNS = "usage_patterns"
    CONTENT_GAPS = "content_gaps"
    USER_BEHAVIOR = "user_behavior"
    PERFORMANCE_METRICS = "performance_metrics"
    ACCESSIBILITY_AUDIT = "accessibility_audit"

class QualityMetric(Enum):
    """Documentation quality metrics"""
    COMPLETENESS = "completeness"
    ACCURACY = "accuracy"
    CLARITY = "clarity"
    STRUCTURE = "structure"
    EXAMPLES = "examples"
    ACCESSIBILITY = "accessibility"
    FRESHNESS = "freshness"
    CONSISTENCY = "consistency"

class ContentType(Enum):
    """Types of documentation content"""
    API_DOCS = "api_docs"
    TUTORIALS = "tutorials"
    GUIDES = "guides"
    REFERENCE = "reference"
    FAQ = "faq"
    TROUBLESHOOTING = "troubleshooting"
    COMPLIANCE = "compliance"
    SECURITY = "security"

@dataclass
class QualityScore:
    """Documentation quality score"""
    metric: QualityMetric
    score: float  # 0-100
    details: Dict[str, Any]
    recommendations: List[str]
    confidence: float  # 0-1

@dataclass
class UsageMetrics:
    """Documentation usage metrics"""
    page_views: int
    unique_visitors: int
    bounce_rate: float
    time_on_page: float  # seconds
    search_queries: List[str]
    download_count: int
    feedback_rating: Optional[float]
    conversion_rate: float  # tutorial completion, etc.

@dataclass
class ContentGap:
    """Identified content gap"""
    gap_id: str
    title: str
    description: str
    priority: str  # high, medium, low
    estimated_impact: float
    suggested_content_type: ContentType
    related_topics: List[str]
    user_requests: int
    search_queries: List[str]

@dataclass
class UserBehaviorPattern:
    """User behavior pattern"""
    pattern_id: str
    pattern_type: str
    description: str
    frequency: int
    user_segments: List[str]
    triggers: List[str]
    outcomes: Dict[str, Any]
    recommendations: List[str]

@dataclass
class AnalyticsReport:
    """Comprehensive analytics report"""
    report_id: str
    analytics_type: AnalyticsType
    generated_at: datetime
    time_period: Tuple[datetime, datetime]
    summary: Dict[str, Any]
    detailed_findings: List[Dict[str, Any]]
    recommendations: List[str]
    action_items: List[str]
    metadata: Dict[str, Any]

class DocumentationAnalytics:
    """
    AI-powered documentation analytics system
    
    Provides comprehensive analytics on documentation quality,
    usage patterns, and optimization opportunities.
    """
    
    def __init__(self):
        self.quality_analyzers = {}
        self.usage_trackers = {}
        self.pattern_detectors = {}
        self.recommendation_engines = {}
        self._initialize_analyzers()
    
    def _initialize_analyzers(self):
        """Initialize analytics components"""
        
        # Quality analyzers
        self.quality_analyzers = {
            QualityMetric.COMPLETENESS: self._analyze_completeness,
            QualityMetric.ACCURACY: self._analyze_accuracy,
            QualityMetric.CLARITY: self._analyze_clarity,
            QualityMetric.STRUCTURE: self._analyze_structure,
            QualityMetric.EXAMPLES: self._analyze_examples,
            QualityMetric.ACCESSIBILITY: self._analyze_accessibility,
            QualityMetric.FRESHNESS: self._analyze_freshness,
            QualityMetric.CONSISTENCY: self._analyze_consistency
        }
        
        # Pattern detectors
        self.pattern_detectors = {
            "navigation_patterns": self._detect_navigation_patterns,
            "search_patterns": self._detect_search_patterns,
            "failure_patterns": self._detect_failure_patterns,
            "success_patterns": self._detect_success_patterns,
            "engagement_patterns": self._detect_engagement_patterns
        }
    
    async def generate_analytics_report(self, analytics_type: AnalyticsType,
                                      time_period: Optional[Tuple[datetime, datetime]] = None,
                                      content_paths: Optional[List[str]] = None,
                                      filters: Optional[Dict[str, Any]] = None) -> AnalyticsReport:
        """Generate comprehensive analytics report"""
        
        print(f"üìä Generating {analytics_type.value} analytics report...")
        
        if time_period is None:
            end_date = datetime.now()
            start_date = end_date - timedelta(days=30)
            time_period = (start_date, end_date)
        
        if filters is None:
            filters = {}
        
        # Generate report based on analytics type
        if analytics_type == AnalyticsType.QUALITY_ANALYSIS:
            report = await self._generate_quality_report(time_period, content_paths, filters)
        elif analytics_type == AnalyticsType.USAGE_PATTERNS:
            report = await self._generate_usage_report(time_period, content_paths, filters)
        elif analytics_type == AnalyticsType.CONTENT_GAPS:
            report = await self._generate_content_gaps_report(time_period, filters)
        elif analytics_type == AnalyticsType.USER_BEHAVIOR:
            report = await self._generate_user_behavior_report(time_period, filters)
        elif analytics_type == AnalyticsType.PERFORMANCE_METRICS:
            report = await self._generate_performance_report(time_period, content_paths, filters)
        elif analytics_type == AnalyticsType.ACCESSIBILITY_AUDIT:
            report = await self._generate_accessibility_report(content_paths, filters)
        else:
            raise ValueError(f"Unsupported analytics type: {analytics_type}")
        
        print(f"   ‚úÖ Generated report with {len(report.detailed_findings)} findings")
        print(f"   üí° {len(report.recommendations)} recommendations")
        
        return report
    
    async def _generate_quality_report(self, time_period: Tuple[datetime, datetime],
                                     content_paths: Optional[List[str]],
                                     filters: Dict[str, Any]) -> AnalyticsReport:
        """Generate documentation quality analytics report"""
        
        print("   üìù Analyzing documentation quality...")
        
        # Analyze documentation files
        if content_paths is None:
            content_paths = await self._discover_documentation_files()
        
        quality_scores = {}
        detailed_findings = []
        overall_recommendations = []
        
        for content_path in content_paths:
            print(f"      üìÑ Analyzing: {Path(content_path).name}")
            
            # Analyze each quality metric
            file_scores = {}
            for metric, analyzer in self.quality_analyzers.items():
                score = await analyzer(content_path, filters)
                file_scores[metric.value] = score
                
                if score.score < 70:  # Below threshold
                    detailed_findings.append({
                        "file": content_path,
                        "metric": metric.value,
                        "score": score.score,
                        "issues": score.details,
                        "recommendations": score.recommendations
                    })
            
            quality_scores[content_path] = file_scores
        
        # Calculate overall quality metrics
        all_scores = []
        metric_averages = {}
        
        for metric in QualityMetric:
            metric_scores = []
            for file_scores in quality_scores.values():
                if metric.value in file_scores:
                    metric_scores.append(file_scores[metric.value].score)
            
            if metric_scores:
                metric_averages[metric.value] = statistics.mean(metric_scores)
                all_scores.extend(metric_scores)
        
        overall_score = statistics.mean(all_scores) if all_scores else 0
        
        # Generate recommendations
        priority_metrics = sorted(metric_averages.items(), key=lambda x: x[1])[:3]
        for metric, score in priority_metrics:
            if score < 75:
                overall_recommendations.append(f"Improve {metric.replace('_', ' ')} (current score: {score:.1f})")
        
        # Create summary
        summary = {
            "overall_quality_score": overall_score,
            "total_files_analyzed": len(content_paths),
            "files_below_threshold": len([f for f in detailed_findings if f["score"] < 70]),
            "metric_averages": metric_averages,
            "top_issues": Counter([f["metric"] for f in detailed_findings]).most_common(5)
        }
        
        return AnalyticsReport(
            report_id=f"quality_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            analytics_type=AnalyticsType.QUALITY_ANALYSIS,
            generated_at=datetime.now(),
            time_period=time_period,
            summary=summary,
            detailed_findings=detailed_findings,
            recommendations=overall_recommendations,
            action_items=[
                "Focus on improving lowest-scoring quality metrics",
                "Review and update files with scores below 70",
                "Establish quality review process for new documentation"
            ],
            metadata={"analyzer_version": "1.0", "content_paths": content_paths}
        )
    
    async def _generate_usage_report(self, time_period: Tuple[datetime, datetime],
                                   content_paths: Optional[List[str]],
                                   filters: Dict[str, Any]) -> AnalyticsReport:
        """Generate usage patterns analytics report"""
        
        print("   üìà Analyzing usage patterns...")
        
        # Simulate usage data collection (in real implementation, this would connect to analytics services)
        usage_data = await self._collect_usage_data(time_period, content_paths, filters)
        
        detailed_findings = []
        recommendations = []
        
        # Analyze popular content
        popular_pages = sorted(usage_data.items(), key=lambda x: x[1].page_views, reverse=True)[:10]
        detailed_findings.append({
            "type": "popular_content",
            "title": "Most Popular Documentation Pages",
            "data": [{"page": page, "views": metrics.page_views} for page, metrics in popular_pages]
        })
        
        # Analyze high bounce rate content
        high_bounce_pages = [(page, metrics) for page, metrics in usage_data.items() 
                           if metrics.bounce_rate > 0.7]
        if high_bounce_pages:
            detailed_findings.append({
                "type": "high_bounce_rate",
                "title": "Pages with High Bounce Rate",
                "data": [{"page": page, "bounce_rate": metrics.bounce_rate} 
                        for page, metrics in high_bounce_pages]
            })
            recommendations.append("Improve content quality for high bounce rate pages")
        
        # Analyze search queries
        all_queries = []
        for metrics in usage_data.values():
            all_queries.extend(metrics.search_queries)
        
        common_queries = Counter(all_queries).most_common(20)
        detailed_findings.append({
            "type": "search_patterns",
            "title": "Most Common Search Queries",
            "data": [{"query": query, "count": count} for query, count in common_queries]
        })
        
        # Calculate metrics
        total_views = sum(metrics.page_views for metrics in usage_data.values())
        avg_time_on_page = statistics.mean([metrics.time_on_page for metrics in usage_data.values()])
        avg_bounce_rate = statistics.mean([metrics.bounce_rate for metrics in usage_data.values()])
        
        summary = {
            "total_page_views": total_views,
            "unique_pages": len(usage_data),
            "average_time_on_page": avg_time_on_page,
            "average_bounce_rate": avg_bounce_rate,
            "most_popular_page": popular_pages[0][0] if popular_pages else None,
            "total_search_queries": len(all_queries)
        }
        
        return AnalyticsReport(
            report_id=f"usage_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            analytics_type=AnalyticsType.USAGE_PATTERNS,
            generated_at=datetime.now(),
            time_period=time_period,
            summary=summary,
            detailed_findings=detailed_findings,
            recommendations=recommendations,
            action_items=[
                "Optimize high-traffic pages for better performance",
                "Create content for common search queries",
                "Improve navigation for high bounce rate pages"
            ],
            metadata={"data_sources": ["web_analytics", "search_logs"]}
        )
    
    async def _generate_content_gaps_report(self, time_period: Tuple[datetime, datetime],
                                          filters: Dict[str, Any]) -> AnalyticsReport:
        """Generate content gaps analytics report"""
        
        print("   üîç Identifying content gaps...")
        
        # Analyze various sources to identify content gaps
        search_gaps = await self._analyze_search_gaps(time_period)
        support_gaps = await self._analyze_support_ticket_gaps(time_period)
        user_feedback_gaps = await self._analyze_user_feedback_gaps(time_period)
        competitive_gaps = await self._analyze_competitive_gaps()
        
        all_gaps = search_gaps + support_gaps + user_feedback_gaps + competitive_gaps
        
        # Prioritize gaps
        prioritized_gaps = await self._prioritize_content_gaps(all_gaps)
        
        detailed_findings = []
        for gap in prioritized_gaps[:20]:  # Top 20 gaps
            detailed_findings.append({
                "gap_id": gap.gap_id,
                "title": gap.title,
                "description": gap.description,
                "priority": gap.priority,
                "estimated_impact": gap.estimated_impact,
                "content_type": gap.suggested_content_type.value,
                "user_requests": gap.user_requests,
                "related_topics": gap.related_topics
            })
        
        # Generate recommendations
        high_priority_gaps = [gap for gap in prioritized_gaps if gap.priority == "high"]
        recommendations = [
            f"Create {gap.suggested_content_type.value.replace('_', ' ')} for: {gap.title}"
            for gap in high_priority_gaps[:5]
        ]
        
        summary = {
            "total_gaps_identified": len(all_gaps),
            "high_priority_gaps": len(high_priority_gaps),
            "most_requested_topics": Counter([topic for gap in all_gaps 
                                            for topic in gap.related_topics]).most_common(10),
            "content_type_distribution": Counter([gap.suggested_content_type.value 
                                                for gap in all_gaps]).most_common()
        }
        
        return AnalyticsReport(
            report_id=f"gaps_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            analytics_type=AnalyticsType.CONTENT_GAPS,
            generated_at=datetime.now(),
            time_period=time_period,
            summary=summary,
            detailed_findings=detailed_findings,
            recommendations=recommendations,
            action_items=[
                "Create content for top 5 high-priority gaps",
                "Establish process for ongoing gap identification",
                "Set up automated monitoring for emerging content needs"
            ],
            metadata={"gap_sources": ["search", "support", "feedback", "competitive"]}
        )
    
    async def _generate_user_behavior_report(self, time_period: Tuple[datetime, datetime],
                                           filters: Dict[str, Any]) -> AnalyticsReport:
        """Generate user behavior analytics report"""
        
        print("   üë• Analyzing user behavior patterns...")
        
        # Detect various behavior patterns
        behavior_patterns = []
        
        for pattern_type, detector in self.pattern_detectors.items():
            patterns = await detector(time_period, filters)
            behavior_patterns.extend(patterns)
        
        # Analyze patterns
        detailed_findings = []
        recommendations = []
        
        pattern_types = defaultdict(list)
        for pattern in behavior_patterns:
            pattern_types[pattern.pattern_type].append(pattern)
        
        for pattern_type, patterns in pattern_types.items():
            detailed_findings.append({
                "pattern_type": pattern_type,
                "patterns_count": len(patterns),
                "top_patterns": [
                    {
                        "description": pattern.description,
                        "frequency": pattern.frequency,
                        "user_segments": pattern.user_segments
                    }
                    for pattern in sorted(patterns, key=lambda x: x.frequency, reverse=True)[:5]
                ]
            })
        
        # Generate insights and recommendations
        high_frequency_patterns = [p for p in behavior_patterns if p.frequency > 100]
        for pattern in high_frequency_patterns:
            if pattern.recommendations:
                recommendations.extend(pattern.recommendations)
        
        summary = {
            "total_patterns_detected": len(behavior_patterns),
            "pattern_types": len(pattern_types),
            "high_frequency_patterns": len(high_frequency_patterns),
            "most_common_pattern_type": max(pattern_types.keys(), 
                                          key=lambda x: len(pattern_types[x])) if pattern_types else None
        }
        
        return AnalyticsReport(
            report_id=f"behavior_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            analytics_type=AnalyticsType.USER_BEHAVIOR,
            generated_at=datetime.now(),
            time_period=time_period,
            summary=summary,
            detailed_findings=detailed_findings,
            recommendations=list(set(recommendations)),  # Remove duplicates
            action_items=[
                "Optimize user flows based on high-frequency patterns",
                "Address pain points identified in failure patterns",
                "Enhance successful patterns to improve overall experience"
            ],
            metadata={"pattern_detectors_used": list(self.pattern_detectors.keys())}
        )
    
    async def _generate_performance_report(self, time_period: Tuple[datetime, datetime],
                                         content_paths: Optional[List[str]],
                                         filters: Dict[str, Any]) -> AnalyticsReport:
        """Generate performance metrics analytics report"""
        
        print("   ‚ö° Analyzing performance metrics...")
        
        # Collect performance data
        performance_data = await self._collect_performance_data(time_period, content_paths)
        
        detailed_findings = []
        recommendations = []
        
        # Analyze page load times
        load_times = [data["load_time"] for data in performance_data.values()]
        avg_load_time = statistics.mean(load_times)
        slow_pages = [(page, data) for page, data in performance_data.items() 
                     if data["load_time"] > 3.0]
        
        if slow_pages:
            detailed_findings.append({
                "type": "slow_loading_pages",
                "title": "Pages with Slow Load Times",
                "data": [{"page": page, "load_time": data["load_time"]} 
                        for page, data in slow_pages]
            })
            recommendations.append("Optimize slow-loading pages")
        
        # Analyze mobile performance
        mobile_issues = [(page, data) for page, data in performance_data.items() 
                        if data.get("mobile_score", 100) < 80]
        
        if mobile_issues:
            detailed_findings.append({
                "type": "mobile_performance",
                "title": "Pages with Mobile Performance Issues",
                "data": [{"page": page, "mobile_score": data["mobile_score"]} 
                        for page, data in mobile_issues]
            })
            recommendations.append("Improve mobile performance for affected pages")
        
        # Analyze accessibility scores
        accessibility_issues = [(page, data) for page, data in performance_data.items() 
                              if data.get("accessibility_score", 100) < 90]
        
        if accessibility_issues:
            detailed_findings.append({
                "type": "accessibility_issues",
                "title": "Pages with Accessibility Issues",
                "data": [{"page": page, "accessibility_score": data["accessibility_score"]} 
                        for page, data in accessibility_issues]
            })
            recommendations.append("Address accessibility issues")
        
        summary = {
            "average_load_time": avg_load_time,
            "pages_analyzed": len(performance_data),
            "slow_pages_count": len(slow_pages),
            "mobile_issues_count": len(mobile_issues),
            "accessibility_issues_count": len(accessibility_issues)
        }
        
        return AnalyticsReport(
            report_id=f"performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            analytics_type=AnalyticsType.PERFORMANCE_METRICS,
            generated_at=datetime.now(),
            time_period=time_period,
            summary=summary,
            detailed_findings=detailed_findings,
            recommendations=recommendations,
            action_items=[
                "Optimize images and assets for faster loading",
                "Implement responsive design improvements",
                "Conduct accessibility audit and remediation"
            ],
            metadata={"performance_tools": ["lighthouse", "web_vitals"]}
        )
    
    async def _generate_accessibility_report(self, content_paths: Optional[List[str]],
                                           filters: Dict[str, Any]) -> AnalyticsReport:
        """Generate accessibility analytics report"""
        
        print("   ‚ôø Conducting accessibility audit...")
        
        if content_paths is None:
            content_paths = await self._discover_documentation_files()
        
        accessibility_issues = []
        detailed_findings = []
        
        for content_path in content_paths:
            issues = await self._analyze_accessibility_issues(content_path)
            if issues:
                accessibility_issues.extend(issues)
                detailed_findings.append({
                    "file": content_path,
                    "issues_count": len(issues),
                    "issues": issues
                })
        
        # Categorize issues
        issue_categories = defaultdict(list)
        for issue in accessibility_issues:
            issue_categories[issue["category"]].append(issue)
        
        category_summary = []
        for category, issues in issue_categories.items():
            category_summary.append({
                "category": category,
                "count": len(issues),
                "severity_distribution": Counter([issue["severity"] for issue in issues])
            })
        
        recommendations = [
            "Add alt text for all images",
            "Ensure proper heading hierarchy",
            "Improve color contrast ratios",
            "Add keyboard navigation support",
            "Include ARIA labels where needed"
        ]
        
        summary = {
            "total_issues": len(accessibility_issues),
            "files_with_issues": len(detailed_findings),
            "category_breakdown": category_summary,
            "critical_issues": len([issue for issue in accessibility_issues 
                                  if issue["severity"] == "critical"])
        }
        
        return AnalyticsReport(
            report_id=f"accessibility_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            analytics_type=AnalyticsType.ACCESSIBILITY_AUDIT,
            generated_at=datetime.now(),
            time_period=(datetime.now(), datetime.now()),  # Point in time audit
            summary=summary,
            detailed_findings=detailed_findings,
            recommendations=recommendations,
            action_items=[
                "Address all critical accessibility issues",
                "Implement accessibility testing in CI/CD pipeline",
                "Train content creators on accessibility best practices"
            ],
            metadata={"accessibility_standards": ["WCAG 2.1 AA"]}
        )
    
    # Quality analysis methods
    async def _analyze_completeness(self, content_path: str, filters: Dict[str, Any]) -> QualityScore:
        """Analyze documentation completeness"""
        
        try:
            with open(content_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            return QualityScore(
                metric=QualityMetric.COMPLETENESS,
                score=0,
                details={"error": str(e)},
                recommendations=["Fix file access issues"],
                confidence=1.0
            )
        
        # Check for essential sections
        essential_sections = ['introduction', 'usage', 'examples', 'api', 'parameters']
        found_sections = []
        
        for section in essential_sections:
            if re.search(rf'\b{section}\b', content, re.IGNORECASE):
                found_sections.append(section)
        
        completeness_score = (len(found_sections) / len(essential_sections)) * 100
        
        # Check for TODO/FIXME items
        todos = len(re.findall(r'TODO|FIXME|XXX', content, re.IGNORECASE))
        
        # Adjust score based on TODOs
        if todos > 0:
            completeness_score = max(0, completeness_score - (todos * 5))
        
        missing_sections = [s for s in essential_sections if s not in found_sections]
        recommendations = []
        
        if missing_sections:
            recommendations.append(f"Add missing sections: {', '.join(missing_sections)}")
        if todos > 0:
            recommendations.append(f"Complete {todos} TODO items")
        
        return QualityScore(
            metric=QualityMetric.COMPLETENESS,
            score=completeness_score,
            details={
                "found_sections": found_sections,
                "missing_sections": missing_sections,
                "todo_count": todos
            },
            recommendations=recommendations,
            confidence=0.8
        )
    
    async def _analyze_accuracy(self, content_path: str, filters: Dict[str, Any]) -> QualityScore:
        """Analyze documentation accuracy"""
        
        # Simulated accuracy analysis
        # In practice, this would use AI models to check for factual accuracy
        accuracy_score = 85.0  # Placeholder
        
        return QualityScore(
            metric=QualityMetric.ACCURACY,
            score=accuracy_score,
            details={"analysis_method": "ai_fact_check"},
            recommendations=["Verify technical details with subject matter experts"],
            confidence=0.7
        )
    
    async def _analyze_clarity(self, content_path: str, filters: Dict[str, Any]) -> QualityScore:
        """Analyze documentation clarity"""
        
        try:
            with open(content_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception:
            return QualityScore(
                metric=QualityMetric.CLARITY,
                score=0,
                details={},
                recommendations=[],
                confidence=0.0
            )
        
        # Analyze readability metrics
        sentences = re.split(r'[.!?]+', content)
        words = content.split()
        
        avg_sentence_length = len(words) / max(len(sentences), 1)
        
        # Simple readability score (Flesch-like)
        clarity_score = max(0, 100 - (avg_sentence_length - 15) * 2)
        
        recommendations = []
        if avg_sentence_length > 25:
            recommendations.append("Break down long sentences for better readability")
        
        complex_words = len([word for word in words if len(word) > 10])
        if complex_words / max(len(words), 1) > 0.1:
            recommendations.append("Consider simpler alternatives for complex terminology")
        
        return QualityScore(
            metric=QualityMetric.CLARITY,
            score=clarity_score,
            details={
                "avg_sentence_length": avg_sentence_length,
                "complex_words_ratio": complex_words / max(len(words), 1)
            },
            recommendations=recommendations,
            confidence=0.6
        )
    
    async def _analyze_structure(self, content_path: str, filters: Dict[str, Any]) -> QualityScore:
        """Analyze documentation structure"""
        
        try:
            with open(content_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception:
            return QualityScore(
                metric=QualityMetric.STRUCTURE,
                score=0,
                details={},
                recommendations=[],
                confidence=0.0
            )
        
        # Check heading hierarchy
        headings = re.findall(r'^(#{1,6})\s+(.+)$', content, re.MULTILINE)
        
        structure_score = 70  # Base score
        recommendations = []
        
        if headings:
            # Check for proper hierarchy
            heading_levels = [len(h[0]) for h in headings]
            
            # Should start with h1
            if heading_levels and heading_levels[0] != 1:
                structure_score -= 20
                recommendations.append("Start with h1 heading")
            
            # Check for skipped levels
            for i in range(1, len(heading_levels)):
                if heading_levels[i] - heading_levels[i-1] > 1:
                    structure_score -= 10
                    recommendations.append("Avoid skipping heading levels")
                    break
        else:
            structure_score = 30
            recommendations.append("Add section headings for better structure")
        
        # Check for lists and code blocks
        lists = len(re.findall(r'^\s*[-*+]\s+', content, re.MULTILINE))
        code_blocks = len(re.findall(r'```', content))
        
        if lists > 0:
            structure_score += 10
        if code_blocks > 0:
            structure_score += 10
        
        return QualityScore(
            metric=QualityMetric.STRUCTURE,
            score=min(100, structure_score),
            details={
                "headings_count": len(headings),
                "lists_count": lists,
                "code_blocks_count": code_blocks
            },
            recommendations=recommendations,
            confidence=0.8
        )
    
    async def _analyze_examples(self, content_path: str, filters: Dict[str, Any]) -> QualityScore:
        """Analyze documentation examples"""
        
        try:
            with open(content_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception:
            return QualityScore(
                metric=QualityMetric.EXAMPLES,
                score=0,
                details={},
                recommendations=[],
                confidence=0.0
            )
        
        # Count different types of examples
        code_blocks = len(re.findall(r'```[\s\S]*?```', content))
        inline_code = len(re.findall(r'`[^`]+`', content))
        
        examples_score = 0
        recommendations = []
        
        if code_blocks > 0:
            examples_score += 50
        else:
            recommendations.append("Add code examples to illustrate concepts")
        
        if inline_code > 0:
            examples_score += 20
        
        # Check for example keywords
        example_keywords = ['example', 'sample', 'demo', 'tutorial']
        example_mentions = sum(len(re.findall(rf'\b{keyword}\b', content, re.IGNORECASE)) 
                             for keyword in example_keywords)
        
        if example_mentions > 0:
            examples_score += 30
        
        if examples_score < 50:
            recommendations.append("Include more practical examples and use cases")
        
        return QualityScore(
            metric=QualityMetric.EXAMPLES,
            score=min(100, examples_score),
            details={
                "code_blocks": code_blocks,
                "inline_code": inline_code,
                "example_mentions": example_mentions
            },
            recommendations=recommendations,
            confidence=0.7
        )
    
    async def _analyze_accessibility(self, content_path: str, filters: Dict[str, Any]) -> QualityScore:
        """Analyze documentation accessibility"""
        
        accessibility_issues = await self._analyze_accessibility_issues(content_path)
        
        accessibility_score = max(0, 100 - len(accessibility_issues) * 10)
        
        recommendations = []
        if accessibility_issues:
            issue_types = set(issue["category"] for issue in accessibility_issues)
            for issue_type in issue_types:
                recommendations.append(f"Address {issue_type} accessibility issues")
        
        return QualityScore(
            metric=QualityMetric.ACCESSIBILITY,
            score=accessibility_score,
            details={"issues": accessibility_issues},
            recommendations=recommendations,
            confidence=0.8
        )
    
    async def _analyze_freshness(self, content_path: str, filters: Dict[str, Any]) -> QualityScore:
        """Analyze documentation freshness"""
        
        try:
            file_path = Path(content_path)
            last_modified = datetime.fromtimestamp(file_path.stat().st_mtime)
            days_old = (datetime.now() - last_modified).days
            
            # Freshness score decreases with age
            freshness_score = max(0, 100 - (days_old / 30) * 20)
            
            recommendations = []
            if days_old > 90:
                recommendations.append("Review and update content - last modified over 3 months ago")
            elif days_old > 180:
                recommendations.append("Content is over 6 months old - consider major revision")
            
            return QualityScore(
                metric=QualityMetric.FRESHNESS,
                score=freshness_score,
                details={"days_old": days_old, "last_modified": last_modified.isoformat()},
                recommendations=recommendations,
                confidence=1.0
            )
        except Exception as e:
            return QualityScore(
                metric=QualityMetric.FRESHNESS,
                score=50,
                details={"error": str(e)},
                recommendations=["Unable to determine file age"],
                confidence=0.0
            )
    
    async def _analyze_consistency(self, content_path: str, filters: Dict[str, Any]) -> QualityScore:
        """Analyze documentation consistency"""
        
        # This would typically compare against style guides and other documents
        consistency_score = 80  # Placeholder
        
        return QualityScore(
            metric=QualityMetric.CONSISTENCY,
            score=consistency_score,
            details={"analysis_method": "style_guide_comparison"},
            recommendations=["Ensure consistent terminology and formatting"],
            confidence=0.6
        )
    
    # Helper methods for data collection and analysis
    async def _discover_documentation_files(self) -> List[str]:
        """Discover documentation files in the workspace"""
        
        doc_patterns = ['*.md', '*.rst', '*.txt', '*.html']
        doc_files = []
        
        workspace_root = Path('/Users/agi_dev/Lukhas_PWM')
        for pattern in doc_patterns:
            doc_files.extend(str(f) for f in workspace_root.rglob(pattern))
        
        return doc_files[:50]  # Limit for demo
    
    async def _collect_usage_data(self, time_period: Tuple[datetime, datetime],
                                content_paths: Optional[List[str]],
                                filters: Dict[str, Any]) -> Dict[str, UsageMetrics]:
        """Collect usage data (simulated)"""
        
        # Simulated usage data
        usage_data = {}
        
        if content_paths is None:
            content_paths = await self._discover_documentation_files()
        
        for path in content_paths[:20]:  # Limit for demo
            usage_data[path] = UsageMetrics(
                page_views=int(100 + hash(path) % 1000),
                unique_visitors=int(50 + hash(path) % 500),
                bounce_rate=0.3 + (hash(path) % 100) / 200,
                time_on_page=120 + (hash(path) % 300),
                search_queries=[f"query_{i}" for i in range(hash(path) % 5)],
                download_count=hash(path) % 50,
                feedback_rating=3.5 + (hash(path) % 30) / 20,
                conversion_rate=0.1 + (hash(path) % 50) / 500
            )
        
        return usage_data
    
    async def _collect_performance_data(self, time_period: Tuple[datetime, datetime],
                                      content_paths: Optional[List[str]]) -> Dict[str, Dict[str, Any]]:
        """Collect performance data (simulated)"""
        
        performance_data = {}
        
        if content_paths is None:
            content_paths = await self._discover_documentation_files()
        
        for path in content_paths[:20]:
            performance_data[path] = {
                "load_time": 1.0 + (hash(path) % 30) / 10,
                "mobile_score": 70 + (hash(path) % 30),
                "accessibility_score": 80 + (hash(path) % 20),
                "performance_score": 75 + (hash(path) % 25)
            }
        
        return performance_data
    
    async def _analyze_accessibility_issues(self, content_path: str) -> List[Dict[str, Any]]:
        """Analyze accessibility issues in content"""
        
        try:
            with open(content_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception:
            return []
        
        issues = []
        
        # Check for images without alt text
        img_tags = re.findall(r'<img[^>]*>', content, re.IGNORECASE)
        for img in img_tags:
            if 'alt=' not in img.lower():
                issues.append({
                    "category": "images",
                    "severity": "high",
                    "description": "Image missing alt text",
                    "element": img[:50] + "..."
                })
        
        # Check for proper heading hierarchy
        headings = re.findall(r'^(#{1,6})\s+(.+)$', content, re.MULTILINE)
        if headings:
            heading_levels = [len(h[0]) for h in headings]
            for i in range(1, len(heading_levels)):
                if heading_levels[i] - heading_levels[i-1] > 1:
                    issues.append({
                        "category": "headings",
                        "severity": "medium",
                        "description": "Skipped heading level",
                        "element": headings[i][1]
                    })
        
        return issues
    
    # Content gaps analysis
    async def _analyze_search_gaps(self, time_period: Tuple[datetime, datetime]) -> List[ContentGap]:
        """Analyze search queries to identify content gaps"""
        
        # Simulated search gap analysis
        gaps = [
            ContentGap(
                gap_id="search_gap_1",
                title="Advanced API Authentication",
                description="Users searching for advanced authentication methods",
                priority="high",
                estimated_impact=0.8,
                suggested_content_type=ContentType.TUTORIALS,
                related_topics=["authentication", "api", "security"],
                user_requests=45,
                search_queries=["api auth", "advanced authentication", "oauth setup"]
            ),
            ContentGap(
                gap_id="search_gap_2",
                title="Troubleshooting Integration Issues",
                description="Common integration problems not covered in docs",
                priority="medium",
                estimated_impact=0.6,
                suggested_content_type=ContentType.TROUBLESHOOTING,
                related_topics=["integration", "troubleshooting", "debugging"],
                user_requests=23,
                search_queries=["integration error", "connection failed", "debug integration"]
            )
        ]
        
        return gaps
    
    async def _analyze_support_ticket_gaps(self, time_period: Tuple[datetime, datetime]) -> List[ContentGap]:
        """Analyze support tickets to identify content gaps"""
        
        # Simulated support ticket analysis
        gaps = [
            ContentGap(
                gap_id="support_gap_1",
                title="Compliance Configuration Examples",
                description="Multiple tickets about compliance setup",
                priority="high",
                estimated_impact=0.9,
                suggested_content_type=ContentType.GUIDES,
                related_topics=["compliance", "configuration", "setup"],
                user_requests=67,
                search_queries=["compliance setup", "configure compliance", "compliance examples"]
            )
        ]
        
        return gaps
    
    async def _analyze_user_feedback_gaps(self, time_period: Tuple[datetime, datetime]) -> List[ContentGap]:
        """Analyze user feedback to identify content gaps"""
        
        # Simulated user feedback analysis
        gaps = [
            ContentGap(
                gap_id="feedback_gap_1",
                title="Quick Start Video Tutorials",
                description="Users requesting video content for getting started",
                priority="medium",
                estimated_impact=0.7,
                suggested_content_type=ContentType.TUTORIALS,
                related_topics=["getting started", "video", "tutorials"],
                user_requests=34,
                search_queries=["video tutorial", "getting started video", "quick start"]
            )
        ]
        
        return gaps
    
    async def _analyze_competitive_gaps(self) -> List[ContentGap]:
        """Analyze competitive landscape to identify content gaps"""
        
        # Simulated competitive analysis
        gaps = [
            ContentGap(
                gap_id="competitive_gap_1",
                title="Performance Optimization Guide",
                description="Competitors have detailed performance guides",
                priority="medium",
                estimated_impact=0.5,
                suggested_content_type=ContentType.GUIDES,
                related_topics=["performance", "optimization", "best practices"],
                user_requests=12,
                search_queries=["performance optimization", "improve speed", "best practices"]
            )
        ]
        
        return gaps
    
    async def _prioritize_content_gaps(self, gaps: List[ContentGap]) -> List[ContentGap]:
        """Prioritize content gaps based on impact and demand"""
        
        # Sort by priority and estimated impact
        priority_order = {"high": 3, "medium": 2, "low": 1}
        
        return sorted(gaps, key=lambda x: (
            priority_order.get(x.priority, 0),
            x.estimated_impact,
            x.user_requests
        ), reverse=True)
    
    # User behavior pattern detection
    async def _detect_navigation_patterns(self, time_period: Tuple[datetime, datetime],
                                        filters: Dict[str, Any]) -> List[UserBehaviorPattern]:
        """Detect navigation patterns"""
        
        return [
            UserBehaviorPattern(
                pattern_id="nav_pattern_1",
                pattern_type="navigation",
                description="Users frequently navigate from API docs to tutorials",
                frequency=234,
                user_segments=["developers", "integrators"],
                triggers=["api_documentation_view"],
                outcomes={"tutorial_completion_rate": 0.73},
                recommendations=["Add direct links from API docs to relevant tutorials"]
            )
        ]
    
    async def _detect_search_patterns(self, time_period: Tuple[datetime, datetime],
                                    filters: Dict[str, Any]) -> List[UserBehaviorPattern]:
        """Detect search patterns"""
        
        return [
            UserBehaviorPattern(
                pattern_id="search_pattern_1",
                pattern_type="search",
                description="Users search for same terms multiple times",
                frequency=156,
                user_segments=["new_users"],
                triggers=["unsuccessful_search"],
                outcomes={"bounce_rate": 0.45},
                recommendations=["Improve search results for common queries"]
            )
        ]
    
    async def _detect_failure_patterns(self, time_period: Tuple[datetime, datetime],
                                     filters: Dict[str, Any]) -> List[UserBehaviorPattern]:
        """Detect failure patterns"""
        
        return [
            UserBehaviorPattern(
                pattern_id="failure_pattern_1",
                pattern_type="failure",
                description="Users abandon tutorial at step 3",
                frequency=89,
                user_segments=["beginners"],
                triggers=["complex_configuration_step"],
                outcomes={"completion_rate": 0.23},
                recommendations=["Simplify step 3 or add more detailed instructions"]
            )
        ]
    
    async def _detect_success_patterns(self, time_period: Tuple[datetime, datetime],
                                     filters: Dict[str, Any]) -> List[UserBehaviorPattern]:
        """Detect success patterns"""
        
        return [
            UserBehaviorPattern(
                pattern_id="success_pattern_1",
                pattern_type="success",
                description="Users who start with quick start guide have higher completion rates",
                frequency=312,
                user_segments=["all_users"],
                triggers=["quick_start_guide_view"],
                outcomes={"completion_rate": 0.87},
                recommendations=["Promote quick start guide more prominently"]
            )
        ]
    
    async def _detect_engagement_patterns(self, time_period: Tuple[datetime, datetime],
                                        filters: Dict[str, Any]) -> List[UserBehaviorPattern]:
        """Detect engagement patterns"""
        
        return [
            UserBehaviorPattern(
                pattern_id="engagement_pattern_1",
                pattern_type="engagement",
                description="Users engage more with interactive content",
                frequency=198,
                user_segments=["developers", "technical_users"],
                triggers=["interactive_content_view"],
                outcomes={"time_on_page": 345, "return_rate": 0.64},
                recommendations=["Create more interactive content and examples"]
            )
        ]

# Export main analytics components
__all__ = ['DocumentationAnalytics', 'AnalyticsReport', 'QualityScore', 
           'UsageMetrics', 'ContentGap', 'UserBehaviorPattern']
