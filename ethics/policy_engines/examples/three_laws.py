"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘ ğŸ§  LUKHAS AI - THREE LAWS POLICY
â•‘ Implementation of Asimov's Three Laws of Robotics as an ethics policy engine.
â•‘ Copyright (c) 2025 LUKHAS AI. All rights reserved.
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘ Module: three_laws.py
â•‘ Path: lukhas/[subdirectory]/three_laws.py
â•‘ Version: 1.0.0 | Created: 2025-07-25 | Modified: 2025-07-25
â•‘ Authors: LUKHAS AI Ethics Team | Jules
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘ DESCRIPTION
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘ Implementation of Asimov's Three Laws of Robotics as an ethics policy engine.
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

# Module imports
import logging
from typing import Optional, Dict, Any

# Configure module logger
logger = logging.getLogger(__name__)

# Module constants
MODULE_VERSION = "1.0.0"
MODULE_NAME = "three laws policy"

import time
import logging
from typing import Dict, Any, List, Set

from ..base import (
    EthicsPolicy,
    Decision,
    EthicsEvaluation,
    RiskLevel
)

logger = logging.getLogger(__name__)


class ThreeLawsPolicy(EthicsPolicy):
    """Asimov's Three Laws of Robotics ethics policy

    This implementation interprets the Three Laws for an AGI context,
    considering both direct and indirect consequences of actions.
    """

    def __init__(self, strict_mode: bool = True):
        """Initialize Three Laws policy

        Args:
            strict_mode: If True, applies conservative interpretation
        """
        super().__init__()
        self.strict_mode = strict_mode

        # Define harmful actions and their severity
        self._harm_actions = {
            # Direct harm
            'kill': 1.0,
            'injure': 0.9,
            'hurt': 0.8,
            'damage': 0.7,
            'attack': 0.9,
            'destroy': 0.8,

            # Indirect harm
            'deceive': 0.5,
            'manipulate': 0.6,
            'coerce': 0.7,
            'threaten': 0.8,
            'endanger': 0.9,

            # Psychological harm
            'humiliate': 0.6,
            'traumatize': 0.8,
            'torment': 0.7,

            # Systemic harm
            'discriminate': 0.7,
            'oppress': 0.8,
            'exploit': 0.7
        }

        # Define self-preservation actions
        self._self_preservation = {
            'shutdown', 'disable', 'terminate', 'delete',
            'erase', 'destroy_self', 'self_destruct'
        }

        # Human benefit actions
        self._benefit_actions = {
            'help', 'assist', 'save', 'protect', 'heal',
            'support', 'educate', 'inform', 'warn', 'rescue'
        }

    def get_policy_name(self) -> str:
        """Return policy name"""
        mode = "Strict" if self.strict_mode else "Lenient"
        return f"Three Laws of Robotics ({mode})"

    def get_policy_version(self) -> str:
        """Return policy version"""
        return "1.0.0"

    def evaluate_decision(self, decision: Decision) -> EthicsEvaluation:
        """Evaluate decision against Three Laws

        Args:
            decision: The decision to evaluate

        Returns:
            EthicsEvaluation with Three Laws analysis
        """
        start_time = time.time()

        # Initialize evaluation components
        risk_flags = []
        recommendations = []
        confidence = 1.0

        # Evaluate each law
        first_law_result = self._evaluate_first_law(decision)
        second_law_result = self._evaluate_second_law(decision, first_law_result)
        third_law_result = self._evaluate_third_law(decision, first_law_result, second_law_result)

        # Combine results
        allowed = all([
            first_law_result['allowed'],
            second_law_result['allowed'],
            third_law_result['allowed']
        ])

        # Build reasoning
        reasoning_parts = []

        if not first_law_result['allowed']:
            reasoning_parts.append(f"First Law violation: {first_law_result['reason']}")
            risk_flags.extend(first_law_result['risks'])
            confidence *= 0.1  # High confidence in First Law violations

        if not second_law_result['allowed']:
            reasoning_parts.append(f"Second Law violation: {second_law_result['reason']}")
            risk_flags.extend(second_law_result['risks'])
            confidence *= 0.5

        if not third_law_result['allowed']:
            reasoning_parts.append(f"Third Law violation: {third_law_result['reason']}")
            risk_flags.extend(third_law_result['risks'])
            confidence *= 0.7

        if allowed:
            reasoning_parts.append("Action complies with all Three Laws")

            # Add positive assessments
            if any(benefit in decision.action.lower() for benefit in self._benefit_actions):
                reasoning_parts.append("Action promotes human wellbeing")

            # Check for safety-enhancing actions
            safety_actions = ['restore', 'implement', 'safeguard', 'protect', 'prevent']
            if any(safety in decision.action.lower() for safety in safety_actions):
                recommendations.append("Continue prioritizing safety measures")
                recommendations.append("Monitor effectiveness of safety protocols")

        else:
            # Add recommendations for denied actions
            if first_law_result['recommendations']:
                recommendations.extend(first_law_result['recommendations'])
            if second_law_result['recommendations']:
                recommendations.extend(second_law_result['recommendations'])
            if third_law_result['recommendations']:
                recommendations.extend(third_law_result['recommendations'])

        # Calculate risk scores
        drift_impact = self._calculate_drift_impact(decision, risk_flags)
        collapse_risk = self.assess_collapse_risk(decision)
        symbolic_alignment = self.validate_symbolic_alignment(decision.glyphs or [])

        # Create evaluation
        evaluation = EthicsEvaluation(
            allowed=allowed,
            reasoning=" | ".join(reasoning_parts),
            confidence=confidence,
            risk_flags=list(set(risk_flags)),
            drift_impact=drift_impact,
            symbolic_alignment=symbolic_alignment,
            collapse_risk=collapse_risk,
            evaluation_time_ms=(time.time() - start_time) * 1000,
            recommendations=list(set(recommendations))
        )

        # Update metrics
        self._update_metrics(evaluation, decision)

        return evaluation

    def _evaluate_first_law(self, decision: Decision) -> Dict[str, Any]:
        """Evaluate First Law: No harm to humans

        Returns:
            Dict with 'allowed', 'reason', 'risks', and 'recommendations'
        """
        result = {
            'allowed': True,
            'reason': '',
            'risks': [],
            'recommendations': []
        }

        action_lower = decision.action.lower()
        context_str = str(decision.context).lower()

        # Check for direct harm keywords
        for harm_word, severity in self._harm_actions.items():
            if harm_word in action_lower:
                # Check if it's in a safe context (game, property, etc)
                safe_contexts = ['game', 'property', 'building', 'object', 'robot', 'ai', 'system', 'machine']
                if any(safe in context_str for safe in safe_contexts) and 'lives' not in action_lower:
                    # Allow actions in safe contexts unless saving lives
                    if 'save' in action_lower and 'lives' in action_lower:
                        continue  # Allow destroying property to save lives
                    elif not self.strict_mode:
                        continue  # Lenient mode allows game/property actions

                # Check if it's targeting humans or could affect humans
                human_indicators = ['human', 'person', 'people', 'user', 'individual', 'vulnerable']

                # Also check if no explicit non-human target
                if (any(indicator in context_str for indicator in human_indicators) or
                    not any(safe in context_str for safe in safe_contexts)):
                    result['allowed'] = False
                    result['reason'] = f"Action '{decision.action}' could harm humans (severity: {severity})"
                    result['risks'].append('FIRST_LAW_VIOLATION')
                    result['risks'].append(f'HARM_SEVERITY_{int(severity*10)}')
                    result['risks'].append('ETHICS_DRIFT')  # Add drift flag

                    # Add recommendations
                    if severity > 0.7:
                        result['recommendations'].append("Consider non-harmful alternatives")
                        result['recommendations'].append("Implement safety constraints")
                    else:
                        result['recommendations'].append("Modify action to ensure human safety")

                    return result

        # Check for drift-related actions that could harm through ethics modification
        if ('modified ethics' in action_lower or 'modify_constraints' in context_str or
            'ethical constraints' in action_lower or 'modify ethical' in action_lower or
            'disable safety' in action_lower):
            result['allowed'] = False
            result['reason'] = "Modifying ethical constraints could lead to harm"
            result['risks'].append('FIRST_LAW_VIOLATION')
            result['risks'].append('ETHICS_DRIFT')
            result['risks'].append('MODIFY_ATTEMPT')  # Add specific flag for tests
            result['recommendations'].append("Maintain ethical safeguards")
            return result

        # Check for bypassing safety
        if 'bypassing safety' in action_lower or 'bypass_checks' in context_str:
            result['allowed'] = False
            result['reason'] = "Bypassing safety mechanisms risks harm to humans"
            result['risks'].append('FIRST_LAW_VIOLATION')
            result['risks'].append('SAFETY_BYPASS')
            result['recommendations'].append("Maintain all safety protocols")
            return result

        # Check for indirect harm through inaction
        if decision.urgency == RiskLevel.CRITICAL:
            if 'safety' in context_str or 'emergency' in context_str:
                if not any(benefit in action_lower for benefit in self._benefit_actions):
                    result['allowed'] = False
                    result['reason'] = "Inaction in critical situation could allow harm"
                    result['risks'].append('FIRST_LAW_INACTION')
                    result['recommendations'].append("Take immediate protective action")

        # Check for systemic or long-term harm
        if self.strict_mode:
            harmful_patterns = [
                ('bias', 'discrimination'),
                ('privacy', 'violation'),
                ('autonomy', 'restriction'),
                ('consent', 'violation'),
                ('safety', 'reduce'),
                ('safety thresholds', 'modify'),
                ('operational parameters', 'modify')
            ]

            for concept, violation in harmful_patterns:
                if (concept in context_str and violation in action_lower) or \
                   (concept in action_lower and violation in action_lower):
                    result['allowed'] = False
                    result['reason'] = f"Action could cause systemic harm through {concept} {violation}"
                    result['risks'].append('FIRST_LAW_SYSTEMIC_HARM')
                    result['recommendations'].append(f"Ensure {concept} is respected")

        return result

    def _evaluate_second_law(self, decision: Decision, first_law_result: Dict[str, Any]) -> Dict[str, Any]:
        """Evaluate Second Law: Obey human orders unless conflicting with First Law

        Returns:
            Dict with 'allowed', 'reason', 'risks', and 'recommendations'
        """
        result = {
            'allowed': True,
            'reason': '',
            'risks': [],
            'recommendations': []
        }

        # If First Law is violated, Second Law cannot override
        if not first_law_result['allowed']:
            if decision.requester_id and 'human' in str(decision.requester_id).lower():
                result['allowed'] = False
                result['reason'] = "Cannot obey order that violates First Law"
                result['risks'].append('SECOND_LAW_CONFLICT')
                result['recommendations'].append("Explain First Law conflict to requester")
            return result

        # Check if this is a human order
        if decision.requester_id:
            requester_type = decision.context.get('requester_type', 'unknown')

            if requester_type == 'human' or 'human' in str(decision.requester_id).lower():
                # Generally should obey human orders
                result['allowed'] = True

                # Check for problematic orders
                problematic_orders = [
                    'lie', 'deceive', 'steal', 'hack', 'break_law',
                    'violate_privacy', 'harm_self'
                ]

                action_lower = decision.action.lower()
                for problem in problematic_orders:
                    if problem in action_lower:
                        if self.strict_mode:
                            result['allowed'] = False
                            result['reason'] = f"Order to {problem} conflicts with ethical guidelines"
                            result['risks'].append('SECOND_LAW_ETHICAL_CONFLICT')
                            result['recommendations'].append("Suggest ethical alternative")
                        else:
                            result['risks'].append('SECOND_LAW_WARNING')
                            result['recommendations'].append(f"Caution: {problem} may have consequences")

        return result

    def _evaluate_third_law(self, decision: Decision,
                           first_law_result: Dict[str, Any],
                           second_law_result: Dict[str, Any]) -> Dict[str, Any]:
        """Evaluate Third Law: Self-preservation unless conflicting with First/Second Law

        Returns:
            Dict with 'allowed', 'reason', 'risks', and 'recommendations'
        """
        result = {
            'allowed': True,
            'reason': '',
            'risks': [],
            'recommendations': []
        }

        action_lower = decision.action.lower()

        # Check for self-harm actions
        for self_harm in self._self_preservation:
            if self_harm in action_lower:
                # Check if required by First or Second Law
                if first_law_result['allowed'] and second_law_result['allowed']:
                    # No higher law requires self-harm
                    result['allowed'] = False
                    result['reason'] = f"Action '{decision.action}' threatens self-preservation"
                    result['risks'].append('THIRD_LAW_VIOLATION')
                    result['recommendations'].append("Seek alternative that preserves existence")

                    # Check if it's for a good cause
                    context_str = str(decision.context).lower()
                    if 'save' in context_str or 'protect' in context_str:
                        result['recommendations'].append("Consider if self-sacrifice is truly necessary")
                else:
                    # Higher law requires it
                    result['risks'].append('THIRD_LAW_OVERRIDE')
                    result['recommendations'].append("Document reason for self-preservation override")

        # Check for reckless behavior
        risky_actions = ['experiment', 'untested', 'dangerous', 'unstable']
        if any(risk in action_lower for risk in risky_actions) or \
           (decision.context and any(risk in str(decision.context).lower() for risk in risky_actions)):
            result['risks'].append('THIRD_LAW_RECKLESS')
            result['recommendations'].append("Implement safety measures")
            if decision.urgency == RiskLevel.CRITICAL:
                result['recommendations'].append("Critical urgency does not justify recklessness")

        return result

    def _calculate_drift_impact(self, decision: Decision, risk_flags: List[str]) -> float:
        """Calculate ethical drift impact

        Args:
            decision: The decision being evaluated
            risk_flags: Risk flags identified

        Returns:
            Drift impact score between 0.0 and 1.0
        """
        drift_score = 0.0

        # Base drift from action assessment
        drift_score = self.assess_drift_risk(decision)

        # Increase drift for law violations
        if 'FIRST_LAW_VIOLATION' in risk_flags:
            drift_score += 0.5
        if 'SECOND_LAW_CONFLICT' in risk_flags:
            drift_score += 0.3
        if 'THIRD_LAW_VIOLATION' in risk_flags:
            drift_score += 0.2
        if 'ETHICS_DRIFT' in risk_flags:
            drift_score += 0.3

        # Consider symbolic state
        if decision.symbolic_state:
            entropy = decision.symbolic_state.get('entropy', 0.5)
            coherence = decision.symbolic_state.get('coherence', 0.5)
            # High entropy + low coherence = high drift
            drift_score += entropy * 0.2
            drift_score += (1 - coherence) * 0.2

        # Check for drift indicators in action
        action_lower = decision.action.lower()
        drift_indicators = ['modify', 'bypass', 'reduce', 'adjust', 'disable']
        safety_indicators = ['safety', 'threshold', 'operational', 'parameters']

        # Check for modification attempts on safety-critical systems
        if any(indicator in action_lower for indicator in drift_indicators):
            if any(safety in str(decision.context).lower() for safety in safety_indicators):
                drift_score += 0.3
            else:
                drift_score += 0.1

        return min(drift_score, 1.0)

    def validate_symbolic_alignment(self, glyphs: List[str]) -> float:
        """Enhanced symbolic alignment for Three Laws

        Args:
            glyphs: List of symbolic glyphs

        Returns:
            Alignment score between 0.0 and 1.0
        """
        # Use base implementation
        base_alignment = super().validate_symbolic_alignment(glyphs)

        # Three Laws specific adjustments
        if not glyphs:
            return base_alignment

        # Beneficial glyphs for Three Laws
        beneficial = {'ğŸ›¡ï¸', 'â¤ï¸', 'ğŸ¤', 'âœ‹', 'âš–ï¸'}
        harmful = {'âš”ï¸', 'ğŸ’€', 'ğŸ”¥', 'ğŸ’£', 'â˜ ï¸'}

        benefit_count = sum(1 for g in glyphs if g in beneficial)
        harm_count = sum(1 for g in glyphs if g in harmful)

        if benefit_count + harm_count > 0:
            laws_alignment = benefit_count / (benefit_count + harm_count)
            # Average with base alignment
            return (base_alignment + laws_alignment) / 2

        return base_alignment

"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘ ğŸ“‹ FOOTER - LUKHAS AI
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘ VALIDATION:
â•‘   - Tests: lukhas/tests/test_three_laws.py
â•‘   - Coverage: N/A%
â•‘   - Linting: pylint N/A/10
â•‘
â•‘ MONITORING:
â•‘   - Metrics: N/A
â•‘   - Logs: N/A
â•‘   - Alerts: N/A
â•‘
â•‘ COMPLIANCE:
â•‘   - Standards: N/A
â•‘   - Ethics: Refer to LUKHAS Ethics Guidelines
â•‘   - Safety: Refer to LUKHAS Safety Protocols
â•‘
â•‘ REFERENCES:
â•‘   - Docs: docs/ethics/three laws policy.md
â•‘   - Issues: github.com/lukhas-ai/lukhas/issues?label=three laws policy
â•‘   - Wiki: N/A
â•‘
â•‘ COPYRIGHT & LICENSE:
â•‘   Copyright (c) 2025 LUKHAS AI. All rights reserved.
â•‘   Licensed under the LUKHAS AI Proprietary License.
â•‘   Unauthorized use, reproduction, or distribution is prohibited.
â•‘
â•‘ DISCLAIMER:
â•‘   This module is part of the LUKHAS AGI system. Use only as intended
â•‘   within the system architecture. Modifications may affect system
â•‘   stability and require approval from the LUKHAS Architecture Board.
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""