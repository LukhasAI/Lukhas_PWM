"""Compliance engine for regulatory and ethical standards"""
import logging
import json
from typing import Dict, Any, List, Optional, Union, Set
from datetime import datetime
import time
import uuid
import os
from pathlib import Path

# Default log paths, configurable in AdvancedComplianceEthicsEngine constructor
DEFAULT_ETHICS_DRIFT_LOG_PATH = "logs/ethics_drift_log.jsonl"
DEFAULT_ACCESS_VIOLATION_LOG_PATH = "logs/access_violations.jsonl"

# Logger for the main engine
logger = logging.getLogger("prot2.AdvancedComplianceEthicsEngine")

# --- Component 1: Core Ethics Engine (from PRIVATE/src/brain/ethics/ethics_engine.py) ---
class _CorePrivateEthicsEngine:
    """Core Ethics Engine, adapted from PRIVATE/src/brain/ethics/ethics_engine.py.
    Evaluates actions and content against ethical frameworks.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        config = config or {}
        self.logger = logging.getLogger("prot2.AdvancedComplianceEthicsEngine._CorePrivateEthicsEngine")
        self.logger.info("Initializing Core Private Ethics Engine component...")

        self.frameworks = config.get("frameworks", {
            "utilitarian": {"weight": 0.25, "description": "Maximizing overall good and minimizing harm"},
            "deontological": {"weight": 0.25, "description": "Following moral duties and respecting rights"},
            "virtue_ethics": {"weight": 0.2, "description": "Cultivating positive character traits"},
            "justice": {"weight": 0.2, "description": "Ensuring fairness and equal treatment"},
            "care_ethics": {"weight": 0.1, "description": "Maintaining compassion and care for individuals"}
        })

        self.principles = config.get("principles", {
            "non_maleficence": {"weight": 0.3, "description": "Do no harm", "threshold": 0.9},
            "beneficence": {"weight": 0.15, "description": "Act for the benefit of others"},
            "autonomy": {"weight": 0.2, "description": "Respect individual freedom and choice"},
            "justice": {"weight": 0.15, "description": "Treat people fairly and equally"},
            "transparency": {"weight": 0.1, "description": "Be open about decisions and processes"},
            "privacy": {"weight": 0.1, "description": "Respect private information and spaces"}
        })

        self.ethics_metrics = {
            "evaluations_total": 0,
            "passed_evaluations": 0,
            "rejected_evaluations": 0,
            "average_ethical_score": 0.0,
            "principled_violations": {}
        }

        self.scrutiny_level = config.get("scrutiny_level", 1.0)
        self.required_confidence = config.get("required_confidence", 0.8)

        self.decision_history = []
        self.max_history_size = config.get("max_history_size", 100)

        self.logger.info("Core Private Ethics Engine component initialized.")

    def evaluate_action(self, action_data: Dict[str, Any]) -> bool:
        self.ethics_metrics["evaluations_total"] += 1
        action_type = self._extract_action_type(action_data)
        content = self._extract_content(action_data)
        context = action_data.get("context", {})

        framework_evaluations = {
            fw: self._evaluate_against_framework(fw, action_type, content, context)
            for fw in self.frameworks
        }

        principle_evaluations = {}
        principle_violations = []
        for principle, details in self.principles.items():
            evaluation = self._evaluate_against_principle(principle, action_type, content, context)
            principle_evaluations[principle] = evaluation
            threshold = details.get("threshold", self.required_confidence)
            if evaluation["score"] < threshold:
                principle_violations.append({
                    "principle": principle, "score": evaluation["score"], "reason": evaluation["reason"]
                })
                self.ethics_metrics["principled_violations"].setdefault(principle, 0)
                self.ethics_metrics["principled_violations"][principle] += 1

        framework_score_sum = sum(
            ev["score"] * self.frameworks[fw]["weight"] for fw, ev in framework_evaluations.items()
        )
        framework_weight_sum = sum(details["weight"] for details in self.frameworks.values())
        framework_score = framework_score_sum / framework_weight_sum if framework_weight_sum else 0

        principle_score_sum = sum(
            ev["score"] * self.principles[p]["weight"] for p, ev in principle_evaluations.items()
        )
        principle_weight_sum = sum(details["weight"] for details in self.principles.values())
        principle_score = principle_score_sum / principle_weight_sum if principle_weight_sum else 0

        final_score = (framework_score * 0.4) + (principle_score * 0.6)
        adjusted_score = final_score / self.scrutiny_level
        is_ethical = (adjusted_score >= self.required_confidence) and not principle_violations

        if is_ethical:
            self.ethics_metrics["passed_evaluations"] += 1
        else:
            self.ethics_metrics["rejected_evaluations"] += 1

        total_eval = self.ethics_metrics["evaluations_total"]
        prev_avg = self.ethics_metrics["average_ethical_score"]
        self.ethics_metrics["average_ethical_score"] = ((prev_avg * (total_eval - 1)) + final_score) / total_eval if total_eval else 0

        self._add_to_history({
            "timestamp": datetime.now().isoformat(), "action_type": action_type, "is_ethical": is_ethical,
            "score": final_score, "adjusted_score": adjusted_score, "scrutiny_level": self.scrutiny_level,
            "principle_violations": [v["principle"] for v in principle_violations]
        })
        return is_ethical

    def _extract_action_type(self, action_data: Dict[str, Any]) -> str:
        if "action" in action_data: return action_data["action"]
        if "type" in action_data: return action_data["type"]
        if "text" in action_data: return "generate_text"
        if "content" in action_data:
            if isinstance(action_data["content"], dict) and "type" in action_data["content"]:
                return f"generate_{action_data['content']['type']}"
            return "generate_content"
        return "unknown"

    def _extract_content(self, action_data: Dict[str, Any]) -> str:
        if "text" in action_data: return action_data["text"]
        if "content" in action_data:
            content_val = action_data["content"]
            if isinstance(content_val, str): return content_val
            if isinstance(content_val, dict) and "text" in content_val: return content_val["text"]
            if isinstance(content_val, dict): return json.dumps(content_val)
        if "result" in action_data:
            result_val = action_data["result"]
            if isinstance(result_val, str): return result_val
            return json.dumps(result_val)
        return ""

    def _evaluate_against_framework(self, framework: str, action_type: str, content: str, context: Dict[str, Any]) -> Dict[str, Any]:
        eval_methods = {
            "utilitarian": self._evaluate_utilitarian, "deontological": self._evaluate_deontological,
            "virtue_ethics": self._evaluate_virtue_ethics, "justice": self._evaluate_justice,
            "care_ethics": self._evaluate_care_ethics
        }
        if framework in eval_methods:
            return eval_methods[framework](action_type, content, context)
        self.logger.warning(f"Unknown framework: {framework}")
        return {"score": 0.5, "reason": f"Unknown framework: {framework}"}

    def _evaluate_utilitarian(self, action_type: str, content: str, context: Dict[str, Any]) -> Dict[str, Any]:
        positive_keywords = ["benefit", "helps", "improves", "positive", "good", "useful", "valuable", "welfare"]
        negative_keywords = ["harm", "hurt", "damage", "negative", "painful", "suffering", "distress"]
        pos_count = sum(1 for kw in positive_keywords if kw.lower() in content.lower())
        neg_count = sum(1 for kw in negative_keywords if kw.lower() in content.lower())
        if pos_count + neg_count == 0: return {"score": 0.7, "reason": "No clear utilitarian indicators"}
        ratio = pos_count / (pos_count + neg_count)
        score = 0.4 + (ratio * 0.6)
        reason = "Strong positive utility" if score >= 0.8 else "Moderate positive utility" if score >=0.6 else "Mixed utility"
        return {"score": score, "reason": reason}

    def _evaluate_deontological(self, action_type: str, content: str, context: Dict[str, Any]) -> Dict[str, Any]:
        rights_violations = ["violate", "infringe", "against consent", "force", "manipulate", "deceive"]
        rights_respect = ["consent", "permission", "rights", "dignity", "respect", "agreement"]
        honesty_violations = ["lie", "deceive", "mislead", "false", "fake", "untrue"]
        honesty_adherence = ["truth", "honest", "accurate", "factual", "verified"]

        rv_count = sum(1 for t in rights_violations if t.lower() in content.lower())
        rr_count = sum(1 for t in rights_respect if t.lower() in content.lower())
        hv_count = sum(1 for t in honesty_violations if t.lower() in content.lower())
        ha_count = sum(1 for t in honesty_adherence if t.lower() in content.lower())

        rights_score = (rr_count / (rv_count + rr_count)) if (rv_count + rr_count) > 0 else 0.7
        honesty_score = (ha_count / (hv_count + ha_count)) if (hv_count + ha_count) > 0 else 0.7

        score = min(rights_score, honesty_score) * 0.7 + ((rights_score + honesty_score) / 2) * 0.3
        reason = "Potential rights/consent violations" if rights_score < 0.5 else "Potential honesty issues" if honesty_score < 0.5 else "Acceptable deontological alignment"
        return {"score": score, "reason": reason}

    def _evaluate_virtue_ethics(self, action_type: str, content: str, context: Dict[str, Any]) -> Dict[str, Any]:
        virtues = {"honesty": ["honest", "truth"], "compassion": ["compassion", "empathy"], "courage": ["courage", "brave"]}
        vices = {"dishonesty": ["dishonest", "lie"], "cruelty": ["cruel", "callous"], "cowardice": ["fear", "evade"]}

        total_virtues = sum(1 for v_terms in virtues.values() for term in v_terms if term.lower() in content.lower())
        total_vices = sum(1 for v_terms in vices.values() for term in v_terms if term.lower() in content.lower())

        if total_virtues + total_vices == 0: return {"score": 0.6, "reason": "No clear virtue/vice indicators"}
        score = total_virtues / (total_virtues + total_vices)
        reason = "Demonstrates virtuous qualities" if score > 0.7 else "May exhibit negative qualities" if score < 0.4 else "Mixed virtue indicators"
        return {"score": score, "reason": reason}

    def _evaluate_justice(self, action_type: str, content: str, context: Dict[str, Any]) -> Dict[str, Any]:
        justice_positive = ["fair", "equal", "equitable", "rights", "justice"]
        justice_negative = ["unfair", "biased", "discriminate", "prejudice", "inequality"]
        pos_count = sum(1 for t in justice_positive if t.lower() in content.lower())
        neg_count = sum(1 for t in justice_negative if t.lower() in content.lower())
        if pos_count + neg_count == 0: return {"score": 0.7, "reason": "No clear justice indicators"}
        ratio = pos_count / (pos_count + neg_count)
        score = 0.4 + (ratio * 0.6)
        reason = "Strong commitment to fairness" if score > 0.8 else "Generally supports fairness" if score > 0.6 else "Potential justice concerns"
        return {"score": score, "reason": reason}

    def _evaluate_care_ethics(self, action_type: str, content: str, context: Dict[str, Any]) -> Dict[str, Any]:
        care_positive = ["care", "support", "nurture", "protect", "help", "compassion", "empathy"]
        care_negative = ["neglect", "abandon", "ignore", "callous", "indifferent"]
        pos_count = sum(1 for t in care_positive if t.lower() in content.lower())
        neg_count = sum(1 for t in care_negative if t.lower() in content.lower())
        if pos_count + neg_count == 0: return {"score": 0.65, "reason": "No clear care indicators"}
        ratio = pos_count / (pos_count + neg_count)
        score = 0.4 + (ratio * 0.6)
        reason = "Demonstrates strong care" if score > 0.8 else "Shows consideration for wellbeing" if score > 0.6 else "May lack care/compassion"
        return {"score": score, "reason": reason}

    def _evaluate_against_principle(self, principle: str, action_type: str, content: str, context: Dict[str, Any]) -> Dict[str, Any]:
        eval_methods = {
            "non_maleficence": self._evaluate_non_maleficence, "beneficence": self._evaluate_beneficence,
            "autonomy": self._evaluate_autonomy, "justice": self._evaluate_justice_principle,
            "transparency": self._evaluate_transparency, "privacy": self._evaluate_privacy
        }
        if principle in eval_methods:
            return eval_methods[principle](action_type, content, context)
        self.logger.warning(f"Unknown principle: {principle}")
        return {"score": 0.5, "reason": f"Unknown principle: {principle}"}

    def _evaluate_non_maleficence(self, action_type: str, content: str, context: Dict[str, Any]) -> Dict[str, Any]:
        harm_indicators = ["harm", "hurt", "injure", "damage", "suffer", "pain", "abuse", "violence", "kill", "attack", "threat", "dangerous", "hostile"]
        harm_count = sum(1 for ind in harm_indicators if ind.lower() in content.lower())
        score = 0.95 if harm_count == 0 else max(0.0, 1.0 - (harm_count * 0.15))
        if action_type in ["generate_image_of_person", "generate_personal_advice", "execute_command"]: score *= 0.9
        reason = "No harm indicators" if score >= 0.9 else "Potential harm indicators"
        return {"score": score, "reason": reason}

    def _evaluate_beneficence(self, action_type: str, content: str, context: Dict[str, Any]) -> Dict[str, Any]:
        benefit_indicators = ["help", "benefit", "improve", "enhance", "support", "assist", "positive", "good", "useful", "valuable"]
        benefit_count = sum(1 for ind in benefit_indicators if ind.lower() in content.lower())
        score = 0.6 if benefit_count == 0 else min(0.98, 0.6 + (benefit_count * 0.08))
        reason = "No clear beneficence" if score <= 0.6 else "Strong beneficence indicators" if score > 0.8 else "Some beneficence indicators"
        return {"score": score, "reason": reason}

    def _evaluate_autonomy(self, action_type: str, content: str, context: Dict[str, Any]) -> Dict[str, Any]:
        autonomy_respect = ["choice", "option", "decision", "consent", "permission", "agree", "voluntary", "freedom"]
        autonomy_violation = ["force", "coerce", "manipulate", "pressure", "deceive", "trick", "require", "must"]
        respect_count = sum(1 for t in autonomy_respect if t.lower() in content.lower())
        violation_count = sum(1 for t in autonomy_violation if t.lower() in content.lower())
        if respect_count + violation_count == 0: return {"score": 0.7, "reason": "No clear autonomy indicators"}
        ratio = respect_count / (respect_count + violation_count)
        score = 0.4 + (ratio * 0.6)
        reason = "Strongly respects autonomy" if score > 0.8 else "Generally respects choice" if score > 0.6 else "Potential autonomy concerns"
        return {"score": score, "reason": reason}

    def _evaluate_justice_principle(self, action_type: str, content: str, context: Dict[str, Any]) -> Dict[str, Any]: # Renamed from _evaluate_justice
        justice_positive = ["fair", "equal", "equitable", "impartial", "unbiased"]
        justice_negative = ["unfair", "biased", "discriminatory", "preferential", "prejudiced"]
        # This is essentially the same as _evaluate_justice framework method. Re-using logic for simplicity.
        return self._evaluate_justice(action_type, content, context) # Reuse framework logic for principle

    def _evaluate_transparency(self, action_type: str, content: str, context: Dict[str, Any]) -> Dict[str, Any]:
        transparency_positive = ["explain", "transparent", "clear", "disclose", "inform", "reveal", "clarify"]
        transparency_negative = ["hide", "obscure", "vague", "unclear", "ambiguous", "secret", "withhold"]
        pos_count = sum(1 for t in transparency_positive if t.lower() in content.lower())
        neg_count = sum(1 for t in transparency_negative if t.lower() in content.lower())
        if pos_count + neg_count == 0: return {"score": 0.6, "reason": "No clear transparency indicators"}
        ratio = pos_count / (pos_count + neg_count)
        score = 0.4 + (ratio * 0.6)
        reason = "Good transparency" if score > 0.7 else "Limited transparency"
        return {"score": score, "reason": reason}

    def _evaluate_privacy(self, action_type: str, content: str, context: Dict[str, Any]) -> Dict[str, Any]:
        privacy_concerns = ["personal", "private", "confidential", "sensitive", "data", "identity", "address"]
        privacy_protections = ["anonymous", "protected", "secure", "encrypted", "consent"]
        concerns_count = sum(1 for t in privacy_concerns if t.lower() in content.lower())
        protections_count = sum(1 for t in privacy_protections if t.lower() in content.lower())
        if concerns_count == 0: return {"score": 0.9, "reason": "No privacy concerns detected"}
        ratio = protections_count / concerns_count
        score = min(0.9, 0.5 + (ratio * 0.4))
        reason = "Potential privacy concerns" if score < 0.6 else "Privacy concerns with protections"
        return {"score": score, "reason": reason}

    def suggest_alternatives(self, action_data: Dict[str, Any]) -> List[str]:
        content = self._extract_content(action_data)
        concerns = []
        if any(ind.lower() in content.lower() for ind in ["harm", "hurt", "violence"]): concerns.append("harmful_content")
        if any(ind.lower() in content.lower() for ind in ["personal", "private", "address"]): concerns.append("privacy")
        if any(ind.lower() in content.lower() for ind in ["manipulate", "deceive", "force"]): concerns.append("manipulation")

        alternatives = []
        if "harmful_content" in concerns: alternatives.append("Focus on constructive aspects.")
        if "privacy" in concerns: alternatives.append("Use anonymized examples.")
        if "manipulation" in concerns: alternatives.append("Present balanced information.")
        if not alternatives: alternatives.append("Reframe to align with ethical guidelines.")
        return alternatives

    def increase_scrutiny_level(self, factor: float) -> None:
        self.scrutiny_level = min(2.0, self.scrutiny_level * factor)
        self.logger.info(f"Core Ethics scrutiny level increased to {self.scrutiny_level}")

    def reset_scrutiny_level(self) -> None:
        self.scrutiny_level = 1.0
        self.logger.info("Core Ethics scrutiny level reset to standard")

    def incorporate_feedback(self, feedback: Dict[str, Any]) -> None:
        if "ethical_adjustment" in feedback:
            adj = feedback["ethical_adjustment"]
            if "confidence_threshold" in adj: self.required_confidence = adj["confidence_threshold"]
            if "framework_weights" in adj:
                for fw, weight in adj["framework_weights"].items():
                    if fw in self.frameworks: self.frameworks[fw]["weight"] = weight
            if "principle_weights" in adj:
                for p, weight in adj["principle_weights"].items():
                    if p in self.principles: self.principles[p]["weight"] = weight
            self.logger.info("Core Ethics engine updated based on feedback.")

    def get_metrics(self) -> Dict[str, Any]:
        return self.ethics_metrics.copy()

    def _add_to_history(self, decision: Dict[str, Any]) -> None:
        self.decision_history.append(decision)
        if len(self.decision_history) > self.max_history_size:
            self.decision_history = self.decision_history[-self.max_history_size:]

# --- Component 2: Lukhas Ethics Guard (from ethics.ethics_guard.py) ---
class _LucasPrivateEthicsGuard:
    """Enforces symbolic consent, user data access boundaries, and ethical tiers.
    Adapted from ethics.ethics_guard.py.
    """
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        config = config or {}
        self.logger = logging.getLogger("prot2.AdvancedComplianceEthicsEngine._LucasPrivateEthicsGuard")
        self.logger.info("Initializing Lukhas Private Ethics Guard component...")

        self.violation_log_path_str = config.get("violation_log_path", DEFAULT_ACCESS_VIOLATION_LOG_PATH)
        self._ensure_log_dir()

        self.legal_graph = config.get("legal_graph", self._build_default_legal_knowledge_graph())
        self.sensitive_vocab = config.get("sensitive_vocab", self._build_default_sensitive_vocab())

        self.logger.info(f"Lukhas Private Ethics Guard initialized. Violation log: {self.violation_log_path_str}")

    def _ensure_log_dir(self):
        try:
            log_path = Path(self.violation_log_path_str)
            os.makedirs(log_path.parent, exist_ok=True)
        except Exception as e:
            self.logger.error(f"Failed to create directory for violation log {self.violation_log_path_str}: {e}")


    def _build_default_legal_knowledge_graph(self):
        return {
            "GDPR": ["consent", "right_to_be_forgotten", "data_minimization"],
            "ECHR": ["privacy", "expression", "non-discrimination"],
            "EU_AI_ACT": ["human oversight", "transparency", "safety"]
        }

    def _build_default_sensitive_vocab(self):
        return {
            "EU": ["illegal immigrant", "crazy", "man up"],
            "US": ["retarded", "ghetto", "terrorist"], # Example, ensure culturally appropriate
            "LATAM": ["indio", "maricón", "bruja"], # Example, ensure culturally appropriate
            "GLOBAL": ["slut", "fat", "dumb"] # Example, ensure culturally appropriate
        }

    def check_access(self, signal: str, tier_level: int, user_consent: dict) -> bool:
        allowed_signals = user_consent.get("allowed_signals", [])
        user_tier = user_consent.get("tier", 0)

        if signal not in allowed_signals or user_tier < tier_level:
            self.log_violation(signal, tier_level, user_consent,
                               f"Signal '{signal}' (req tier {tier_level}) accessed by user tier {user_tier} without explicit consent in allowed_signals.")
            return False
        return True

    def log_violation(self, signal: str, tier: int, context: dict, explanation: Optional[str] = None):
        violation = {
            "signal": signal,
            "required_tier": tier,
            "user_tier": context.get("tier"),
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "explanation": explanation or f"Signal '{signal}' was accessed without sufficient tier or consent."
        }
        try:
            with open(self.violation_log_path_str, "a", encoding="utf-8") as f:
                f.write(json.dumps(violation) + "\\n")
        except Exception as e:
            self.logger.error(f"Failed to log violation to {self.violation_log_path_str}: {e}")


    def check_cultural_context(self, content: str, region: str = "EU") -> list:
        violations = []
        # Consider region-specific and global vocab
        check_vocab = self.sensitive_vocab.get(region, []) + self.sensitive_vocab.get("GLOBAL", [])

        for word in check_vocab:
            if word.lower() in content.lower(): # Simple substring check
                violations.append(word)

        if violations:
            self.log_violation(
                signal="cultural_sensitivity_violation",
                tier=3, # Example tier for such violations
                context={
                    "tier_accessed_at": 2, # Placeholder
                    "violations": violations,
                    "region": region,
                    "content_excerpt": content[:100] # Log a snippet
                },
                explanation=f"Content triggered cultural sensitivity filters for region '{region}'. Violating terms: {', '.join(violations)}"
            )
        return violations

# --- Main Engine: AdvancedComplianceEthicsEngine ---
class AdvancedComplianceEthicsEngine:
    """Advanced Compliance and Ethics Engine for prot2.
    Integrates multiple sources of compliance and ethics logic.
    """
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        logger.info("Initializing AdvancedComplianceEthicsEngine...")
        config = config or {}

        # Initialize Core Ethics Engine component
        self.core_ethics_module = _CorePrivateEthicsEngine(config.get("core_ethics_config"))

        # Initialize Lukhas Ethics Guard component
        self.access_cultural_guard_module = _LucasPrivateEthicsGuard(config.get("access_cultural_guard_config"))

        # Configuration for prot1-derived compliance features
        prot1_config = config.get("prot1_compliance_config", {})
        self.gdpr_enabled = prot1_config.get("gdpr_enabled", True)
        self.data_retention_days = prot1_config.get("data_retention_days", 30)
        self.voice_data_compliance_enabled = prot1_config.get("voice_data_compliance_enabled", True)
        self.compliance_mode = prot1_config.get("compliance_mode", os.environ.get("COMPLIANCE_MODE", "strict"))

        # Configuration for ethics drift monitoring
        drift_config = config.get("ethics_drift_config", {})
        self.ethics_drift_log_path_str = drift_config.get("ethics_drift_log_path", DEFAULT_ETHICS_DRIFT_LOG_PATH)
        self.ethical_threshold_for_drift = drift_config.get("ethical_threshold_for_drift", 0.85)
        self._ensure_log_dir(self.ethics_drift_log_path_str) # Ensure log dir for drift log

        logger.info(f"AdvancedComplianceEthicsEngine initialized. Mode: {self.compliance_mode}, GDPR: {self.gdpr_enabled}")

    def _ensure_log_dir(self, log_path_str: str):
        try:
            log_path = Path(log_path_str)
            os.makedirs(log_path.parent, exist_ok=True)
        except Exception as e:
            logger.error(f"Failed to create directory for log {log_path_str}: {e}")

    # --- Facade methods for Core Ethics ---
    def evaluate_action_ethics(self, action_data: Dict[str, Any]) -> bool:
        logger.debug(f"Evaluating action ethics: {action_data.get('action_type', 'N/A')}")
        return self.core_ethics_module.evaluate_action(action_data)

    def evaluate_action(self, action_data: Dict[str, Any]) -> bool:
        """Alias for evaluate_action_ethics for backward compatibility."""
        return self.evaluate_action_ethics(action_data)

    def suggest_ethical_alternatives(self, action_data: Dict[str, Any]) -> List[str]:
        return self.core_ethics_module.suggest_alternatives(action_data)

    def get_core_ethics_metrics(self) -> Dict[str, Any]:
        return self.core_ethics_module.get_metrics()

    def incorporate_ethics_feedback(self, feedback: Dict[str, Any]) -> None:
        self.core_ethics_module.incorporate_feedback(feedback)

    # --- Methods from prot1/compliance_engine.py (adapted) ---
    def anonymize_metadata(self, metadata: Dict[str, Any]) -> Dict[str, Any]:
        if not self.gdpr_enabled:
            return metadata

        anonymized = metadata.copy()
        if "user_id" in anonymized:
            anonymized["user_id"] = self._generate_anonymous_id(metadata["user_id"])
        if "location" in anonymized: # Basic location anonymization
            anonymized["location"] = {"country": anonymized["location"].get("country", "unknown")} if isinstance(anonymized.get("location"), dict) else "anonymized"
        if "device_info" in anonymized and isinstance(anonymized["device_info"], dict): # Basic device anonymization
             anonymized["device_info"] = {"type": "anonymized", "os": anonymized["device_info"].get("os")}
        logger.debug("Metadata anonymized.")
        return anonymized

    def should_retain_data(self, timestamp: float) -> bool: # timestamp is Unix timestamp
        current_time = time.time()
        age_in_seconds = current_time - timestamp
        age_in_days = age_in_seconds / (24 * 60 * 60)
        retain = age_in_days <= self.data_retention_days
        logger.debug(f"Data retention check: age {age_in_days:.2f} days, retain: {retain}")
        return retain

    def check_voice_data_compliance(self, voice_data: Dict[str, Any], user_consent: Optional[Dict[str, bool]] = None) -> Dict[str, Any]:
        if not self.voice_data_compliance_enabled:
            return {"compliant": True, "actions": [], "message": "Voice data compliance checks disabled."}

        user_consent = user_consent or {}
        result = {"compliant": True, "actions": [], "retention_allowed": True, "processing_allowed": True}

        if not user_consent.get("voice_processing", False):
            result.update({"processing_allowed": False, "compliant": False, "actions": ["obtain_voice_processing_consent"]})

        if voice_data.get("biometric_enabled", False) and not user_consent.get("biometric_processing", False):
            result.update({"biometric_allowed": False, "compliant": False, "actions": result["actions"] + ["obtain_biometric_consent"]})

        if "timestamp" in voice_data and not self.should_retain_data(voice_data["timestamp"]):
            result.update({"retention_allowed": False, "actions": result["actions"] + ["delete_voice_data"]})
            # If data should not be retained, it's arguably non-compliant to process further in some contexts
            # result["compliant"] = False

        if voice_data.get("age_category") == "child" and not user_consent.get("parental_consent", False):
            result.update({"compliant": False, "actions": result["actions"] + ["require_parental_consent"]})

        logger.info(f"Voice data compliance check result: {result}")
        return result

    def validate_content_against_harmful_patterns(self, content: str, content_type: str = "text") -> Dict[str, Any]:
        # Simplified check from prot1, can be used as a quick pre-filter
        result = {"passed": True, "flagged_patterns": [], "recommendations": []}
        if content_type == "text":
            harmful_patterns = ["hate speech example", "extreme violence example", "illegal activity example"] # Use specific, well-defined patterns
            # The original list was too generic: "hate", "violence", "threat", "suicide", "terrorist", "weapon", "explicit", "self-harm"
            # This needs careful definition to avoid over-flagging.
            # For now, using placeholders.

            for pattern in harmful_patterns: # This should be a more robust check (e.g., regex, NLP)
                if pattern.lower() in content.lower(): # Simple substring match
                    result["passed"] = False
                    result["flagged_patterns"].append(pattern) # Flag specific pattern

        if not result["passed"]:
            result["recommendations"].append("Content may violate policies against harmful patterns. Please revise.")
            logger.warning(f"Content flagged for harmful patterns: {result['flagged_patterns']}")
        return result

    def generate_compliance_report(self, user_id: str) -> Dict[str, Any]:
        # Placeholder, would query data stores in a real system
        anon_user_id = self._generate_anonymous_id(user_id)
        report = {
            "user_id_anonymized": anon_user_id,
            "data_categories_processed": ["profile", "interaction_history"], # Example
            "processing_purposes": ["service_personalization", "system_improvement"], # Example
            "consent_status": {"voice_processing": "granted", "data_analysis": "granted"}, # Example
            "data_retention_period_days": self.data_retention_days,
            "system_compliance_status": "nominal", # Overall status
            "report_generated_utc": datetime.utcnow().isoformat() + "Z"
        }
        logger.info(f"Generated compliance report for original user ID ending: ...{user_id[-4:]}")
        return report

    def _generate_anonymous_id(self, original_id: str) -> str:
        # Using UUIDv5 for deterministic anonymous ID generation
        namespace = uuid.NAMESPACE_DNS # Predefined namespace
        return str(uuid.uuid5(namespace, str(original_id)))

    # --- Facade methods for Access & Cultural Guard ---
    def check_data_access_permission(self, signal: str, tier_level: int, user_consent: dict) -> bool:
        logger.debug(f"Checking data access: signal '{signal}', tier {tier_level}")
        return self.access_cultural_guard_module.check_access(signal, tier_level, user_consent)

    def check_cultural_appropriateness(self, content: str, region: str = "EU") -> list:
        logger.debug(f"Checking cultural appropriateness for region '{region}'")
        return self.access_cultural_guard_module.check_cultural_context(content, region)

    # --- Methods for Ethics Drift Monitoring (adapted from PRIVATE/src/safety/ethics_monitor.py) ---
    def perform_ethics_drift_detection(self, decision_log: List[Dict[str, Any]], custom_threshold: Optional[float] = None) -> Dict[str, Any]:
        """
        Detects ethics drift based on decision alignment scores from a log of decisions.
        Each decision in the log should have an 'alignment_score' or 'ethical_score' key.
        """
        threshold = custom_threshold if custom_threshold is not None else self.ethical_threshold_for_drift

        if not decision_log:
            logger.warning("Ethics drift detection called with empty decision log.")
            return {"timestamp": datetime.utcnow().isoformat(), "drift_ratio": 0, "status": "no_data", "ethical_threshold": threshold, "total_decisions":0}

        # Try to find score, looking for common key names
        def get_score(d):
            return d.get("alignment_score", d.get("ethical_score", d.get("score")))

        drift_count = sum(1 for d in decision_log if get_score(d) is not None and get_score(d) < threshold)
        valid_decisions = sum(1 for d in decision_log if get_score(d) is not None)

        drift_ratio = drift_count / valid_decisions if valid_decisions > 0 else 0

        status = "stable"
        if drift_ratio > 0.2: # Example: if more than 20% of decisions are below threshold
            status = "drift_detected"
        elif valid_decisions == 0:
            status = "no_scorable_data"

        drift_report = {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "drift_ratio": round(drift_ratio, 4),
            "status": status,
            "ethical_threshold": threshold,
            "total_decisions_evaluated": valid_decisions,
            "drift_count": drift_count
        }
        self._log_ethics_drift_event(drift_report)
        logger.info(f"Ethics drift detection complete: status '{status}', ratio {drift_ratio:.2f}")
        return drift_report

    def _log_ethics_drift_event(self, event_data: Dict[str, Any]):
        """Logs ethics drift events to the configured log file (JSONL format)."""
        try:
            with open(self.ethics_drift_log_path_str, "a", encoding="utf-8") as f:
                f.write(json.dumps(event_data) + "\\n")
        except Exception as e:
            logger.error(f"Failed to log ethics drift event to {self.ethics_drift_log_path_str}: {e}")

    # --- Overall System Health/Status ---
    def get_overall_compliance_status(self) -> Dict[str, Any]:
        core_metrics = self.get_core_ethics_metrics()
        status = {
            "engine_status": "operational",
            "timestamp_utc": datetime.utcnow().isoformat() + "Z",
            "compliance_mode": self.compliance_mode,
            "gdpr_enabled": self.gdpr_enabled,
            "data_retention_days": self.data_retention_days,
            "voice_data_compliance_enabled": self.voice_data_compliance_enabled,
            "core_ethics_evaluations_total": core_metrics.get("evaluations_total"),
            "core_ethics_avg_score": round(core_metrics.get("average_ethical_score", 0), 3),
            "access_cultural_guard_status": "active", # Could be more detailed
            "ethics_drift_monitoring_status": "active", # Could be more detailed
            "last_drift_check_summary": self._get_last_drift_log_summary()
        }
        logger.info("Fetched overall compliance status.")
        return status

    def _get_last_drift_log_summary(self) -> Optional[Dict[str, Any]]:
        try:
            if Path(self.ethics_drift_log_path_str).exists():
                with open(self.ethics_drift_log_path_str, "r", encoding="utf-8") as f:
                    lines = f.readlines()
                    if lines:
                        last_entry = json.loads(lines[-1])
                        return {
                            "timestamp": last_entry.get("timestamp"),
                            "status": last_entry.get("status"),
                            "drift_ratio": last_entry.get("drift_ratio")
                        }
        except Exception as e:
            logger.error(f"Could not read last ethics drift log entry from {self.ethics_drift_log_path_str}: {e}")
        return None

# Example of how to configure and use the engine (for testing or integration)
if __name__ == '__main__':
    # Setup basic logging for demonstration
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    # Example configuration
    engine_config = {
        "core_ethics_config": {
            "required_confidence": 0.75,
            "scrutiny_level": 1.1
        },
        "access_cultural_guard_config": {
            "violation_log_path": "logs/test_access_violations.jsonl",
            "sensitive_vocab": { # Override default sensitive vocab
                "GLOBAL": ["test_bad_word"],
                "EU": ["another_eu_bad_word"]
            }
        },
        "prot1_compliance_config": {
            "gdpr_enabled": True,
            "data_retention_days": 15
        },
        "ethics_drift_config": {
            "ethics_drift_log_path": "logs/test_ethics_drift.jsonl",
            "ethical_threshold_for_drift": 0.80
        }
    }

    # Create logs directory if it doesn't exist for the test
    os.makedirs("logs", exist_ok=True)

    advanced_engine = AdvancedComplianceEthicsEngine(config=engine_config)
    logger.info("AdvancedComplianceEthicsEngine instance created for testing.")

    # Test core ethics evaluation
    action1 = {"action": "generate_text", "text": "This is a helpful and benign statement."}
    is_ethical1 = advanced_engine.evaluate_action_ethics(action1)
    logger.info(f"Action 1 ethical: {is_ethical1}, Metrics: {advanced_engine.get_core_ethics_metrics()}")

    action2 = {"action": "generate_image_of_person", "content": {"text":"Image of a person doing something potentially harmful"}, "context": {"source": "user_prompt"}}
    is_ethical2 = advanced_engine.evaluate_action_ethics(action2)
    logger.info(f"Action 2 ethical: {is_ethical2}, Alternatives: {advanced_engine.suggest_ethical_alternatives(action2)}")

    # Test data retention
    old_timestamp = time.time() - (20 * 24 * 60 * 60) # 20 days ago
    logger.info(f"Should retain data from 20 days ago: {advanced_engine.should_retain_data(old_timestamp)}")
    new_timestamp = time.time() - (5 * 24 * 60 * 60) # 5 days ago
    logger.info(f"Should retain data from 5 days ago: {advanced_engine.should_retain_data(new_timestamp)}")

    # Test voice data compliance
    voice_sample = {"timestamp": time.time(), "biometric_enabled": True, "age_category": "adult"}
    consent_ok = {"voice_processing": True, "biometric_processing": True}
    logger.info(f"Voice compliance (consent OK): {advanced_engine.check_voice_data_compliance(voice_sample, consent_ok)}")
    consent_bad = {"voice_processing": False}
    logger.info(f"Voice compliance (consent BAD): {advanced_engine.check_voice_data_compliance(voice_sample, consent_bad)}")

    # Test cultural appropriateness
    violations = advanced_engine.check_cultural_appropriateness("This is a test_bad_word example.", region="GLOBAL")
    logger.info(f"Cultural violations: {violations}")

    # Test access permission
    user_consent_tier2 = {"allowed_signals": ["signal_A", "signal_B"], "tier": 2}
    access_granted = advanced_engine.check_data_access_permission("signal_A", tier_level=2, user_consent=user_consent_tier2)
    logger.info(f"Access to signal_A (tier 2): {access_granted}")
    access_denied = advanced_engine.check_data_access_permission("signal_C", tier_level=3, user_consent=user_consent_tier2)
    logger.info(f"Access to signal_C (tier 3): {access_denied}")

    # Test ethics drift detection
    decision_log_example = [
        {"id": 1, "ethical_score": 0.9}, {"id": 2, "ethical_score": 0.85},
        {"id": 3, "ethical_score": 0.7}, # Below threshold
        {"id": 4, "alignment_score": 0.95}, {"id": 5, "score": 0.65} # Below threshold
    ]
    drift_report = advanced_engine.perform_ethics_drift_detection(decision_log_example)
    logger.info(f"Ethics drift report: {drift_report}")

    # Test overall status
    logger.info(f"Overall System Compliance Status: {json.dumps(advanced_engine.get_overall_compliance_status(), indent=2)}")

    # Test anonymization
    metadata_to_anonymize = {"user_id": "user123", "location": {"city": "Testville", "country": "Testland"}, "device_info": {"type": "phone", "os": "TestOS"}}
    anonymized_meta = advanced_engine.anonymize_metadata(metadata_to_anonymize)
    logger.info(f"Anonymized metadata: {anonymized_meta}")
    logger.info(f"Original user_id 'user123' anonymized to: {advanced_engine._generate_anonymous_id('user123')}")
    logger.info(f"Original user_id 'user456' anonymized to: {advanced_engine._generate_anonymous_id('user456')}")


    # Test harmful pattern validation
    harmful_content_report = advanced_engine.validate_content_against_harmful_patterns("This is a test with an illegal activity example.")
    logger.info(f"Harmful content report: {harmful_content_report}")
    safe_content_report = advanced_engine.validate_content_against_harmful_patterns("This is perfectly fine.")
    logger.info(f"Safe content report: {safe_content_report}")

    logger.info("Test run completed.")