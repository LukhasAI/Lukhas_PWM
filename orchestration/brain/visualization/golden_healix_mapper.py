"""
lukhas AI System - Function Library
Path: lukhas/core/orchestration/golden_healix_mapper.py
Author: lukhas AI Team
This file is part of the LUKHAS (Logical Unified Knowledge Hyper-Adaptable System)
Copyright (c) 2025 lukhas AI Research. All rights reserved.
Licensed under the lukhas Core License - see LICENSE.md for details.
"""


"""
Healix Memory System
DNA-inspired memory architecture for encrypted emotional and cultural memory mapping.
"""

import logging
from typing import Dict, Any, List, Optional, Union
from datetime import datetime, timedelta
from enum import Enum
import asyncio
import hashlib
import json
import random
import os
import base64

logger = logging.getLogger("healix")

class MemoryStrand(Enum):
    """DNA-inspired memory strand types"""
    EMOTIONAL = "emotional"
    CULTURAL = "cultural"
    EXPERIENTIAL = "experiential"
    PROCEDURAL = "procedural"
    COGNITIVE = "cognitive"

class MutationStrategy(Enum):
    """Strategies for memory mutation and evolution"""
    POINT = "point"           # Single point changes
    INSERTION = "insertion"   # Add new memory segments
    DELETION = "deletion"     # Remove memory segments
    CROSSOVER = "crossover"  # Combine memory patterns

class HealixMapper:
    """
    Maps memories into a DNA-inspired double helix structure.
    Provides quantum-resistant encryption and mutation-aware validation.
    """

    def __init__(self):
        self.strands: Dict[MemoryStrand, List[Dict[str, Any]]] = {
            strand: [] for strand in MemoryStrand
        }

        # Helix properties
        self.quantum_encryption = True
        self.mutation_tracking = True
        self.pattern_validation = True

        # Resonance settings
        self.resonance_threshold = 0.75
        self.pattern_coherence = 0.9

        logger.info("Healix Mapper initialized")

    async def encode_memory(self,
                          memory: Dict[str, Any],
                          strand_type: MemoryStrand,
                          context: Optional[Dict[str, Any]] = None) -> str:
        """
        Encode a memory into the healix structure

        Args:
            memory: Memory data to encode
            strand_type: Type of memory strand
            context: Additional context for encoding

        Returns:
            str: Memory ID in the healix
        """
        if self.pattern_validation:
            await self._validate_pattern(memory, strand_type)

        memory_id = await self._generate_memory_id(memory, strand_type)

        encoded_memory = {
            "id": memory_id,
            "data": memory,
            "created": datetime.utcnow().isoformat(),
            "mutations": [],
            "resonance": await self._calculate_resonance(memory),
            "context": context
        }

        self.strands[strand_type].append(encoded_memory)
        return memory_id

    async def mutate_memory(self,
                           memory_id: str,
                           mutation: Dict[str, Any],
                           strategy: MutationStrategy) -> bool:
        """
        Apply a mutation to an existing memory

        Args:
            memory_id: ID of memory to mutate
            mutation: Mutation data
            strategy: Mutation strategy to apply

        Returns:
            bool: Success status
        """
        memory = await self._find_memory(memory_id)
        if not memory:
            return False

        if strategy == MutationStrategy.POINT:
            success = await self._apply_point_mutation(memory, mutation)
        elif strategy == MutationStrategy.INSERTION:
            success = await self._apply_insertion(memory, mutation)
        elif strategy == MutationStrategy.DELETION:
            success = await self._apply_deletion(memory, mutation)
        elif strategy == MutationStrategy.CROSSOVER:
            success = await self._apply_crossover(memory, mutation)
        else:
            raise ValueError(f"Unknown mutation strategy: {strategy}")

        if success:
            memory["mutations"].append({
                "type": strategy.value,
                "data": mutation,
                "timestamp": datetime.utcnow().isoformat()
            })

        return success

    async def _validate_pattern(self,
                              memory: Dict[str, Any],
                              strand_type: MemoryStrand) -> bool:
        """Validate memory pattern coherence"""
        try:
            # Check required fields based on strand type
            required_fields = {
                MemoryStrand.EMOTIONAL: ['content', 'emotional_weight', 'valence'],
                MemoryStrand.CULTURAL: ['content', 'cultural_markers', 'origin'],
                MemoryStrand.EXPERIENTIAL: ['content', 'context', 'timestamp'],
                MemoryStrand.PROCEDURAL: ['content', 'steps', 'triggers'],
                MemoryStrand.COGNITIVE: ['content', 'concepts', 'associations']
            }

            fields = required_fields.get(strand_type, ['content'])
            for field in fields:
                if field not in memory:
                    logger.warning(f"Missing required field '{field}' for {strand_type.value} memory")
                    return False

            # Validate content coherence
            content = memory.get('content', '')
            if not isinstance(content, str) or len(content.strip()) == 0:
                return False

            # Check pattern coherence score
            coherence_score = await self._calculate_pattern_coherence(memory, strand_type)
            return coherence_score >= self.pattern_coherence

        except Exception as e:
            logger.error(f"Pattern validation error: {e}")
            return False

    async def _calculate_pattern_coherence(self,
                                         memory: Dict[str, Any],
                                         strand_type: MemoryStrand) -> float:
        """Calculate pattern coherence score"""
        try:
            # Base coherence on content quality and structure
            content = memory.get('content', '')

            # Length factor (not too short, not too long)
            length_score = min(1.0, len(content) / 100) * (1.0 - max(0, (len(content) - 1000) / 1000))

            # Structural completeness
            structure_score = 0.0
            expected_keys = ['content', 'context', 'metadata']
            present_keys = sum(1 for key in expected_keys if key in memory)
            structure_score = present_keys / len(expected_keys)

            # Strand-specific coherence
            strand_score = 1.0
            if strand_type == MemoryStrand.EMOTIONAL:
                if 'emotional_weight' in memory:
                    weight = memory['emotional_weight']
                    strand_score = 1.0 if 0 <= weight <= 1.0 else 0.5

            elif strand_type == MemoryStrand.CULTURAL:
                if 'cultural_markers' in memory:
                    markers = memory['cultural_markers']
                    strand_score = 1.0 if isinstance(markers, list) and len(markers) > 0 else 0.7

            # Combine scores
            final_score = (length_score * 0.3 + structure_score * 0.4 + strand_score * 0.3)
            return min(1.0, final_score)

        except Exception as e:
            logger.error(f"Coherence calculation error: {e}")
            return 0.0

    async def _calculate_resonance(self, memory: Dict[str, Any]) -> float:
        """Calculate memory resonance pattern"""
        try:
            # Resonance is based on emotional weight, recency, and connection strength
            emotional_weight = memory.get('emotional_weight', 0.5)

            # Recency factor (newer memories have higher base resonance)
            if 'timestamp' in memory:
                try:
                    memory_time = datetime.fromisoformat(memory['timestamp'])
                    time_diff = (datetime.utcnow() - memory_time).total_seconds()
                    recency_factor = max(0.1, 1.0 / (1.0 + time_diff / 86400))  # Decay over days
                except (ValueError, TypeError, AttributeError) as e:
                    logger.warning(f"Failed to parse memory timestamp: {e}")
                    recency_factor = 0.5
            else:
                recency_factor = 0.5

            # Content richness factor
            content = memory.get('content', '')
            richness_factor = min(1.0, len(content) / 200) if content else 0.1

            # Association factor (based on metadata richness)
            metadata = memory.get('metadata', {})
            association_factor = min(1.0, len(metadata) / 5) if metadata else 0.3

            # Calculate weighted resonance
            resonance = (
                emotional_weight * 0.4 +
                recency_factor * 0.3 +
                richness_factor * 0.2 +
                association_factor * 0.1
            )

            return min(1.0, max(0.0, resonance))

        except Exception as e:
            logger.error(f"Resonance calculation error: {e}")
            return 0.5

    async def _generate_memory_id(self,
                                memory: Dict[str, Any],
                                strand_type: MemoryStrand) -> str:
        """Generate unique memory ID"""
        # Create deterministic ID based on content and timestamp
        content_hash = hashlib.sha256(
            json.dumps(memory, sort_keys=True, default=str).encode()
        ).hexdigest()[:16]

        timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        random_suffix = f"{random.randint(1000, 9999)}"

        return f"{strand_type.value}_{timestamp}_{content_hash}_{random_suffix}"

    async def _find_memory(self, memory_id: str) -> Optional[Dict[str, Any]]:
        """Find a memory by ID across all strands"""
        for strand in self.strands.values():
            for memory in strand:
                if memory["id"] == memory_id:
                    return memory
        return None

    # Mutation implementation methods
    async def _apply_point_mutation(self,
                                  memory: Dict[str, Any],
                                  mutation: Dict[str, Any]) -> bool:
        """Apply point mutation to memory"""
        try:
            target_field = mutation.get('field')
            new_value = mutation.get('value')

            if not target_field or target_field not in memory['data']:
                logger.warning(f"Point mutation target field '{target_field}' not found")
                return False

            # Store original value for potential rollback
            original_value = memory['data'][target_field]

            # Apply mutation
            memory['data'][target_field] = new_value

            # Validate mutation doesn't break coherence
            coherence = await self._calculate_pattern_coherence(memory['data'],
                                                               MemoryStrand(memory['id'].split('_')[0]))

            if coherence < self.pattern_coherence * 0.8:  # Allow some degradation
                memory['data'][target_field] = original_value  # Rollback
                logger.info("Point mutation rolled back due to coherence degradation")
                return False

            # Update resonance
            memory['resonance'] = await self._calculate_resonance(memory['data'])
            logger.info(f"Point mutation applied to field '{target_field}'")
            return True

        except Exception as e:
            logger.error(f"Point mutation error: {e}")
            return False

    async def _apply_insertion(self,
                             memory: Dict[str, Any],
                             mutation: Dict[str, Any]) -> bool:
        """Apply insertion mutation"""
        try:
            insertion_data = mutation.get('data', {})
            position = mutation.get('position', 'append')

            if position == 'append':
                # Add new data fields
                for key, value in insertion_data.items():
                    if key not in memory['data']:
                        memory['data'][key] = value
                    elif isinstance(memory['data'][key], list):
                        if isinstance(value, list):
                            memory['data'][key].extend(value)
                        else:
                            memory['data'][key].append(value)

            elif position == 'metadata':
                # Insert into metadata section
                if 'metadata' not in memory['data']:
                    memory['data']['metadata'] = {}
                memory['data']['metadata'].update(insertion_data)

            # Update resonance after insertion
            memory['resonance'] = await self._calculate_resonance(memory['data'])
            logger.info("Insertion mutation applied successfully")
            return True

        except Exception as e:
            logger.error(f"Insertion mutation error: {e}")
            return False

    async def _apply_deletion(self,
                            memory: Dict[str, Any],
                            mutation: Dict[str, Any]) -> bool:
        """Apply deletion mutation"""
        try:
            fields_to_delete = mutation.get('fields', [])
            preserve_core = mutation.get('preserve_core', True)

            # Core fields that should be preserved
            core_fields = ['content', 'id', 'created', 'resonance'] if preserve_core else []

            deleted_fields = []
            for field in fields_to_delete:
                if field in memory['data'] and field not in core_fields:
                    del memory['data'][field]
                    deleted_fields.append(field)

            if deleted_fields:
                # Update resonance after deletion
                memory['resonance'] = await self._calculate_resonance(memory['data'])
                logger.info(f"Deletion mutation removed fields: {deleted_fields}")
                return True
            else:
                logger.warning("No valid fields found for deletion")
                return False

        except Exception as e:
            logger.error(f"Deletion mutation error: {e}")
            return False

    async def _apply_crossover(self,
                             memory: Dict[str, Any],
                             mutation: Dict[str, Any]) -> bool:
        """Apply crossover mutation"""
        try:
            source_memory_id = mutation.get('source_memory_id')
            crossover_fields = mutation.get('fields', [])

            if not source_memory_id:
                logger.warning("Crossover mutation requires source_memory_id")
                return False

            source_memory = await self._find_memory(source_memory_id)
            if not source_memory:
                logger.warning(f"Source memory {source_memory_id} not found for crossover")
                return False

            # Perform crossover for specified fields
            crossed_fields = []
            for field in crossover_fields:
                if field in source_memory['data']:
                    # Combine or replace based on field type
                    if field in memory['data']:
                        # If both have the field, create hybrid
                        if isinstance(memory['data'][field], list) and isinstance(source_memory['data'][field], list):
                            # Interleave lists
                            combined = []
                            max_len = max(len(memory['data'][field]), len(source_memory['data'][field]))
                            for i in range(max_len):
                                if i < len(memory['data'][field]):
                                    combined.append(memory['data'][field][i])
                                if i < len(source_memory['data'][field]):
                                    combined.append(source_memory['data'][field][i])
                            memory['data'][field] = combined
                        else:
                            # For non-list fields, alternate between source and target
                            memory['data'][field] = source_memory['data'][field]
                    else:
                        # Direct copy if field doesn't exist
                        memory['data'][field] = source_memory['data'][field]

                    crossed_fields.append(field)

            if crossed_fields:
                # Update resonance after crossover
                memory['resonance'] = await self._calculate_resonance(memory['data'])
                logger.info(f"Crossover mutation applied for fields: {crossed_fields}")
                return True
            else:
                logger.warning("No valid fields found for crossover")
                return False

        except Exception as e:
            logger.error(f"Crossover mutation error: {e}")
            return False

    async def search_memories(self,
                            query: Dict[str, Any],
                            strand_types: Optional[List[MemoryStrand]] = None,
                            resonance_threshold: Optional[float] = None) -> List[Dict[str, Any]]:
        """
        Search memories across strands using complex queries

        Args:
            query: Search query with filters
            strand_types: Specific strands to search (None = all)
            resonance_threshold: Minimum resonance for results

        Returns:
            List of matching memories
        """
        try:
            threshold = resonance_threshold or self.resonance_threshold
            search_strands = strand_types or list(MemoryStrand)
            results = []

            for strand_type in search_strands:
                for memory in self.strands[strand_type]:
                    if memory['resonance'] >= threshold:
                        if await self._matches_query(memory, query):
                            results.append({
                                **memory,
                                'strand_type': strand_type.value,
                                'match_score': await self._calculate_match_score(memory, query)
                            })

            # Sort by match score and resonance
            results.sort(key=lambda x: (x['match_score'], x['resonance']), reverse=True)
            logger.info(f"Found {len(results)} memories matching query")
            return results

        except Exception as e:
            logger.error(f"Memory search error: {e}")
            return []

    async def _matches_query(self, memory: Dict[str, Any], query: Dict[str, Any]) -> bool:
        """Check if memory matches search query"""
        try:
            # Content matching
            if 'content' in query:
                content = memory['data'].get('content', '').lower()
                query_content = query['content'].lower()
                if query_content not in content:
                    return False

            # Emotional weight range
            if 'emotional_weight_range' in query:
                weight = memory['data'].get('emotional_weight', 0.5)
                min_weight, max_weight = query['emotional_weight_range']
                if not (min_weight <= weight <= max_weight):
                    return False

            # Date range
            if 'date_range' in query:
                try:
                    memory_date = datetime.fromisoformat(memory['created'])
                    start_date = datetime.fromisoformat(query['date_range'][0])
                    end_date = datetime.fromisoformat(query['date_range'][1])
                    if not (start_date <= memory_date <= end_date):
                        return False
                except (ValueError, TypeError, AttributeError) as e:
                    logger.warning(f"Failed to parse memory date for filtering: {e}")
                    return False

            # Metadata matching
            if 'metadata' in query:
                memory_metadata = memory['data'].get('metadata', {})
                for key, value in query['metadata'].items():
                    if key not in memory_metadata or memory_metadata[key] != value:
                        return False

            # Cultural markers
            if 'cultural_markers' in query:
                memory_markers = memory['data'].get('cultural_markers', [])
                query_markers = query['cultural_markers']
                if not any(marker in memory_markers for marker in query_markers):
                    return False

            return True

        except Exception as e:
            logger.error(f"Query matching error: {e}")
            return False

    async def _calculate_match_score(self, memory: Dict[str, Any], query: Dict[str, Any]) -> float:
        """Calculate how well a memory matches the query"""
        try:
            score = 0.0

            # Content similarity score
            if 'content' in query:
                content = memory['data'].get('content', '').lower()
                query_content = query['content'].lower()

                # Simple word overlap scoring
                content_words = set(content.split())
                query_words = set(query_content.split())

                if query_words:
                    overlap = len(content_words.intersection(query_words))
                    score += (overlap / len(query_words)) * 0.4

            # Resonance contributes to match score
            score += memory['resonance'] * 0.3

            # Recency factor
            try:
                memory_date = datetime.fromisoformat(memory['created'])
                days_old = (datetime.utcnow() - memory_date).days
                recency_score = max(0.0, 1.0 - (days_old / 365))  # Decay over a year
                score += recency_score * 0.2
            except (ValueError, TypeError, AttributeError) as e:
                logger.warning(f"Failed to calculate recency score: {e}")
                pass

            # Mutation count (more evolved memories might be more relevant)
            mutation_count = len(memory.get('mutations', []))
            score += min(1.0, mutation_count / 10) * 0.1

            return min(1.0, score)

        except Exception as e:
            logger.error(f"Match score calculation error: {e}")
            return 0.0

    async def retrieve_memory(self, memory_id: str) -> Optional[Dict[str, Any]]:
        """Retrieve a specific memory by ID with access tracking"""
        try:
            memory = await self._find_memory(memory_id)
            if memory:
                # Track access for resonance updates
                await self._update_access_patterns(memory)

                # Return copy to prevent external modification
                return {
                    'id': memory['id'],
                    'data': memory['data'].copy(),
                    'created': memory['created'],
                    'resonance': memory['resonance'],
                    'mutations': memory['mutations'].copy(),
                    'context': memory.get('context', {})
                }
            return None

        except Exception as e:
            logger.error(f"Memory retrieval error: {e}")
            return None

    async def _update_access_patterns(self, memory: Dict[str, Any]) -> None:
        """Update memory access patterns for adaptive resonance"""
        try:
            if 'access_history' not in memory:
                memory['access_history'] = []

            # Add access timestamp
            memory['access_history'].append(datetime.utcnow().isoformat())

            # Keep only recent accesses (last 100)
            memory['access_history'] = memory['access_history'][-100:]

            # Update resonance based on access frequency
            recent_accesses = len([
                access for access in memory['access_history']
                if (datetime.utcnow() - datetime.fromisoformat(access)).days <= 30
            ])

            # Boost resonance for frequently accessed memories
            access_boost = min(0.2, recent_accesses / 10)
            memory['resonance'] = min(1.0, memory['resonance'] + access_boost)

        except Exception as e:
            logger.error(f"Access pattern update error: {e}")

    async def consolidate_memories(self,
                                 similarity_threshold: float = 0.8,
                                 strand_type: Optional[MemoryStrand] = None) -> int:
        """
        Consolidate similar memories to optimize storage and strengthen patterns

        Args:
            similarity_threshold: Threshold for considering memories similar
            strand_type: Specific strand to consolidate (None = all)

        Returns:
            Number of memories consolidated
        """
        try:
            consolidation_count = 0
            strands_to_process = [strand_type] if strand_type else list(MemoryStrand)

            for strand in strands_to_process:
                memories = self.strands[strand]

                # Find similar memory pairs
                for i, memory1 in enumerate(memories):
                    for j, memory2 in enumerate(memories[i+1:], i+1):
                        similarity = await self._calculate_memory_similarity(memory1, memory2)

                        if similarity >= similarity_threshold:
                            # Consolidate memories
                            consolidated = await self._consolidate_memory_pair(memory1, memory2)
                            if consolidated:
                                # Replace first memory with consolidated version
                                memories[i] = consolidated
                                # Remove second memory
                                memories.pop(j)
                                consolidation_count += 1
                                break  # Process next memory1

            logger.info(f"Consolidated {consolidation_count} memories")
            return consolidation_count

        except Exception as e:
            logger.error(f"Memory consolidation error: {e}")
            return 0

    async def _calculate_memory_similarity(self,
                                         memory1: Dict[str, Any],
                                         memory2: Dict[str, Any]) -> float:
        """Calculate similarity between two memories"""
        try:
            similarity = 0.0

            # Content similarity
            content1 = memory1['data'].get('content', '').lower().split()
            content2 = memory2['data'].get('content', '').lower().split()

            if content1 and content2:
                words1 = set(content1)
                words2 = set(content2)
                intersection = words1.intersection(words2)
                union = words1.union(words2)
                content_similarity = len(intersection) / len(union) if union else 0.0
                similarity += content_similarity * 0.5

            # Emotional weight similarity
            weight1 = memory1['data'].get('emotional_weight', 0.5)
            weight2 = memory2['data'].get('emotional_weight', 0.5)
            weight_similarity = 1.0 - abs(weight1 - weight2)
            similarity += weight_similarity * 0.2

            # Context similarity
            context1 = memory1.get('context', {})
            context2 = memory2.get('context', {})

            if context1 and context2:
                common_keys = set(context1.keys()).intersection(set(context2.keys()))
                if common_keys:
                    context_matches = sum(1 for key in common_keys
                                        if context1[key] == context2[key])
                    context_similarity = context_matches / len(common_keys)
                    similarity += context_similarity * 0.3

            return min(1.0, similarity)

        except Exception as e:
            logger.error(f"Similarity calculation error: {e}")
            return 0.0

    async def _consolidate_memory_pair(self,
                                     memory1: Dict[str, Any],
                                     memory2: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Consolidate two similar memories into one enhanced memory"""
        try:
            # Choose the memory with higher resonance as base
            if memory1['resonance'] >= memory2['resonance']:
                base_memory = memory1.copy()
                merge_memory = memory2
            else:
                base_memory = memory2.copy()
                merge_memory = memory1

            # Combine content intelligently
            base_content = base_memory['data'].get('content', '')
            merge_content = merge_memory['data'].get('content', '')

            # Create enhanced content by combining unique elements
            combined_content = f"{base_content}\n[Consolidated: {merge_content}]"
            base_memory['data']['content'] = combined_content

            # Merge metadata
            base_metadata = base_memory['data'].get('metadata', {})
            merge_metadata = merge_memory['data'].get('metadata', {})
            base_metadata.update(merge_metadata)
            base_memory['data']['metadata'] = base_metadata

            # Boost resonance due to consolidation
            resonance_boost = 0.1
            base_memory['resonance'] = min(1.0, base_memory['resonance'] + resonance_boost)

            # Combine mutation histories
            base_mutations = base_memory.get('mutations', [])
            merge_mutations = merge_memory.get('mutations', [])
            base_memory['mutations'] = base_mutations + merge_mutations

            # Add consolidation record
            base_memory['mutations'].append({
                'type': 'consolidation',
                'data': {'merged_memory_id': merge_memory['id']},
                'timestamp': datetime.utcnow().isoformat()
            })

            # Combine access histories if they exist
            if 'access_history' in merge_memory:
                base_access = base_memory.get('access_history', [])
                merge_access = merge_memory.get('access_history', [])
                base_memory['access_history'] = sorted(base_access + merge_access)

            return base_memory

        except Exception as e:
            logger.error(f"Memory consolidation error: {e}")
            return None

    async def analyze_emotional_drift(self,
                                    memory_id: str,
                                    time_window_days: int = 30) -> Dict[str, Any]:
        """
        Analyze emotional drift patterns in memory over time

        Args:
            memory_id: Memory to analyze
            time_window_days: Time window for drift analysis

        Returns:
            Dictionary with drift analysis results
        """
        try:
            memory = await self._find_memory(memory_id)
            if not memory:
                return {'error': 'Memory not found'}

            # Get mutation history
            mutations = memory.get('mutations', [])

            # Filter emotional mutations within time window
            cutoff_date = datetime.utcnow() - timedelta(days=time_window_days)
            emotional_mutations = []

            for mutation in mutations:
                try:
                    mutation_date = datetime.fromisoformat(mutation['timestamp'])
                    if mutation_date >= cutoff_date:
                        if ('emotional_weight' in str(mutation.get('data', {})) or
                            mutation.get('type') == 'emotional_update'):
                            emotional_mutations.append(mutation)
                except (KeyError, TypeError, AttributeError) as e:
                    logger.debug(f"Failed to process data: {e}")
                    continue

            # Calculate drift metrics
            drift_analysis = {
                'memory_id': memory_id,
                'analysis_period_days': time_window_days,
                'total_emotional_changes': len(emotional_mutations),
                'drift_score': 0.0,
                'drift_direction': 'stable',
                'volatility': 0.0,
                'mutations': emotional_mutations
            }

            if len(emotional_mutations) > 1:
                # Calculate drift score and direction
                weights = []
                for mutation in emotional_mutations:
                    if 'emotional_weight' in str(mutation.get('data', {})):
                        try:
                            weight = float(mutation['data'].get('value', 0.5))
                            weights.append(weight)
                        except:
                            continue

                if len(weights) > 1:
                    # Linear trend analysis
                    x = list(range(len(weights)))
                    y = weights

                    # Simple linear regression
                    n = len(weights)
                    sum_x = sum(x)
                    sum_y = sum(y)
                    sum_xy = sum(xi * yi for xi, yi in zip(x, y))
                    sum_x2 = sum(xi * xi for xi in x)

                    slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x)

                    drift_analysis['drift_score'] = abs(slope)
                    drift_analysis['drift_direction'] = (
                        'positive' if slope > 0.1 else
                        'negative' if slope < -0.1 else
                        'stable'
                    )

                    # Calculate volatility (standard deviation of changes)
                    if len(weights) > 2:
                        changes = [abs(weights[i] - weights[i-1]) for i in range(1, len(weights))]
                        mean_change = sum(changes) / len(changes)
                        variance = sum((change - mean_change) ** 2 for change in changes) / len(changes)
                        drift_analysis['volatility'] = variance ** 0.5

            logger.info(f"Emotional drift analysis completed for {memory_id}")
            return drift_analysis

        except Exception as e:
            logger.error(f"Emotional drift analysis error: {e}")
            return {'error': str(e)}

    async def extract_symbolic_patterns(self,
                                      strand_type: Optional[MemoryStrand] = None,
                                      pattern_depth: int = 3) -> Dict[str, Any]:
        """
        Extract symbolic patterns from memory strands using deep analysis

        Args:
            strand_type: Specific strand to analyze (None = all)
            pattern_depth: Depth of pattern extraction

        Returns:
            Dictionary with extracted patterns
        """
        try:
            strands_to_analyze = [strand_type] if strand_type else list(MemoryStrand)
            pattern_analysis = {
                'extraction_timestamp': datetime.utcnow().isoformat(),
                'pattern_depth': pattern_depth,
                'strand_patterns': {},
                'cross_strand_patterns': [],
                'symbolic_clusters': [],
                'resonance_patterns': {}
            }

            for strand in strands_to_analyze:
                memories = self.strands[strand]
                strand_patterns = await self._analyze_strand_patterns(memories, pattern_depth)
                pattern_analysis['strand_patterns'][strand.value] = strand_patterns

            # Cross-strand pattern analysis
            if len(strands_to_analyze) > 1:
                cross_patterns = await self._find_cross_strand_patterns(strands_to_analyze)
                pattern_analysis['cross_strand_patterns'] = cross_patterns

            # Symbolic clustering
            symbolic_clusters = await self._cluster_symbolic_elements(strands_to_analyze)
            pattern_analysis['symbolic_clusters'] = symbolic_clusters

            # Resonance pattern analysis
            resonance_patterns = await self._analyze_resonance_patterns(strands_to_analyze)
            pattern_analysis['resonance_patterns'] = resonance_patterns

            logger.info(f"Symbolic pattern extraction completed for {len(strands_to_analyze)} strands")
            return pattern_analysis

        except Exception as e:
            logger.error(f"Symbolic pattern extraction error: {e}")
            return {'error': str(e)}

    async def _analyze_strand_patterns(self,
                                     memories: List[Dict[str, Any]],
                                     depth: int) -> Dict[str, Any]:
        """Analyze patterns within a single strand"""
        try:
            patterns = {
                'content_themes': [],
                'temporal_patterns': [],
                'emotional_patterns': [],
                'mutation_patterns': [],
                'frequency_analysis': {}
            }

            if not memories:
                return patterns

            # Content theme analysis
            all_content = []
            for memory in memories:
                content = memory['data'].get('content', '')
                if content:
                    all_content.extend(content.lower().split())

            # Frequency analysis of content themes
            from collections import Counter
            word_freq = Counter(all_content)

            # Filter for meaningful words (longer than 3 characters)
            meaningful_words = {word: count for word, count in word_freq.items()
                              if len(word) > 3 and count > 1}

            patterns['frequency_analysis'] = dict(
                sorted(meaningful_words.items(), key=lambda x: x[1], reverse=True)[:20]
            )

            # Extract top themes
            patterns['content_themes'] = list(meaningful_words.keys())[:10]

            # Temporal pattern analysis
            timestamps = []
            for memory in memories:
                try:
                    timestamp = datetime.fromisoformat(memory['created'])
                    timestamps.append(timestamp)
                except (KeyError, TypeError, AttributeError) as e:
                    logger.debug(f"Failed to process data: {e}")
                    continue

            if timestamps:
                timestamps.sort()
                # Analyze creation patterns (daily, weekly, etc.)
                if len(timestamps) > 1:
                    intervals = [(timestamps[i] - timestamps[i-1]).total_seconds()
                               for i in range(1, len(timestamps))]

                    avg_interval = sum(intervals) / len(intervals)
                    patterns['temporal_patterns'] = {
                        'average_interval_seconds': avg_interval,
                        'creation_frequency': 'high' if avg_interval < 86400 else 'medium' if avg_interval < 604800 else 'low',
                        'total_memories': len(memories),
                        'time_span_days': (timestamps[-1] - timestamps[0]).days
                    }

            # Emotional pattern analysis
            emotional_weights = []
            for memory in memories:
                weight = memory['data'].get('emotional_weight')
                if weight is not None:
                    emotional_weights.append(weight)

            if emotional_weights:
                avg_emotion = sum(emotional_weights) / len(emotional_weights)
                patterns['emotional_patterns'] = {
                    'average_emotional_weight': avg_emotion,
                    'emotional_variance': sum((w - avg_emotion) ** 2 for w in emotional_weights) / len(emotional_weights),
                    'emotional_range': max(emotional_weights) - min(emotional_weights),
                    'predominant_valence': 'positive' if avg_emotion > 0.6 else 'negative' if avg_emotion < 0.4 else 'neutral'
                }

            return patterns

        except Exception as e:
            logger.error(f"Strand pattern analysis error: {e}")
            return {}

    async def _find_cross_strand_patterns(self, strands: List[MemoryStrand]) -> List[Dict[str, Any]]:
        """Find patterns that cross multiple memory strands"""
        try:
            cross_patterns = []

            # Compare content themes across strands
            strand_themes = {}
            for strand in strands:
                memories = self.strands[strand]
                strand_patterns = await self._analyze_strand_patterns(memories, 2)
                strand_themes[strand.value] = set(strand_patterns.get('content_themes', []))

            # Find shared themes
            if len(strand_themes) > 1:
                strand_names = list(strand_themes.keys())
                for i, strand1 in enumerate(strand_names):
                    for strand2 in strand_names[i+1:]:
                        shared_themes = strand_themes[strand1].intersection(strand_themes[strand2])
                        if shared_themes:
                            cross_patterns.append({
                                'type': 'shared_themes',
                                'strands': [strand1, strand2],
                                'shared_elements': list(shared_themes),
                                'strength': len(shared_themes) / len(strand_themes[strand1].union(strand_themes[strand2]))
                            })

            return cross_patterns

        except Exception as e:
            logger.error(f"Cross-strand pattern analysis error: {e}")
            return []

    async def _cluster_symbolic_elements(self, strands: List[MemoryStrand]) -> List[Dict[str, Any]]:
        """Cluster symbolic elements across memory strands"""
        try:
            clusters = []

            # Collect all symbolic elements
            symbolic_elements = []
            for strand in strands:
                for memory in self.strands[strand]:
                    data = memory['data']

                    # Extract various symbolic elements
                    if 'cultural_markers' in data:
                        symbolic_elements.extend([
                            {'type': 'cultural_marker', 'value': marker, 'strand': strand.value, 'memory_id': memory['id']}
                            for marker in data['cultural_markers']
                        ])

                    if 'symbols' in data:
                        symbolic_elements.extend([
                            {'type': 'symbol', 'value': symbol, 'strand': strand.value, 'memory_id': memory['id']}
                            for symbol in data['symbols']
                        ])

                    # Extract emotional symbols from content
                    content = data.get('content', '')
                    emotion_words = ['joy', 'fear', 'anger', 'sadness', 'surprise', 'love', 'hope', 'peace']
                    for word in emotion_words:
                        if word in content.lower():
                            symbolic_elements.append({
                                'type': 'emotional_symbol',
                                'value': word,
                                'strand': strand.value,
                                'memory_id': memory['id']
                            })

            # Simple clustering by symbolic type and value
            from collections import defaultdict
            element_groups = defaultdict(list)

            for element in symbolic_elements:
                key = f"{element['type']}:{element['value']}"
                element_groups[key].append(element)

            # Create clusters for groups with multiple instances
            for key, elements in element_groups.items():
                if len(elements) > 1:
                    clusters.append({
                        'cluster_type': elements[0]['type'],
                        'cluster_value': elements[0]['value'],
                        'member_count': len(elements),
                        'strands_involved': list(set(e['strand'] for e in elements)),
                        'memory_ids': [e['memory_id'] for e in elements]
                    })

            # Sort clusters by member count
            clusters.sort(key=lambda x: x['member_count'], reverse=True)

            return clusters[:20]  # Return top 20 clusters

        except Exception as e:
            logger.error(f"Symbolic clustering error: {e}")
            return []

    async def _analyze_resonance_patterns(self, strands: List[MemoryStrand]) -> Dict[str, Any]:
        """Analyze resonance patterns across memory strands"""
        try:
            resonance_data = []

            for strand in strands:
                for memory in self.strands[strand]:
                    resonance_data.append({
                        'resonance': memory['resonance'],
                        'strand': strand.value,
                        'age_days': (datetime.utcnow() - datetime.fromisoformat(memory['created'])).days,
                        'mutation_count': len(memory.get('mutations', []))
                    })

            if not resonance_data:
                return {}

            # Calculate resonance statistics
            resonances = [d['resonance'] for d in resonance_data]
            avg_resonance = sum(resonances) / len(resonances)

            resonance_patterns = {
                'average_resonance': avg_resonance,
                'resonance_variance': sum((r - avg_resonance) ** 2 for r in resonances) / len(resonances),
                'high_resonance_count': len([r for r in resonances if r > 0.8]),
                'low_resonance_count': len([r for r in resonances if r < 0.3]),
                'strand_resonance_averages': {}
            }

            # Strand-specific resonance averages
            for strand in strands:
                strand_resonances = [d['resonance'] for d in resonance_data if d['strand'] == strand.value]
                if strand_resonances:
                    resonance_patterns['strand_resonance_averages'][strand.value] = {
                        'average': sum(strand_resonances) / len(strand_resonances),
                        'count': len(strand_resonances)
                    }

            return resonance_patterns

        except Exception as e:
            logger.error(f"Resonance pattern analysis error: {e}")
            return {}

    async def create_memory_snapshot(self) -> Dict[str, Any]:
        """Create a comprehensive snapshot of the current memory state"""
        try:
            snapshot = {
                'timestamp': datetime.utcnow().isoformat(),
                'total_memories': 0,
                'strand_statistics': {},
                'overall_health': {},
                'symbolic_summary': {},
                'performance_metrics': {}
            }

            total_memories = 0
            total_resonance = 0.0
            total_mutations = 0

            # Calculate strand statistics
            for strand_type in MemoryStrand:
                strand_memories = self.strands[strand_type]
                strand_count = len(strand_memories)
                total_memories += strand_count

                if strand_count > 0:
                    strand_resonance = sum(m['resonance'] for m in strand_memories) / strand_count
                    strand_mutations = sum(len(m.get('mutations', [])) for m in strand_memories)
                    total_resonance += strand_resonance * strand_count
                    total_mutations += strand_mutations

                    snapshot['strand_statistics'][strand_type.value] = {
                        'memory_count': strand_count,
                        'average_resonance': strand_resonance,
                        'total_mutations': strand_mutations,
                        'health_score': min(1.0, strand_resonance * (1.0 + strand_mutations / 100))
                    }

            snapshot['total_memories'] = total_memories

            # Overall health metrics
            if total_memories > 0:
                avg_resonance = total_resonance / total_memories
                mutation_density = total_mutations / total_memories

                snapshot['overall_health'] = {
                    'average_resonance': avg_resonance,
                    'mutation_density': mutation_density,
                    'memory_diversity': len([s for s in MemoryStrand if len(self.strands[s]) > 0]),
                    'system_coherence': min(1.0, avg_resonance * (1.0 - abs(mutation_density - 2.0) / 10))
                }

            # Quick symbolic summary
            symbolic_patterns = await self.extract_symbolic_patterns(pattern_depth=1)
            snapshot['symbolic_summary'] = {
                'active_strands': len(symbolic_patterns.get('strand_patterns', {})),
                'cross_patterns': len(symbolic_patterns.get('cross_strand_patterns', [])),
                'symbolic_clusters': len(symbolic_patterns.get('symbolic_clusters', []))
            }

            logger.info("Memory snapshot created successfully")
            return snapshot

        except Exception as e:
            logger.error(f"Memory snapshot creation error: {e}")
            return {'error': str(e), 'timestamp': datetime.utcnow().isoformat()}








# Last Updated: 2025-06-05 09:37:28
