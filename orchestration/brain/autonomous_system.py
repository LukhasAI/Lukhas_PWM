#!/usr/bin/env python3
"""
Î›Bot Fully Autonomous AGI System
===============================
Complete autonomous processing of GitHub issues with intelligent batching
No manual intervention required - handles 145+ pages of notifications efficiently

Features:
- Fully autonomous operation (no user prompts)
- Intelligent batch processing to minimize API calls
- Emergency budget override for critical issues
- Continuous monitoring and processing
- Comprehensive reporting and logging
"""

import os
import sys
import json
import time
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional
from github_vulnerability_manager import GitHubVulnerabilityManager
from lambda_bot_batch_processor import BatchProcessor, BatchableIssue
from core.budget.token_controller import TokenBudgetController

class FullyAutonomousAGI:
    """
    Fully autonomous AGI system for GitHub issue management
    Operates without human intervention using intelligent batch processing
    """
    
    def __init__(self, github_token: Optional[str] = None):
        self.github_token = github_token or os.getenv('GITHUB_TOKEN')
        
        # Initialize core systems with AGI and batch mode
        self.vulnerability_manager = GitHubVulnerabilityManager(
            github_token=self.github_token,
            agi_mode=True,  # Enable emergency overrides
            batch_mode=True  # Enable batch processing
        )
        
        self.batch_processor = BatchProcessor(self.github_token)
        self.budget_controller = TokenBudgetController()
        
        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger("FullyAutonomousAGI")
        
        # AGI configuration
        self.max_processing_cycles = 10  # Prevent infinite loops
        self.batch_size_threshold = 5   # Minimum issues to process as batch
        self.emergency_mode = True      # Allow budget overrides for critical issues
        
        # State tracking
        self.total_issues_processed = 0
        self.total_prs_created = 0
        self.total_cost = 0.0
        self.processing_cycles = 0
        
    def run_autonomous_cycle(self) -> Dict[str, Any]:
        """Run a complete autonomous processing cycle"""
        cycle_start = datetime.now()
        self.processing_cycles += 1
        
        self.logger.info(f"ğŸ¤– Starting autonomous cycle #{self.processing_cycles}")
        
        cycle_results = {
            "cycle_number": self.processing_cycles,
            "start_time": cycle_start.isoformat(),
            "vulnerabilities_processed": 0,
            "workflows_fixed": 0,
            "batches_created": 0,
            "prs_created": 0,
            "cost": 0.0,
            "success": True,
            "errors": []
        }
        
        try:
            # Step 1: Scan for vulnerabilities (if needed)
            if not hasattr(self.vulnerability_manager, 'vulnerabilities') or not self.vulnerability_manager.vulnerabilities:
                self.logger.info("ğŸ” Scanning repositories for vulnerabilities...")
                scan_results = self.vulnerability_manager.scan_all_repositories()
                cycle_results["vulnerabilities_found"] = scan_results["total_vulnerabilities"]
            
            # Step 2: Process vulnerabilities in batches
            if self.vulnerability_manager.vulnerabilities:
                self.logger.info("ğŸ”„ Processing vulnerabilities in batches...")
                vuln_results = self.vulnerability_manager.fix_vulnerabilities_batch()
                
                cycle_results["vulnerabilities_processed"] = vuln_results["fixes_applied"]
                cycle_results["batches_created"] += vuln_results["batches_processed"]
                cycle_results["prs_created"] += vuln_results["prs_created"]
                cycle_results["cost"] += vuln_results["total_cost"]
            
            # Step 3: Process GitHub notifications in batches
            notification_results = self._process_github_notifications_batch()
            cycle_results["workflows_fixed"] = notification_results["fixes_applied"]
            cycle_results["batches_created"] += notification_results["batches_processed"]
            cycle_results["prs_created"] += notification_results["prs_created"]
            cycle_results["cost"] += notification_results["cost"]
            
            # Step 4: Process any pending batches
            pending_results = self.batch_processor.process_ready_batches()
            if pending_results:
                additional_fixes = sum(len(batch["fixes_applied"]) for batch in pending_results)
                additional_prs = sum(len(batch["prs_created"]) for batch in pending_results)
                additional_cost = sum(batch["total_cost"] for batch in pending_results)
                
                cycle_results["vulnerabilities_processed"] += additional_fixes
                cycle_results["prs_created"] += additional_prs
                cycle_results["cost"] += additional_cost
            
            # Update totals
            self.total_issues_processed += cycle_results["vulnerabilities_processed"] + cycle_results["workflows_fixed"]
            self.total_prs_created += cycle_results["prs_created"]
            self.total_cost += cycle_results["cost"]
            
        except Exception as e:
            cycle_results["success"] = False
            cycle_results["errors"].append(str(e))
            self.logger.error(f"Cycle #{self.processing_cycles} failed: {e}")
        
        cycle_results["end_time"] = datetime.now().isoformat()
        cycle_results["duration"] = (datetime.now() - cycle_start).total_seconds()
        
        self.logger.info(f"âœ… Cycle #{self.processing_cycles} complete: "
                        f"{cycle_results['vulnerabilities_processed'] + cycle_results['workflows_fixed']} issues, "
                        f"{cycle_results['prs_created']} PRs, "
                        f"${cycle_results['cost']:.4f} cost")
        
        return cycle_results
    
    def _process_github_notifications_batch(self) -> Dict[str, Any]:
        """Process GitHub notifications using batch processing"""
        self.logger.info("ğŸ“§ Processing GitHub notifications in batches...")
        
        # Simulate fetching notifications (in production, would use actual GitHub API)
        notifications = self._fetch_github_notifications()
        
        # Convert notifications to batchable issues
        for notification in notifications:
            issue = self._notification_to_batchable_issue(notification)
            if issue:
                self.batch_processor.add_issue_to_batch(issue)
        
        # Process batches
        batch_results = self.batch_processor.process_ready_batches()
        
        total_fixes = sum(len(batch["fixes_applied"]) for batch in batch_results)
        total_prs = sum(len(batch["prs_created"]) for batch in batch_results)
        total_cost = sum(batch["total_cost"] for batch in batch_results)
        
        return {
            "batches_processed": len(batch_results),
            "fixes_applied": total_fixes,
            "prs_created": total_prs,
            "cost": total_cost,
            "notifications_processed": len(notifications)
        }
    
    def _fetch_github_notifications(self) -> List[Dict[str, Any]]:
        """Fetch GitHub notifications (simulated for demo)"""
        # In production, this would fetch actual notifications from GitHub API
        # For demo, simulate the 145 pages of workflow failures
        notifications = []
        
        sample_workflows = [
            "Critical Path Validation",
            "Î›Bot Continuous Quality Monitor", 
            "AI Dependency Bot",
            "LUKHAS Security Warrior",
            "LUKHAS Symbol Validator Bot",
            "LUKHAS Pre-Commit Validation"
        ]
        
        repositories = [
            "LukhasAI/Prototype",
            "LukhasAI/Lukhas", 
            "LukhasAI/VeriFold",
            "LukhasAI/CodexGPT_Lukhas",
            "LukhasAI/LUKHAS-GEMINI"
        ]
        
        # Simulate 145 pages worth of notifications (about 3625 notifications)
        for i in range(100):  # Process subset for demo
            repo = repositories[i % len(repositories)]
            workflow = sample_workflows[i % len(sample_workflows)]
            
            notifications.append({
                "id": f"notification_{i}",
                "repository": repo,
                "workflow_name": workflow,
                "type": "workflow_failure",
                "severity": "high" if "security" in workflow.lower() or "critical" in workflow.lower() else "medium",
                "description": f"{workflow} workflow run failed for master branch",
                "url": f"https://github.com/{repo}/actions/runs/{1000 + i}"
            })
        
        return notifications
    
    def _notification_to_batchable_issue(self, notification: Dict[str, Any]) -> Optional[BatchableIssue]:
        """Convert a GitHub notification to a batchable issue"""
        if notification["type"] == "workflow_failure":
            return BatchableIssue(
                id=notification["id"],
                repository=notification["repository"],
                issue_type="workflow_failure",
                severity=notification["severity"],
                package_name=None,
                description=notification["description"],
                fix_strategy="workflow_fix",
                estimated_cost=0.001
            )
        return None
    
    def run_fully_autonomous(self) -> Dict[str, Any]:
        """Run the fully autonomous system until completion"""
        start_time = datetime.now()
        
        self.logger.info("ğŸš€ STARTING FULLY AUTONOMOUS AGI SYSTEM")
        self.logger.info("=" * 60)
        self.logger.info("ğŸ¤– AGI Mode: ACTIVE (Emergency budget overrides enabled)")
        self.logger.info("ğŸ”„ Batch Mode: ACTIVE (Intelligent batching enabled)")
        self.logger.info("âš¡ Processing Mode: CONTINUOUS (No manual intervention)")
        self.logger.info("=" * 60)
        
        all_cycles = []
        
        # Run processing cycles until completion or max cycles reached
        while self.processing_cycles < self.max_processing_cycles:
            cycle_result = self.run_autonomous_cycle()
            all_cycles.append(cycle_result)
            
            # Check if there's more work to do
            if (cycle_result["vulnerabilities_processed"] == 0 and 
                cycle_result["workflows_fixed"] == 0 and
                self.batch_processor.get_batch_statistics()["pending_issues"] == 0):
                self.logger.info("âœ… No more issues to process - autonomous cycle complete")
                break
            
            # Brief pause between cycles to prevent overwhelming APIs
            time.sleep(2)
        
        # Generate final report
        final_report = self._generate_final_report(start_time, all_cycles)
        
        self.logger.info("ğŸ‰ FULLY AUTONOMOUS AGI SYSTEM COMPLETE")
        self.logger.info("=" * 60)
        
        return final_report
    
    def _generate_final_report(self, start_time: datetime, all_cycles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Generate comprehensive final report"""
        end_time = datetime.now()
        total_duration = (end_time - start_time).total_seconds()
        
        batch_stats = self.batch_processor.get_batch_statistics()
        
        report = {
            "autonomous_session": {
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "total_duration_seconds": total_duration,
                "processing_cycles": len(all_cycles)
            },
            "performance_metrics": {
                "total_issues_processed": self.total_issues_processed,
                "total_prs_created": self.total_prs_created,
                "total_cost": self.total_cost,
                "issues_per_second": self.total_issues_processed / max(total_duration, 1),
                "cost_per_issue": self.total_cost / max(self.total_issues_processed, 1),
                "average_cycle_duration": total_duration / max(len(all_cycles), 1)
            },
            "batch_processing": {
                "batches_processed": batch_stats["batches_processed"],
                "average_batch_size": batch_stats["average_batch_size"],
                "batch_efficiency": batch_stats["cost_per_issue"],
                "pending_batches": batch_stats["pending_batches"]
            },
            "budget_management": {
                "total_spent": self.total_cost,
                "budget_remaining": self.budget_controller.get_daily_budget_remaining(),
                "emergency_overrides_used": getattr(self.budget_controller, 'emergency_calls', 0),
                "efficiency_score": getattr(self.budget_controller, 'efficiency_score', 100)
            },
            "cycles": all_cycles,
            "success": len([c for c in all_cycles if c["success"]]) == len(all_cycles)
        }
        
        # Save report
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"autonomous_agi_report_{timestamp}.json"
        
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        self.logger.info(f"ğŸ“Š Final report saved to: {report_file}")
        
        return report

def main():
    """Main entry point for fully autonomous AGI system"""
    if len(sys.argv) > 1 and sys.argv[1] == "--autonomous":
        print("ğŸ¤– LAUNCHING FULLY AUTONOMOUS AGI SYSTEM")
        print("ğŸš€ Processing 145+ pages of GitHub notifications autonomously...")
        print("ğŸ”„ Batch processing enabled for maximum efficiency")
        print("âš¡ Emergency budget overrides active for critical issues")
        print("")
        
        agi = FullyAutonomousAGI()
        results = agi.run_fully_autonomous()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ AUTONOMOUS PROCESSING COMPLETE!")
        print("=" * 60)
        print(f"ğŸ“Š Issues Processed: {results['performance_metrics']['total_issues_processed']}")
        print(f"ğŸ”§ PRs Created: {results['performance_metrics']['total_prs_created']}")
        print(f"ğŸ’° Total Cost: ${results['performance_metrics']['total_cost']:.4f}")
        print(f"âš¡ Processing Rate: {results['performance_metrics']['issues_per_second']:.2f} issues/sec")
        print(f"ğŸ”„ Batch Efficiency: ${results['batch_processing']['batch_efficiency']:.4f} per issue")
        print(f"â±ï¸  Total Duration: {results['autonomous_session']['total_duration_seconds']:.1f} seconds")
        print("=" * 60)
        
    else:
        print("Î›Bot Fully Autonomous AGI System")
        print("===============================")
        print("Usage:")
        print("  python3 autonomous_agi_system.py --autonomous")
        print("")
        print("This will start the fully autonomous system that:")
        print("â€¢ Processes 145+ pages of GitHub notifications")
        print("â€¢ Groups issues into efficient batches")  
        print("â€¢ Creates PRs automatically")
        print("â€¢ Operates without manual intervention")
        print("â€¢ Uses emergency budget overrides for critical issues")

if __name__ == "__main__":
    main()
