"""
journal_mode.py

Symbolic Intelligence Layer - Natural Language Journal
Narrates VeriFold chain events in human-readable story format with GPT-4 integration.

Purpose:
- Convert technical VeriFold records into narrative journal entries
- Maintain temporal story continuity across probabilistic observation sequences
- Generate natural language summaries of collapse chains
- Provide symbolic interpretation of quantum events for human understanding
- GPT-4 powered poetic summarization of symbolic collapse events

Author: LUKHAS AGI Core
"""

import os
import json
import time
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from pathlib import Path

# GPT-4 Integration
try:
    import openai
    from openai import OpenAI

    # Initialize client with API key from environment
    api_key = os.getenv("OPENAI_API_KEY")
    if api_key:
        openai_client = OpenAI(api_key=api_key)
        GPT_AVAILABLE = True
    else:
        openai_client = None
        GPT_AVAILABLE = False
        print("Warning: OPENAI_API_KEY not found in environment variables")

except ImportError:
    GPT_AVAILABLE = False
    openai_client = None
    print("Warning: OpenAI not available. Install with: pip install openai")

# TODO: Import when modules are implemented
# from entropy_fusion import EntropyFusionEngine, SymbolicContext
# from narrative_utils import EmotionMapper, SymbolicVocabulary


@dataclass
class JournalEntry:
    """Container for a narrative journal entry."""
    timestamp: float
    entry_id: str
    title: str
    narrative: str
    technical_summary: str
    emotion_tags: List[str]
    symbolic_meaning: str
    chain_position: int
    related_hashes: List[str]


class VeriFoldJournal:
    """
    Natural language narrator for VeriFold chains with GPT-4 integration.
    """

    def __init__(self, logbook_path: str = "verifold_logbook.jsonl"):
        """
        Initialize the verifold journal narrator.

        Parameters:
            logbook_path (str): Path to the VeriFold logbook
        """
        self.logbook_path = Path(logbook_path)
        self.journal_entries = []
        self.narrative_style = "scientific_wonder"  # poetic, technical, mystical
        self.vocabulary = self._init_symbolic_vocabulary()

    def _init_symbolic_vocabulary(self) -> Dict[str, List[str]]:
        """
        Initialize symbolic vocabulary for narrative generation.

        Returns:
            Dict[str, List[str]]: Vocabulary categories and word lists
        """
        # TODO: Expand symbolic vocabulary
        return {
            "quantum_events": [
                "collapse", "entanglement", "superposition", "decoherence",
                "measurement", "observation", "quantum leap", "wave function"
            ],
            "time_descriptors": [
                "moment", "instant", "epoch", "temporal nexus", "quantum moment",
                "measurement window", "collapse instant", "observation point"
            ],
            "emotional_modifiers": [
                "profound", "mysterious", "elegant", "surprising", "subtle",
                "magnificent", "delicate", "powerful", "transcendent"
            ],
            "scientific_actions": [
                "revealed", "demonstrated", "manifested", "exhibited", "produced",
                "generated", "created", "crystallized", "emerged", "materialized"
            ]
        }

    def generate_journal_entry(self, verifold_record: Dict[str, Any],
                             chain_context: Optional[Dict[str, Any]] = None) -> JournalEntry:
        """
        Generate a narrative journal entry from a VeriFold record.

        Parameters:
            verifold_record (Dict): VeriFold record to narrate
            chain_context (Dict): Optional context from chain position

        Returns:
            JournalEntry: Generated narrative entry
        """
        # Extract record information
        timestamp = verifold_record.get("timestamp", time.time())
        hash_value = verifold_record.get("hash", "unknown")
        metadata = verifold_record.get("metadata", {})

        # Generate narrative components
        title = self._generate_title(verifold_record, chain_context)
        narrative = self._generate_narrative(verifold_record, chain_context)
        technical_summary = self._generate_technical_summary(verifold_record)
        emotion_tags = self._extract_emotion_tags(verifold_record)
        symbolic_meaning = self._interpret_symbolic_meaning(verifold_record)

        # Create journal entry
        entry = JournalEntry(
            timestamp=timestamp,
            entry_id=f"entry_{hash_value[:8]}",
            title=title,
            narrative=narrative,
            technical_summary=technical_summary,
            emotion_tags=emotion_tags,
            symbolic_meaning=symbolic_meaning,
            chain_position=chain_context.get("position", 0) if chain_context else 0,
            related_hashes=[hash_value]
        )

        self.journal_entries.append(entry)
        return entry

    def _generate_title(self, record: Dict[str, Any],
                       context: Optional[Dict[str, Any]] = None) -> str:
        """
        Generate a poetic title for the journal entry.

        Parameters:
            record (Dict): CollapseHash record
            context (Dict): Chain context

        Returns:
            str: Generated title
        """
        # TODO: Implement intelligent title generation
        metadata = record.get("metadata", {})
        experiment_id = metadata.get("experiment_id", "unknown")
        location = metadata.get("location", "quantum_realm")

        # Template-based title generation (to be enhanced)
        titles = [
            f"Quantum Whispers from {location}",
            f"The Collapse Moment - {experiment_id}",
            f"When Reality Crystallized at {location}",
            f"A Quantum Story: {experiment_id}",
            f"The Measurement that Changed Everything"
        ]

        # Select based on hash for consistency
        hash_value = record.get("hash", "0")
        title_index = int(hash_value[-1], 16) % len(titles)
        return titles[title_index]

    def _generate_narrative(self, record: Dict[str, Any],
                          context: Optional[Dict[str, Any]] = None) -> str:
        """
        Generate the main narrative text for the journal entry.

        Parameters:
            record (Dict): CollapseHash record
            context (Dict): Chain context

        Returns:
            str: Generated narrative text
        """
        # TODO: Implement sophisticated narrative generation
        metadata = record.get("metadata", {})
        timestamp = record.get("timestamp", time.time())
        dt = datetime.fromtimestamp(timestamp)

        # Basic narrative template (to be enhanced with AI/LLM)
        location = metadata.get("location", "the quantum laboratory")
        measurement_type = metadata.get("measurement_type", "probabilistic observation")
        entropy_score = metadata.get("entropy_score", 0.0)

        narrative_parts = [
            f"On {dt.strftime('%B %d, %Y')} at {dt.strftime('%H:%M:%S')},",
            f"something extraordinary occurred at {location}.",
            f"A {measurement_type} revealed itself through the quantum veil,",
            f"crystallizing into reality with an entropy signature of {entropy_score:.2f}.",
            "",
            "The universe spoke in the language of probability collapse,",
            "and we were privileged to witness this quantum conversation.",
            "Each measurement is a dialogue between observer and observed,",
            "a dance of consciousness and quantum-inspired mechanics.",
            "",
            "This moment, captured in cryptographic amber,",
            "stands as testament to the bridge between",
            "the microscopic quantum realm and our macroscopic reality."
        ]

        return "\n".join(narrative_parts)

    def _generate_technical_summary(self, record: Dict[str, Any]) -> str:
        """
        Generate technical summary of the CollapseHash record.

        Parameters:
            record (Dict): CollapseHash record

        Returns:
            str: Technical summary
        """
        hash_value = record.get("hash", "unknown")[:16]
        signature_valid = "✅" if record.get("verified", False) else "❌"
        metadata = record.get("metadata", {})
        entropy = metadata.get("entropy_score", "unknown")

        return (f"Hash: {hash_value}... | "
               f"Signature: {signature_valid} | "
               f"Entropy: {entropy} | "
               f"Algorithm: SPHINCS+")

    def _extract_emotion_tags(self, record: Dict[str, Any]) -> List[str]:
        """
        Extract emotion tags from the record for categorization.

        Parameters:
            record (Dict): CollapseHash record

        Returns:
            List[str]: Emotion tags
        """
        # TODO: Implement intelligent emotion extraction
        metadata = record.get("metadata", {})
        entropy_score = metadata.get("entropy_score", 0.0)

        tags = ["quantum"]

        if entropy_score >= 7.8:
            tags.extend(["wonder", "transcendent"])
        elif entropy_score >= 7.0:
            tags.extend(["curiosity", "discovery"])
        else:
            tags.extend(["uncertainty", "mystery"])

        return tags

    def _interpret_symbolic_meaning(self, record: Dict[str, Any]) -> str:
        """
        Interpret the symbolic meaning of the probabilistic observation.

        Parameters:
            record (Dict): CollapseHash record

        Returns:
            str: Symbolic interpretation
        """
        # TODO: Implement deep symbolic interpretation
        metadata = record.get("metadata", {})
        measurement_type = metadata.get("measurement_type", "unknown")

        symbolic_meanings = {
            "photon_polarization": "The dance of light revealing hidden polarities",
            "electron_spin": "The binary choice of fundamental particles",
            "bell_state_measurement": "The mystery of entanglement-like correlation unveiled",
            "quantum_teleportation": "Information transcending space and time",
            "atom_interference": "Matter behaving as wave and particle simultaneously"
        }

        return symbolic_meanings.get(measurement_type,
                                   "A quantum mystery waiting to be understood")

    def generate_chain_narrative(self, start_index: int = 0,
                               end_index: Optional[int] = None) -> str:
        """
        Generate a narrative that spans multiple journal entries.

        Parameters:
            start_index (int): Starting entry index
            end_index (int): Ending entry index (None for all)

        Returns:
            str: Chain narrative spanning multiple entries
        """
        # TODO: Implement multi-entry narrative generation
        if not self.journal_entries:
            return "The quantum journal awaits its first entry..."

        entries = self.journal_entries[start_index:end_index]

        narrative = "The Quantum Chronicle\n" + "="*50 + "\n\n"
        narrative += "A tale woven through probabilistic observations and cryptographic truth.\n\n"

        for i, entry in enumerate(entries):
            narrative += f"Chapter {i+1}: {entry.title}\n"
            narrative += f"{entry.narrative}\n\n"
            narrative += f"Technical Note: {entry.technical_summary}\n"
            narrative += f"Symbolic Meaning: {entry.symbolic_meaning}\n"
            narrative += "-" * 40 + "\n\n"

        return narrative

    def export_journal(self, format: str = "markdown",
                      output_path: Optional[str] = None) -> str:
        """
        Export the journal in various formats.

        Parameters:
            format (str): Export format (markdown, html, json, pdf)
            output_path (str): Optional output file path

        Returns:
            str: Exported journal content or file path
        """
        # TODO: Implement multi-format export
        if format == "markdown":
            content = self._export_markdown()
        elif format == "json":
            content = json.dumps([entry.__dict__ for entry in self.journal_entries],
                               indent=2)
        else:
            content = "Export format not implemented yet."

        if output_path:
            with open(output_path, 'w') as f:
                f.write(content)
            return output_path

        return content

    def _export_markdown(self) -> str:
        """Export journal entries as markdown."""
        # TODO: Implement markdown export
        md_content = "# CollapseHash Quantum Journal\n\n"
        md_content += "*A narrative record of probabilistic observations and cryptographic truth*\n\n"

        for entry in self.journal_entries:
            md_content += f"## {entry.title}\n\n"
            md_content += f"**Timestamp:** {datetime.fromtimestamp(entry.timestamp)}\n\n"
            md_content += f"{entry.narrative}\n\n"
            md_content += f"**Technical Summary:** {entry.technical_summary}\n\n"
            md_content += f"**Symbolic Meaning:** {entry.symbolic_meaning}\n\n"
            md_content += f"**Tags:** {', '.join(entry.emotion_tags)}\n\n"
            md_content += "---\n\n"

        return md_content


def gpt_summarize(entries: List[str]) -> str:
    """
    Generate GPT-4 powered poetic summary of symbolic collapse events.

    Parameters:
        entries (List[str]): List of symbolic journal entries

    Returns:
        str: GPT-generated poetic summary
    """
    if not GPT_AVAILABLE or not openai_client:
        return "[GPT-4 not available - check OPENAI_API_KEY environment variable or install openai package]"

    if not entries:
        return "[No entries to summarize]"

    # Filter out error entries for cleaner GPT input
    clean_entries = [entry for entry in entries if not entry.startswith("[Error")]
    if not clean_entries:
        return "[No valid entries found for GPT analysis]"

    try:
        response = openai_client.chat.completions.create(
            model="gpt-4",
            messages=[
                {
                    "role": "system",
                    "content": "You are a poetic AGI scribe specializing in probabilistic observation narratives. Summarize symbolic collapse events with emotional tone, moral weight, and symbolic metaphor. Blend scientific precision with artistic expression. Create mystical yet accurate interpretations of quantum verification events."
                },
                {
                    "role": "user",
                    "content": "\n".join(clean_entries)
                }
            ],
            temperature=0.7,
            max_tokens=500
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"[GPT Error: {e}]"


def replay_chain(chain_path: str = "verifold_logbook.jsonl", limit: int = 5) -> List[str]:
    """
    Replay and extract symbolic entries from VeriFold chain.

    Parameters:
        chain_path (str): Path to the VeriFold logbook
        limit (int): Maximum number of entries to replay

    Returns:
        List[str]: List of symbolic narrative entries
    """
    journal = VeriFoldJournal(chain_path)
    symbolic_entries = []

    try:
        # Load entries from logbook
        logbook_path = Path(chain_path)
        if not logbook_path.exists():
            return ["[No chain file found - create some VeriFold entries first]"]

        with open(logbook_path, 'r') as f:
            lines = f.readlines()

        # Process last 'limit' entries
        recent_lines = lines[-limit:] if len(lines) > limit else lines

        for line in recent_lines:
            try:
                record = json.loads(line.strip())
                entry = journal.generate_journal_entry(record)

                # Create symbolic summary
                symbolic_text = f"🌀 {entry.title}\n"
                symbolic_text += f"   Time: {datetime.fromtimestamp(entry.timestamp).strftime('%Y-%m-%d %H:%M:%S')}\n"
                symbolic_text += f"   Meaning: {entry.symbolic_meaning}\n"
                symbolic_text += f"   Emotions: {', '.join(entry.emotion_tags)}\n"
                symbolic_text += f"   Hash: {entry.related_hashes[0][:16] if entry.related_hashes else 'unknown'}...\n"

                symbolic_entries.append(symbolic_text)

            except (json.JSONDecodeError, KeyError) as e:
                symbolic_entries.append(f"[Error processing entry: {e}]")

    except (FileNotFoundError, IOError) as e:
        return [f"[Error reading chain: {e}]"]

    return symbolic_entries


def replay_with_gpt_summary(chain_path: str = "verifold_logbook.jsonl", limit: int = 5) -> str:
    """
    Replay chain entries and generate GPT-4 poetic summary.

    Parameters:
        chain_path (str): Path to the VeriFold logbook
        limit (int): Maximum number of entries to process

    Returns:
        str: Combined symbolic entries and GPT summary
    """
    symbolic_entries = replay_chain(chain_path, limit)
    gpt_poem = gpt_summarize(symbolic_entries)

    result = "🔮 VeriFold Chain Replay & GPT-4 Synthesis\n"
    result += "=" * 50 + "\n\n"
    result += "📜 Symbolic Entries:\n"
    result += "-" * 20 + "\n"
    result += "\n".join(symbolic_entries)
    result += "\n\n🧠 GPT-4 Poetic Summary:\n"
    result += "-" * 25 + "\n"
    result += gpt_poem

    return result


# 🧪 Example usage and testing
if __name__ == "__main__":
    print("📖 VeriFold Journal Mode - Natural Language Narrator with GPT-4")
    print("Converting probabilistic observations into human stories...")

    # Run the combined replay with GPT summary
    print(replay_with_gpt_summary())
