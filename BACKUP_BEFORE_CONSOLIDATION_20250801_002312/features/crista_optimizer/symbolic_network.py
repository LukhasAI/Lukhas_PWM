# ═══════════════════════════════════════════════════════════════════════════
# FILENAME: symbolic_network.py
# MODULE: core.adaptive_systems.crista_optimizer.symbolic_network
# DESCRIPTION: Defines the core symbolic network infrastructure for LUKHAS AGI,
#              including node and connection types, and network management operations.
# DEPENDENCIES: logging, numpy, typing, dataclasses, enum, json, time, .crista_optimizer
# LICENSE: PROPRIETARY - LUKHAS AI SYSTEMS - UNAUTHORIZED ACCESS PROHIBITED
# ═══════════════════════════════════════════════════════════════════════════

import structlog
import numpy as np
from typing import Dict, List, Optional, Set, Tuple, Any # Set and json not directly used, but kept for potential future use.
from dataclasses import dataclass, field
from enum import Enum
import json # Kept for future use, e.g. richer metadata serialization
import time

# Assuming NetworkConfig is defined in crista_optimizer.py or a shared config location.
# If crista_optimizer.py also imports this file, this could lead to circular dependency.
# It's better if NetworkConfig is in its own file or a common config module.
# For now, proceeding with the assumption it's resolvable.
# ΛNOTE: Attempting to import NetworkConfig from .crista_optimizer. Includes a fallback placeholder for NetworkConfig to handle potential ImportError, which might occur during isolated testing or if the module structure changes. This ensures the module can load with a default config for basic type hinting or functionality, though full operation might be impaired.
try:
    from .crista_optimizer import NetworkConfig
except ImportError:
    # Fallback or placeholder if direct import fails (e.g. during isolated testing or if structure changes)
    logger_temp = structlog.get_logger("ΛTRACE.core.adaptive_systems.crista_optimizer.symbolic_network_import_fallback")
    logger_temp.warning("ΛTRACE: Could not import NetworkConfig from .crista_optimizer. Using a placeholder if available or will error later.")
    # Define a placeholder NetworkConfig if necessary for type hinting or basic functionality
    @dataclass
    class NetworkConfig:
        fission_threshold: float = 0.7
        fusion_threshold: float = 0.3
        remodeling_rate: float = 0.42 # Unused in this file directly by SymbolicNetwork
        max_nodes: int = 1000
        min_nodes: int = 10
        entropy_balance_weight: float = 0.1 # Used in SymbolicNetwork.entropy_balance_pass


# Initialize logger for ΛTRACE using structlog
# Assumes structlog is configured in a higher-level __init__.py (e.g., core/__init__.py)
logger = structlog.get_logger("ΛTRACE.core.adaptive_systems.crista_optimizer.symbolic_network")
logger.info("ΛTRACE: Initializing symbolic_network module.")

# Enum for Symbolic Node Types
# Defines the various types of symbolic nodes that can exist in the network.
class NodeType(Enum):
    """Defines the various types of symbolic nodes that can exist in the network."""
    MEMORY = "memory"
    PROCESSING = "processing"
    DECISION = "decision"
    SENSOR = "sensor"
    ACTUATOR = "actuator"
    META = "meta" # For self-representation or higher-order processing
    BRIDGE = "bridge" # For connecting to other networks or systems

logger.info(f"ΛTRACE: NodeType Enum defined with values: {[ntype.value for ntype in NodeType]}")

# Enum for Connection Types
# Defines the types of connections that can exist between symbolic nodes.
class ConnectionType(Enum):
    """Defines the types of connections that can exist between symbolic nodes."""
    STRONG = "strong"
    WEAK = "weak"
    INHIBITORY = "inhibitory" # Reduces target node activity/weight
    EXCITATORY = "excitatory" # Increases target node activity/weight
    TEMPORAL = "temporal"   # Connection related to time-based sequences
    SPATIAL = "spatial"     # Connection related to spatial relationships

logger.info(f"ΛTRACE: ConnectionType Enum defined with values: {[ctype.value for ctype in ConnectionType]}")

# Dataclass for a Symbolic Node
# Represents a single symbolic processing node within the cognitive network.
# It holds state, performance metrics, connectivity information, and metadata.
@dataclass
class SymbolicNode:
    """
    Represents a single symbolic processing node within the cognitive network.
    It holds state, performance metrics, connectivity information, and metadata.
    """
    node_id: str
    node_type: NodeType = NodeType.PROCESSING
    symbolic_weight: float = 1.0 # Represents importance or capacity
    position: Tuple[float, float, float] = (0.0, 0.0, 0.0) # For spatial organization if used
    metadata: Dict[str, Any] = field(default_factory=dict) # For arbitrary additional data

    # Performance metrics
    error_level: float = 0.0       # Normalized error (0.0 to 1.0)
    activity_level: float = 0.0    # Normalized activity (0.0 to 1.0)
    entropy: float = 0.0           # Measure of disorder or uncertainty
    processing_load: float = 0.0   # Normalized processing load (0.0 to 1.0)

    # Connectivity
    connections: List[str] = field(default_factory=list) # List of target node_ids
    connection_weights: Dict[str, float] = field(default_factory=dict) # target_id -> weight

    # Temporal tracking
    creation_time: float = field(default_factory=time.time)
    last_update: float = field(default_factory=time.time)
    update_count: int = 0

    # State
    is_active: bool = True     # Whether the node is currently active/enabled
    is_critical: bool = False  # Whether the node is in a critical state (e.g., high error/load)

    # Post-initialization logger setup
    def __post_init__(self):
        self.logger = logger.getChild(f"SymbolicNode.{self.node_id}")
        self.logger.info(f"ΛTRACE: SymbolicNode '{self.node_id}' (Type: {self.node_type.value}) created. Weight: {self.symbolic_weight:.2f}, Position: {self.position}")

    # Method to update performance metrics
    def update_metrics(self, error: float, activity: float, entropy: float, load: Optional[float] = None) -> None:
        """
        Updates the node's performance metrics, ensuring values are within valid ranges.
        Also updates the 'last_update' timestamp and increments 'update_count'.
        Auto-detects critical state based on updated metrics.
        """
        self.logger.debug(f"ΛTRACE: Updating metrics for SymbolicNode '{self.node_id}'. Input: error={error:.2f}, activity={activity:.2f}, entropy={entropy:.2f}, load={load if load is not None else 'N/A'}")
        self.error_level = max(0.0, min(1.0, error))
        self.activity_level = max(0.0, min(1.0, activity))
        self.entropy = max(0.0, entropy) # Entropy can be > 1 depending on calculation

        if load is not None:
            self.processing_load = max(0.0, min(1.0, load))

        self.last_update = time.time()
        self.update_count += 1

        # Auto-detect critical state based on thresholds
        # These thresholds could be configurable
        prev_critical_state = self.is_critical
        self.is_critical = (self.error_level > 0.8 or
                           self.processing_load > 0.9 or
                           (self.activity_level < 0.1 and self.is_active)) # Low activity for an active node

        if self.is_critical != prev_critical_state:
            self.logger.info(f"ΛTRACE: SymbolicNode '{self.node_id}' critical state changed to {self.is_critical}.")
        self.logger.debug(f"ΛTRACE: SymbolicNode '{self.node_id}' metrics updated. Error: {self.error_level:.2f}, Activity: {self.activity_level:.2f}, Entropy: {self.entropy:.2f}, Load: {self.processing_load:.2f}, Critical: {self.is_critical}")

    # Method to split the node
    def split(self, style: str = "crista_junction", split_ratio: float = 0.5) -> List['SymbolicNode']:
        """
        Splits this node into two new child nodes. The symbolic weight, connections,
        and performance characteristics are distributed based on the split_ratio.
        Args:
            style (str): A descriptive tag for the split operation.
            split_ratio (float): The ratio for distributing weight to the first child (0.1 to 0.9).
        Returns:
            List[SymbolicNode]: A list containing the two newly created child nodes.
        """
        self.logger.info(f"ΛTRACE: Splitting SymbolicNode '{self.node_id}' (Style: {style}, Ratio: {split_ratio:.2f}).")
        if not (0.1 <= split_ratio <= 0.9): # Ensure ratio is valid
            self.logger.warning(f"ΛTRACE: Invalid split_ratio {split_ratio} for node '{self.node_id}'. Defaulting to 0.5.")
            split_ratio = 0.5

        child_nodes: List[SymbolicNode] = []
        ratios = [split_ratio, 1.0 - split_ratio]

        for i, ratio_val in enumerate(ratios):
            # Generate a unique ID for the child node
            child_id = f"{self.node_id}_child{i}_{int(time.time()*1000)}"
            child_pos = self._calculate_child_position(i, len(ratios))

            child_metadata = self.metadata.copy()
            child_metadata['parent_node'] = self.node_id
            child_metadata['split_style'] = style
            child_metadata['split_index'] = i
            child_metadata['split_ratio_applied'] = ratio_val

            child = SymbolicNode(
                node_id=child_id,
                node_type=self.node_type, # Child inherits type
                symbolic_weight=self.symbolic_weight * ratio_val,
                position=child_pos,
                metadata=child_metadata
            )
            self.logger.debug(f"ΛTRACE: Created child node '{child_id}' with weight {child.symbolic_weight:.2f}, position {child.position}.")

            # Distribute connections and their weights
            child.connections = self.connections.copy() # Inherit all connections
            child.connection_weights = {
                conn_id: weight * ratio_val # Scale connection weights
                for conn_id, weight in self.connection_weights.items()
            }
            self.logger.debug(f"ΛTRACE: Child node '{child_id}' inherited {len(child.connections)} connections with scaled weights.")

            # Inherit and slightly vary performance characteristics
            # Adding a small random jitter to error to differentiate children slightly
            error_jitter = (np.random.rand() - 0.5) * 0.05
            child.update_metrics(
                error=max(0.0, min(1.0, self.error_level + error_jitter)),
                activity=self.activity_level * ratio_val, # Activity scaled by ratio
                entropy=self.entropy * ratio_val,       # Entropy scaled by ratio
                load=self.processing_load * ratio_val     # Load scaled by ratio
            )
            self.logger.debug(f"ΛTRACE: Child node '{child_id}' metrics initialized after split.")
            child_nodes.append(child)

        self.logger.info(f"ΛTRACE: SymbolicNode '{self.node_id}' successfully split into {len(child_nodes)} child nodes.")
        return child_nodes

    # Private helper to calculate child position
    def _calculate_child_position(self, index: int, total_children: int) -> Tuple[float, float, float]:
        """Calculates a slightly offset position for a child node after a split."""
        # Simple offset logic, can be made more sophisticated (e.g., based on style)
        x, y, z = self.position
        # Offset along a diagonal, proportional to index
        offset_scale = 0.1 # Defines how far children are placed from parent
        offset_x = offset_scale * (index - (total_children -1) / 2.0)
        offset_y = offset_scale * (index - (total_children -1) / 2.0) * (-1 if index % 2 == 0 else 1) # Alternate y offset
        return (x + offset_x, y + offset_y, z) # Keep z the same for now

    # Method to merge this node with another
    def merge_with(self, other_node: 'SymbolicNode') -> 'SymbolicNode':
        """
        Merges this node with another SymbolicNode, creating a new node that combines
        their characteristics (weight, connections, metrics, metadata).
        Args:
            other_node (SymbolicNode): The other node to merge with.
        Returns:
            SymbolicNode: The new, merged SymbolicNode.
        """
        self.logger.info(f"ΛTRACE: Merging SymbolicNode '{self.node_id}' with '{other_node.node_id}'.")

        # Create a unique ID for the merged node
        merged_id = f"merged_{self.node_id}_{other_node.node_id}_{int(time.time()*1000)}"
        # Sum symbolic weights
        merged_weight = self.symbolic_weight + other_node.symbolic_weight
        self.logger.debug(f"ΛTRACE: Merged node ID: '{merged_id}', Merged weight: {merged_weight:.2f}.")

        # Average positions (simple averaging)
        merged_position = tuple(np.mean(np.array([self.position, other_node.position]), axis=0))
        self.logger.debug(f"ΛTRACE: Merged position: {merged_position}.")

        # Combine metadata, prioritizing self for conflicts, could be more nuanced
        merged_metadata = other_node.metadata.copy() # Start with other's metadata
        merged_metadata.update(self.metadata)       # Update with self's, overwriting conflicts
        merged_metadata['merged_from_nodes'] = [self.node_id, other_node.node_id]
        merged_metadata['merge_timestamp'] = time.time()
        self.logger.debug(f"ΛTRACE: Metadata combined for merged node.")

        # Create the new merged node. Node type from 'self' is prioritized.
        merged_node = SymbolicNode(
            node_id=merged_id,
            node_type=self.node_type,
            symbolic_weight=merged_weight,
            position=merged_position,
            metadata=merged_metadata
        )
        self.logger.debug(f"ΛTRACE: Merged SymbolicNode object created: '{merged_id}'.")

        # Merge connections: union of connections
        merged_node.connections = list(set(self.connections + other_node.connections))
        self.logger.debug(f"ΛTRACE: Merged connections list created with {len(merged_node.connections)} unique connections.")

        # Merge connection weights: weighted average based on original nodes' symbolic weights
        all_conn_ids = set(self.connection_weights.keys()) | set(other_node.connection_weights.keys())
        for conn_id in all_conn_ids:
            w1 = self.connection_weights.get(conn_id, 0.0) * self.symbolic_weight
            w2 = other_node.connection_weights.get(conn_id, 0.0) * other_node.symbolic_weight
            # Ensure total_merged_weight is not zero to avoid division by zero if both nodes had zero weight (unlikely but possible)
            total_merged_weight_for_conn = self.symbolic_weight + other_node.symbolic_weight
            merged_node.connection_weights[conn_id] = (w1 + w2) / total_merged_weight_for_conn if total_merged_weight_for_conn > 0 else 0.0
        self.logger.debug(f"ΛTRACE: Connection weights merged for {len(merged_node.connection_weights)} connections.")

        # Merge performance metrics: weighted average by symbolic_weight for error, activity, load. Sum for entropy.
        if merged_weight > 0:
            merged_error = (self.error_level * self.symbolic_weight + other_node.error_level * other_node.symbolic_weight) / merged_weight
            merged_activity = (self.activity_level * self.symbolic_weight + other_node.activity_level * other_node.symbolic_weight) / merged_weight
            merged_load = (self.processing_load * self.symbolic_weight + other_node.processing_load * other_node.symbolic_weight) / merged_weight
        else: # Avoid division by zero if both nodes had zero weight
            merged_error = (self.error_level + other_node.error_level) / 2
            merged_activity = (self.activity_level + other_node.activity_level) / 2
            merged_load = (self.processing_load + other_node.processing_load) / 2
        merged_entropy = self.entropy + other_node.entropy # Entropy is often additive or combined in more complex ways

        merged_node.update_metrics(
            error=merged_error,
            activity=merged_activity,
            entropy=merged_entropy,
            load=merged_load
        )
        self.logger.info(f"ΛTRACE: SymbolicNode '{self.node_id}' and '{other_node.node_id}' successfully merged into '{merged_id}'.")
        return merged_node

    # Method to add a connection
    def add_connection(self, target_node_id: str, weight: float = 1.0, connection_type: ConnectionType = ConnectionType.STRONG) -> None:
        """Adds or updates a connection to a target node with a specified weight and type."""
        self.logger.debug(f"ΛTRACE: Adding/updating connection from '{self.node_id}' to '{target_node_id}' (Weight: {weight:.2f}, Type: {connection_type.value}).")
        if target_node_id not in self.connections:
            self.connections.append(target_node_id)
        self.connection_weights[target_node_id] = weight
        # Store connection type in metadata, prefixed to avoid clashes
        self.metadata[f'connection_type_to_{target_node_id}'] = connection_type.value
        self.last_update = time.time()

    # Method to remove a connection
    def remove_connection(self, target_node_id: str) -> None:
        """Removes a connection to a target node."""
        self.logger.debug(f"ΛTRACE: Removing connection from '{self.node_id}' to '{target_node_id}'.")
        if target_node_id in self.connections:
            self.connections.remove(target_node_id)
        self.connection_weights.pop(target_node_id, None) # Remove weight if exists
        self.metadata.pop(f'connection_type_to_{target_node_id}', None) # Remove type from metadata
        self.last_update = time.time()

    # Method to get connection strength
    def get_connection_strength(self, target_node_id: str) -> float:
        """Returns the weight of the connection to the target node, or 0.0 if not connected."""
        strength = self.connection_weights.get(target_node_id, 0.0)
        self.logger.debug(f"ΛTRACE: Connection strength from '{self.node_id}' to '{target_node_id}': {strength:.2f}.")
        return strength

    # Method to check connectivity
    def is_connected_to(self, target_node_id: str) -> bool:
        """Checks if this node is directly connected to the target node."""
        connected = target_node_id in self.connections
        self.logger.debug(f"ΛTRACE: Node '{self.node_id}' connected to '{target_node_id}': {connected}.")
        return connected

    # Method to get a summary of the node's state
    def get_state_summary(self) -> Dict[str, Any]:
        """
        Returns a comprehensive dictionary summarizing the node's current state,
        including ID, type, weight, performance metrics, connectivity, and status.
        """
        self.logger.debug(f"ΛTRACE: Getting state summary for SymbolicNode '{self.node_id}'.")
        avg_conn_weight = np.mean(list(self.connection_weights.values())) if self.connection_weights else 0.0
        summary = {
            'node_id': self.node_id,
            'node_type': self.node_type.value,
            'symbolic_weight': self.symbolic_weight,
            'position': self.position,
            'metadata_keys': list(self.metadata.keys()), # Only keys to keep summary concise
            'performance': {
                'error_level': self.error_level,
                'activity_level': self.activity_level,
                'entropy': self.entropy,
                'processing_load': self.processing_load
            },
            'connectivity': {
                'connection_count': len(self.connections),
                'connected_nodes': self.connections[:10], # Sample of connected nodes
                'total_connection_weight': sum(self.connection_weights.values()),
                'average_connection_weight': avg_conn_weight
            },
            'status_flags': {
                'is_active': self.is_active,
                'is_critical': self.is_critical
            },
            'temporal': {
                'creation_timestamp': self.creation_time,
                'age_seconds': round(time.time() - self.creation_time, 2),
                'last_update_timestamp': self.last_update,
                'updates_count': self.update_count
            }
        }
        self.logger.debug(f"ΛTRACE: State summary for SymbolicNode '{self.node_id}' generated.")
        return summary

# Class for Managing the Symbolic Network
# Manages the overall topology and operations of the symbolic cognitive network.
# This includes adding/removing nodes and connections, performing network-wide
# operations like entropy balancing, and providing network statistics.
class SymbolicNetwork:
    """
    Manages the overall topology and operations of the symbolic cognitive network.
    This includes adding/removing nodes and connections, performing network-wide
    operations like entropy balancing, and providing network statistics.
    """
    # Initialization
    def __init__(self, config: NetworkConfig):
        """
        Initializes the SymbolicNetwork.
        Args:
            config (NetworkConfig): Configuration settings for the network's behavior.
        """
        self.config = config
        self.nodes: Dict[str, SymbolicNode] = {} # node_id -> SymbolicNode object
        self.connections: List[Tuple[str, str]] = [] # List of (source_id, target_id)
        self.connection_metadata: Dict[Tuple[str, str], Dict[str, Any]] = {} # (src,dst) -> metadata dict

        self.logger = logger.getChild("SymbolicNetwork") # Child logger for this class instance
        self.logger.info(f"ΛTRACE: SymbolicNetwork initialized with config: MaxNodes={config.max_nodes}, MinNodes={config.min_nodes}.")

        # Network state tracking
        self.creation_time: float = time.time()
        self.last_optimization_time: float = 0.0
        self.optimization_count: int = 0

        # Performance and event logging
        self.performance_history: List[Dict[str, Any]] = [] # Log of performance snapshots
        self.event_log: List[Dict[str, Any]] = [] # Log of significant network events

    # Method to add a node
    def add_node(self, node: SymbolicNode) -> bool:
        """
        Adds a SymbolicNode to the network.
        Returns True if successful, False otherwise (e.g., node exists, max nodes reached).
        """
        self.logger.debug(f"ΛTRACE: Attempting to add node '{node.node_id}' (Type: {node.node_type.value}) to network.")
        if node.node_id in self.nodes:
            self.logger.warning(f"ΛTRACE: Node '{node.node_id}' already exists in the network. Addition aborted.")
            return False

        if len(self.nodes) >= self.config.max_nodes:
            self.logger.warning(f"ΛTRACE: Maximum node count ({self.config.max_nodes}) reached. Cannot add node '{node.node_id}'.")
            return False

        self.nodes[node.node_id] = node
        self._log_event('node_added', {'node_id': node.node_id, 'type': node.node_type.value, 'weight': node.symbolic_weight})
        self.logger.info(f"ΛTRACE: Node '{node.node_id}' added to network. Total nodes: {len(self.nodes)}.")
        return True

    # Method to remove a node
    def remove_node(self, node_id: str) -> bool:
        """
        Removes a SymbolicNode from the network by its ID.
        Also removes all connections to and from this node.
        Returns True if successful, False otherwise (e.g., node not found, min nodes reached).
        """
        self.logger.debug(f"ΛTRACE: Attempting to remove node '{node_id}' from network.")
        if node_id not in self.nodes:
            self.logger.warning(f"ΛTRACE: Node '{node_id}' not found in network. Removal aborted.")
            return False

        if len(self.nodes) <= self.config.min_nodes:
            self.logger.warning(f"ΛTRACE: Minimum node count ({self.config.min_nodes}) reached. Cannot remove node '{node_id}'.")
            return False

        # Remove all global connections involving this node
        original_connections_count = len(self.connections)
        self.connections = [
            (src, dst) for src, dst in self.connections
            if src != node_id and dst != node_id
        ]
        self.logger.debug(f"ΛTRACE: Removed {original_connections_count - len(self.connections)} global connections involving node '{node_id}'.")

        # Clean up connection metadata for connections involving the removed node
        keys_to_remove_meta = [key for key in self.connection_metadata.keys() if node_id in key]
        for key in keys_to_remove_meta:
            del self.connection_metadata[key]
        self.logger.debug(f"ΛTRACE: Cleaned up {len(keys_to_remove_meta)} entries from connection_metadata for node '{node_id}'.")

        # Remove from other nodes' local connection lists
        for other_node_id, other_node_obj in self.nodes.items():
            if other_node_id != node_id and other_node_obj.is_connected_to(node_id):
                other_node_obj.remove_connection(node_id) # This logs at SymbolicNode level
                self.logger.debug(f"ΛTRACE: Removed connection from node '{other_node_id}' to removed node '{node_id}'.")

        removed_node_type = self.nodes[node_id].node_type.value
        del self.nodes[node_id]
        self._log_event('node_removed', {'node_id': node_id, 'type': removed_node_type})
        self.logger.info(f"ΛTRACE: Node '{node_id}' removed from network. Total nodes: {len(self.nodes)}.")
        return True

    # Method to add a connection
    def add_connection(self, source_node_id: str, target_node_id: str, weight: float = 1.0,
                      connection_type: ConnectionType = ConnectionType.STRONG) -> bool:
        """
        Adds a directed connection between two nodes in the network.
        Updates connection metadata and the source node's local connection list.
        Returns True if successful, False otherwise (e.g., nodes not found).
        """
        self.logger.debug(f"ΛTRACE: Attempting to add connection from '{source_node_id}' to '{target_node_id}' (Weight: {weight:.2f}, Type: {connection_type.value}).")
        if source_node_id not in self.nodes or target_node_id not in self.nodes:
            self.logger.warning(f"ΛTRACE: Cannot add connection: one or both nodes not found ('{source_node_id}' or '{target_node_id}').")
            return False

        connection_tuple = (source_node_id, target_node_id)
        if connection_tuple not in self.connections:
            self.connections.append(connection_tuple)
            self.logger.debug(f"ΛTRACE: Global connection entry {connection_tuple} added.")
        else:
            self.logger.debug(f"ΛTRACE: Global connection entry {connection_tuple} already exists, metadata will be updated.")

        # Update connection metadata (overwrite if exists, or add new)
        self.connection_metadata[connection_tuple] = {
            'weight': weight,
            'type': connection_type.value,
            'timestamp_created_updated': time.time()
        }
        self.logger.debug(f"ΛTRACE: Connection metadata for {connection_tuple} updated/set.")

        # Update the source node's local connection list and weights
        self.nodes[source_node_id].add_connection(target_node_id, weight, connection_type) # This logs at SymbolicNode

        self._log_event('connection_added', {
            'source': source_node_id, 'target': target_node_id,
            'weight': weight, 'type': connection_type.value
        })
        self.logger.info(f"ΛTRACE: Connection from '{source_node_id}' to '{target_node_id}' added/updated. Total connections: {len(self.connections)}.")
        return True

    # Method to remove a connection
    def remove_connection(self, source_node_id: str, target_node_id: str) -> bool:
        """
        Removes a directed connection between two nodes.
        Updates connection metadata and the source node's local list.
        Returns True if successful, False if connection didn't exist.
        """
        self.logger.debug(f"ΛTRACE: Attempting to remove connection from '{source_node_id}' to '{target_node_id}'.")
        connection_tuple = (source_node_id, target_node_id)
        if connection_tuple in self.connections:
            self.connections.remove(connection_tuple)
            self.connection_metadata.pop(connection_tuple, None) # Remove metadata if it exists
            self.logger.debug(f"ΛTRACE: Global connection entry {connection_tuple} and its metadata removed.")

            if source_node_id in self.nodes:
                self.nodes[source_node_id].remove_connection(target_node_id) # This logs at SymbolicNode

            self._log_event('connection_removed', {'source': source_node_id, 'target': target_node_id})
            self.logger.info(f"ΛTRACE: Connection from '{source_node_id}' to '{target_node_id}' removed. Total connections: {len(self.connections)}.")
            return True
        else:
            self.logger.warning(f"ΛTRACE: Connection from '{source_node_id}' to '{target_node_id}' not found for removal.")
            return False

    # Method to get high-error nodes
    def high_error_nodes(self) -> List[SymbolicNode]:
        """Returns a list of active nodes whose error level exceeds the configured fission threshold."""
        self.logger.debug(f"ΛTRACE: Identifying high error nodes (Threshold: {self.config.fission_threshold:.2f}).")
        nodes_list = [
            node for node in self.nodes.values()
            if node.error_level > self.config.fission_threshold and node.is_active
        ]
        self.logger.info(f"ΛTRACE: Found {len(nodes_list)} high error active nodes.")
        return nodes_list

    # Method to get low-activity pairs for merging
    def low_activity_pairs(self) -> List[Tuple[SymbolicNode, SymbolicNode]]:
        """
        Identifies pairs of active, non-critical nodes with low activity levels that are
        compatible and candidates for merging. Returns a limited number of pairs.
        """
        self.logger.debug(f"ΛTRACE: Identifying low activity node pairs for potential fusion (Threshold: {self.config.fusion_threshold:.2f}).")
        # Filter for active, non-critical nodes with low activity
        candidate_nodes = [
            node for node in self.nodes.values()
            if (node.activity_level < self.config.fusion_threshold and
                node.is_active and not node.is_critical)
        ]
        self.logger.debug(f"ΛTRACE: Found {len(candidate_nodes)} candidate nodes for low-activity pairing.")

        pairs: List[Tuple[SymbolicNode, SymbolicNode]] = []
        # Generate unique pairs of these candidate nodes
        for i in range(len(candidate_nodes)):
            for j in range(i + 1, len(candidate_nodes)):
                node1, node2 = candidate_nodes[i], candidate_nodes[j]
                # Further check for merge compatibility (e.g., type, weight ratio, spatial proximity)
                if self._are_merge_compatible(node1, node2): # This logs its own details
                    pairs.append((node1, node2))

        self.logger.debug(f"ΛTRACE: Found {len(pairs)} compatible low-activity pairs before sorting/limiting.")
        # Sort pairs by the sum of their activity levels (lowest sum first) to prioritize merging the least active
        pairs.sort(key=lambda p_nodes: p_nodes[0].activity_level + p_nodes[1].activity_level)
        limited_pairs = pairs[:5]  # Limit to a small number per cycle
        self.logger.info(f"ΛTRACE: Returning {len(limited_pairs)} low activity pairs for fusion (limited to 5).")
        return limited_pairs

    # Private helper to check merge compatibility
    def _are_merge_compatible(self, node1: SymbolicNode, node2: SymbolicNode) -> bool:
        """Checks if two nodes are suitable for merging based on type, weight, and position."""
        self.logger.debug(f"ΛTRACE: Checking merge compatibility between '{node1.node_id}' and '{node2.node_id}'.")
        # Rule 1: Must be of the same NodeType for logical consistency
        if node1.node_type != node2.node_type:
            self.logger.debug(f"ΛTRACE: Merge incompatible (type mismatch): {node1.node_type.value} vs {node2.node_type.value}.")
            return False

        # Rule 2: Symbolic weights should not be drastically different (e.g., ratio < 5)
        # Avoid division by zero if a weight is zero or very small.
        min_weight = min(node1.symbolic_weight, node2.symbolic_weight)
        max_weight = max(node1.symbolic_weight, node2.symbolic_weight)
        if min_weight < 0.01: # If one node has negligible weight, consider them compatible for merging (it gets absorbed)
             pass # Compatible or handle as a special case if needed.
        elif max_weight / min_weight > 5.0: # Example threshold for weight difference
            self.logger.debug(f"ΛTRACE: Merge incompatible (weight ratio > 5): {node1.symbolic_weight:.2f} vs {node2.symbolic_weight:.2f}.")
            return False

        # Rule 3: Spatial proximity (if positions are meaningful and used in the system)
        # This threshold (1.0) is arbitrary and depends on coordinate system scale
        distance = np.linalg.norm(np.array(node1.position) - np.array(node2.position))
        if distance > 1.0:  # Example threshold for spatial compatibility
            self.logger.debug(f"ΛTRACE: Merge incompatible (distance > 1.0): {distance:.2f}.")
            return False

        self.logger.debug(f"ΛTRACE: Nodes '{node1.node_id}' and '{node2.node_id}' are merge-compatible.")
        return True

    # Method to merge node pairs
    def merge_nodes(self, node_pairs_to_merge: List[Tuple[SymbolicNode, SymbolicNode]]) -> None:
        """
        Merges specified pairs of nodes. For each pair, a new merged node is created
        and added to the network, while the original two nodes are removed.
        """
        self.logger.info(f"ΛTRACE: Attempting to merge {len(node_pairs_to_merge)} node pairs.")
        merged_count = 0
        for node1, node2 in node_pairs_to_merge:
            # Ensure nodes still exist and network is above min node count before merging
            if (node1.node_id in self.nodes and node2.node_id in self.nodes and
                len(self.nodes) > self.config.min_nodes + 1): # +1 because we remove 2, add 1

                self.logger.debug(f"ΛTRACE: Processing merge for pair: ('{node1.node_id}', '{node2.node_id}').")
                # Create merged node using the method from SymbolicNode
                merged_node = node1.merge_with(node2) # This logs at SymbolicNode level

                # Order of operations: remove old, then add new to avoid ID conflicts if merged_id was predictable
                self.remove_node(node1.node_id) # remove_node logs
                self.remove_node(node2.node_id) # remove_node logs
                self.add_node(merged_node)      # add_node logs

                self._log_event('nodes_merged_in_network', {
                    'original_node_ids': [node1.node_id, node2.node_id],
                    'new_merged_node_id': merged_node.node_id,
                    'final_node_count': len(self.nodes)
                })
                self.logger.info(f"ΛTRACE: Successfully merged '{node1.node_id}' and '{node2.node_id}' into '{merged_node.node_id}'.")
                merged_count +=1
            else:
                self.logger.warning(f"ΛTRACE: Skipping merge for pair ('{node1.node_id}', '{node2.node_id}'). Conditions not met (existence or min_nodes). Current nodes: {len(self.nodes)}")
        self.logger.info(f"ΛTRACE: Merged {merged_count} node pairs in this operation.")

    # Method for entropy balancing pass
    def entropy_balance_pass(self) -> None:
        """
        Performs an entropy balancing pass across all nodes in the network.
        Node entropies are adjusted towards the network average, weighted by configuration.
        """
        self.logger.debug("ΛTRACE: Starting entropy balance pass for the network.")
        if not self.nodes:
            self.logger.info("ΛTRACE: No nodes in network; entropy balance pass skipped.")
            return

        total_network_entropy = sum(node.entropy for node in self.nodes.values())
        average_network_entropy = total_network_entropy / len(self.nodes)
        self.logger.debug(f"ΛTRACE: Network total entropy: {total_network_entropy:.4f}, Average entropy: {average_network_entropy:.4f}.")

        adjustments_count = 0
        for node_obj in self.nodes.values():
            entropy_difference = node_obj.entropy - average_network_entropy
            entropy_adjustment = entropy_difference * self.config.entropy_balance_weight

            # Only apply adjustment if it's significant to avoid minor float changes
            if abs(entropy_adjustment) > 1e-4:
                new_node_entropy = max(0, node_obj.entropy - entropy_adjustment) # Entropy cannot be negative
                self.logger.debug(f"ΛTRACE: Node '{node_obj.node_id}' old entropy: {node_obj.entropy:.4f}, adjustment: {-entropy_adjustment:.4f}, new entropy: {new_node_entropy:.4f}.")
                node_obj.entropy = new_node_entropy # Direct update, metrics update method not used for isolated entropy change
                node_obj.last_update = time.time() # Mark node as updated
                adjustments_count += 1

        self._log_event('network_entropy_balanced', {
            'nodes_adjusted': adjustments_count,
            'average_entropy_target': average_network_entropy
        })
        self.logger.info(f"ΛTRACE: Entropy balance pass completed. {adjustments_count} nodes had their entropy adjusted.")

    # Method to relink drifted or invalid edges
    def relink_drifted_edges(self) -> None:
        """
        Validates all connections in the network. Removes connections where
        the source or destination node no longer exists. Cleans up associated metadata.
        """
        self.logger.debug("ΛTRACE: Starting relink of drifted/invalid edges.")
        active_node_ids_set = set(self.nodes.keys())
        original_connections_count = len(self.connections)

        # Filter global connections list
        valid_global_connections = [
            (src, dst) for src, dst in self.connections
            if src in active_node_ids_set and dst in active_node_ids_set
        ]
        num_removed_global = original_connections_count - len(valid_global_connections)
        self.connections = valid_global_connections
        if num_removed_global > 0:
             self.logger.debug(f"ΛTRACE: Removed {num_removed_global} invalid entries from global connections list.")

        # Clean up connection metadata for removed connections
        valid_global_connections_set = set(valid_global_connections)
        metadata_keys_to_remove = [
            key for key in self.connection_metadata.keys()
            if key not in valid_global_connections_set
        ]
        for key in metadata_keys_to_remove:
            del self.connection_metadata[key]
        if metadata_keys_to_remove:
            self.logger.debug(f"ΛTRACE: Removed {len(metadata_keys_to_remove)} entries from connection_metadata for invalid connections.")

        # Additionally, ensure individual nodes' connection lists are consistent (though add/remove should handle this)
        # This is a more thorough check.
        for node_id, node_obj in self.nodes.items():
            original_node_conn_count = len(node_obj.connections)
            valid_node_connections = [conn_id for conn_id in node_obj.connections if conn_id in active_node_ids_set]
            if len(valid_node_connections) < original_node_conn_count:
                self.logger.debug(f"ΛTRACE: Node '{node_id}' connections list updated. Removed {original_node_conn_count - len(valid_node_connections)} invalid targets.")
                node_obj.connections = valid_node_connections
                # Also clean weights and metadata within the node for removed connections
                node_obj.connection_weights = {k: v for k, v in node_obj.connection_weights.items() if k in valid_node_connections}
                # Metadata for connection types in SymbolicNode
                meta_keys_to_del_node = [mk for mk in list(node_obj.metadata.keys()) if mk.startswith('connection_type_to_') and mk.split('connection_type_to_')[-1] not in valid_node_connections]
                for mk_del in meta_keys_to_del_node:
                    del node_obj.metadata[mk_del]

        if num_removed_global > 0 or metadata_keys_to_remove:
            self._log_event('network_edges_relinked', {'removed_global_connections_count': num_removed_global})
        self.logger.info(f"ΛTRACE: Relink drifted edges pass completed. Total connections now: {len(self.connections)}.")

    # Method to get network statistics
    def get_network_statistics(self) -> Dict[str, Any]:
        """
        Computes and returns a comprehensive dictionary of current network statistics,
        including node counts, connection counts, performance averages, and more.
        """
        self.logger.debug("ΛTRACE: Calculating network statistics.")
        if not self.nodes:
            self.logger.info("ΛTRACE: Network is empty. Returning empty_network status.")
            return {'status': 'empty_network', 'timestamp': time.time()}

        nodes_by_type_count: Dict[str, int] = {}
        for node_obj in self.nodes.values():
            ntype_val = node_obj.node_type.value
            nodes_by_type_count[ntype_val] = nodes_by_type_count.get(ntype_val, 0) + 1

        error_levels_list = [node.error_level for node in self.nodes.values()]
        activity_levels_list = [node.activity_level for node in self.nodes.values()]
        symbolic_weights_list = [node.symbolic_weight for node in self.nodes.values()]

        num_nodes = len(self.nodes)
        num_connections = len(self.connections)

        stats = {
            'timestamp': time.time(),
            'node_count': num_nodes,
            'connection_count': num_connections,
            'nodes_by_type': nodes_by_type_count,
            'performance_summary': {
                'average_error': np.mean(error_levels_list) if error_levels_list else 0.0,
                'max_error': np.max(error_levels_list) if error_levels_list else 0.0,
                'average_activity': np.mean(activity_levels_list) if activity_levels_list else 0.0,
                'min_activity': np.min(activity_levels_list) if activity_levels_list else 0.0,
                'total_symbolic_weight': sum(symbolic_weights_list)
            },
            'connectivity_metrics': {
                # Density: actual_connections / max_possible_connections (for directed graph: N*(N-1))
                'density': num_connections / (num_nodes * (num_nodes - 1)) if num_nodes > 1 else 0.0,
                'average_degree_out': num_connections / num_nodes if num_nodes > 0 else 0.0 # Assumes connections list is primary
            },
            'health_indicators': {
                'critical_node_count': sum(1 for node in self.nodes.values() if node.is_critical),
                'inactive_node_count': sum(1 for node in self.nodes.values() if not node.is_active),
            },
            'operational_info':{
                'network_age_seconds': round(time.time() - self.creation_time, 2),
                'total_optimizations_run': self.optimization_count,
                'time_since_last_optimization_seconds': round(time.time() - self.last_optimization_time, 2) if self.last_optimization_time > 0 else -1
            }
        }
        self.logger.info(f"ΛTRACE: Network statistics calculated: {stats['node_count']} nodes, {stats['connection_count']} connections.")
        # Storing a snapshot of performance for history, could be selective
        self.performance_history.append({k: stats[k] for k in ('timestamp', 'node_count', 'connection_count', 'performance_summary')})
        if len(self.performance_history) > 100: # Limit history size
             self.performance_history = self.performance_history[-100:]
        return stats

    # Private method to log network events
    def _log_event(self, event_type_str: str, event_data_dict: Dict[str, Any]) -> None:
        """Logs significant network events internally for auditing or later analysis."""
        # This is an internal log, separate from ΛTRACE but can be used by it.
        self.logger.debug(f"ΛTRACE_EVENT ({event_type_str}): {event_data_dict}")
        log_entry = {
            'timestamp': time.time(),
            'event_type': event_type_str,
            'data': event_data_dict,
            'current_node_count': len(self.nodes),
            'current_connection_count': len(self.connections)
        }
        self.event_log.append(log_entry)

        # Keep only a capped number of recent events to manage memory
        max_event_log_size = 1000
        if len(self.event_log) > max_event_log_size:
            self.event_log = self.event_log[-max_event_log_size:]

    # Method to export network state
    def export_network_state(self) -> Dict[str, Any]:
        """
        Exports the complete current state of the network, including configuration,
        nodes, connections, statistics, and recent events, for serialization or backup.
        """
        self.logger.info("ΛTRACE: Exporting complete network state.")
        # Create a serializable representation of NetworkConfig
        serializable_config = {
            'fission_threshold': self.config.fission_threshold,
            'fusion_threshold': self.config.fusion_threshold,
            'remodeling_rate': self.config.remodeling_rate, # Although not used here, it's part of the config
            'max_nodes': self.config.max_nodes,
            'min_nodes': self.config.min_nodes,
            'entropy_balance_weight': self.config.entropy_balance_weight
        }

        exported_state = {
            'network_config_snapshot': serializable_config,
            'nodes_summary': {
                node_id: node.get_state_summary() # This method provides a dict
                for node_id, node in self.nodes.items()
            },
            'connections_list': [ # List of connection dicts for easier iteration
                {
                    'source_node_id': src,
                    'target_node_id': dst,
                    'properties': self.connection_metadata.get((src, dst), {}) # Get metadata like weight, type
                }
                for src, dst in self.connections
            ],
            'current_statistics_snapshot': self.get_network_statistics(), # Get fresh stats
            'recent_internal_events': self.event_log[-50:] if self.event_log else [] # Last 50 events
        }
        self.logger.info(f"ΛTRACE: Network state exported successfully. Nodes: {len(exported_state['nodes_summary'])}, Connections: {len(exported_state['connections_list'])}.")
        return exported_state

    # Method to validate network integrity
    def validate_network_integrity(self) -> Dict[str, Any]:
        """
        Performs a series of checks to validate the integrity of the network structure
        and identify potential issues like orphaned connections or performance anomalies.
        Returns:
            Dict[str, Any]: A dictionary containing validation status and a list of identified issues.
        """
        self.logger.info("ΛTRACE: Validating network integrity.")
        found_issues: List[str] = []
        node_ids_set = set(self.nodes.keys())

        # Check 1: Orphaned global connections (source or destination node does not exist)
        for src_id, dst_id in self.connections:
            if src_id not in node_ids_set:
                issue = f"Orphaned connection: Source node '{src_id}' (in target '{dst_id}') does not exist."
                found_issues.append(issue)
                self.logger.warning(f"ΛTRACE: Integrity issue: {issue}")
            if dst_id not in node_ids_set:
                issue = f"Orphaned connection: Target node '{dst_id}' (from source '{src_id}') does not exist."
                found_issues.append(issue)
                self.logger.warning(f"ΛTRACE: Integrity issue: {issue}")

        # Check 2: Inconsistent node-level connections (node lists a connection to a non-existent node)
        for node_id_str, node_obj in self.nodes.items():
            for connected_node_id in node_obj.connections:
                if connected_node_id not in node_ids_set:
                    issue = f"Node '{node_id_str}' has inconsistent connection: Target '{connected_node_id}' does not exist."
                    found_issues.append(issue)
                    self.logger.warning(f"ΛTRACE: Integrity issue: {issue}")

        # Check 3: Performance anomalies (e.g., too many critical nodes)
        # Threshold for "too many" could be configurable or a percentage
        critical_nodes_count = sum(1 for node in self.nodes.values() if node.is_critical)
        if self.nodes and critical_nodes_count > len(self.nodes) * 0.5: # If more than 50% nodes are critical
            issue = f"Performance anomaly: High number of critical nodes ({critical_nodes_count} out of {len(self.nodes)})."
            found_issues.append(issue)
            self.logger.warning(f"ΛTRACE: Integrity issue: {issue}")

        validation_result = {
            'is_valid': len(found_issues) == 0,
            'issues_found': found_issues,
            'issues_count': len(found_issues),
            'validation_timestamp': time.time()
        }
        self.logger.info(f"ΛTRACE: Network integrity validation complete. Valid: {validation_result['is_valid']}. Issues: {validation_result['issues_count']}.")
        return validation_result

# ═══════════════════════════════════════════════════════════════════════════
# FILENAME: symbolic_network.py
# VERSION: 1.1.0 # Assuming this is an evolution of a prior version
# TIER SYSTEM: Tier 2-4 (Core infrastructure for adaptive systems)
# ΛTRACE INTEGRATION: ENABLED
# CAPABILITIES: Symbolic node representation, Network topology management (add/remove nodes/connections),
#               Node splitting and merging logic, Performance metrics tracking,
#               Entropy balancing, Edge relinking, State export, Integrity validation.
# FUNCTIONS: None directly exposed at module level.
# CLASSES: NodeType (Enum), ConnectionType (Enum), SymbolicNode (Dataclass), SymbolicNetwork.
# DECORATORS: @dataclass.
# DEPENDENCIES: logging, numpy, typing, dataclasses, enum, time, .crista_optimizer (for NetworkConfig).
# INTERFACES: SymbolicNode and SymbolicNetwork classes are the main interfaces.
# ERROR HANDLING: Logs warnings for invalid operations (e.g., adding existing node, removing non-existent).
#                 Integrity validation identifies structural issues. More robust error raising could be added.
# LOGGING: ΛTRACE_ENABLED via Python's logging module for detailed tracing of operations.
#          Internal event logging within SymbolicNetwork for auditing.
# AUTHENTICATION: Not applicable at this component level.
# HOW TO USE:
#   from core.adaptive_systems.crista_optimizer.symbolic_network import SymbolicNetwork, SymbolicNode, NodeType
#   from core.adaptive_systems.crista_optimizer.crista_optimizer import NetworkConfig # Or from common config
#   net_conf = NetworkConfig()
#   network = SymbolicNetwork(config=net_conf)
#   node1 = SymbolicNode(node_id="n1", node_type=NodeType.PROCESSING)
#   network.add_node(node1)
#   # ... further network operations ...
#   stats = network.get_network_statistics()
# INTEGRATION NOTES: This module is foundational for CristaOptimizer. Ensure NetworkConfig is consistent.
#                    Node metrics (error, activity, etc.) are expected to be updated by external processes
#                    or the optimizer itself to drive adaptive behaviors.
# MAINTENANCE: Review thresholds for critical state and merge compatibility.
#              Enhance _are_merge_compatible for more sophisticated checks if needed.
#              Consider performance implications for very large networks.
# CONTACT: LUKHAS DEVELOPMENT TEAM
# LICENSE: PROPRIETARY - LUKHAS AI SYSTEMS - UNAUTHORIZED ACCESS PROHIBITED
# ═══════════════════════════════════════════════════════════════════════════
